{
  "dataset": "crawl4ai",
  "runs": [
    {
      "run_id": 1,
      "timestamp": "2025-02-12T07:50:25.341350",
      "negative_results": [
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult.cleaned_html",
          "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult.cleaned_html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.screenshot",
          "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.screenshot",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "attribute access",
          "relationship_explanation": "After crawling, the extracted content is retrieved via the 'extracted_content' attribute of the CrawlResult, which holds the JSON output from the extraction process.",
          "predicted_trace_chain": "docs/md_v2/basic/content-selection.md -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The final step in the documented example involves accessing the 'extracted_content' attribute from the crawl result to process the extracted data.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 21,
          "artifact_title": "ExtractionStrategy",
          "predicted_relationship": "explicit",
          "relationship_type": "abstract class definition",
          "relationship_explanation": "The documentation mentions the concept of 'Extraction Strategy' for defining how to extract structured data. This concept maps directly to the ExtractionStrategy abstract base class.",
          "predicted_trace_chain": "docs/md_v2/advanced/session-management-advanced.md -> ExtractionStrategy",
          "predicted_trace_chain_explanation": "The documentation\u2019s discussion on extraction strategies is directly traced to the ExtractionStrategy class, which defines the interface for data extraction methods.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 1,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "default dependency",
          "relationship_explanation": "AsyncWebCrawler, when instantiated without a custom crawler_strategy, defaults to using AsyncPlaywrightCrawlerStrategy. Although not explicitly mentioned in the text snippet, it underpins the crawling implementation.",
          "predicted_trace_chain": "docs/md_v2/basic/quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
          "predicted_trace_chain_explanation": "The chain extends from the documentation file to AsyncWebCrawler, which internally instantiates AsyncPlaywrightCrawlerStrategy as its default strategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "base class",
          "relationship_explanation": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, establishing a base class relationship that defines the abstract crawling interface used by the default strategy.",
          "predicted_trace_chain": "docs/md_v2/basic/quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "Starting from the documentation file, the chain follows AsyncWebCrawler\u2019s default dependency on AsyncPlaywrightCrawlerStrategy, which in turn inherits from AsyncCrawlerStrategy, thus outlining the inheritance hierarchy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 14,
          "artifact_title": "CrawlResult.links",
          "predicted_relationship": "implicit",
          "relationship_type": "defines",
          "relationship_explanation": "The 'links' attribute in the CrawlResult model holds the links that are included in the crawl results. Smart link filtering parameters in the documentation are intended to control what ends up in this attribute.",
          "predicted_trace_chain": "docs/md_v2/advanced/content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult.links",
          "predicted_trace_chain_explanation": "The crawling process started by 'arun()' eventually produces a CrawlResult, and its 'links' attribute reflects the impact of any filtering options\u2014thus closing the traceability loop from the documentation to the result model.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "contrast",
          "relationship_explanation": "The documentation highlights the advantage of having 'No External Dependencies' by contrasting CSS-based extraction with LLM\u2010based strategies that require API calls. LLMExtractionStrategy represents the latter approach and is thereby implicitly referenced as an alternative method.",
          "predicted_trace_chain": "docs/md_v2/extraction/css.md -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "Even though the documentation focuses on JsonCssExtractionStrategy, it implicitly sets it apart from LLMExtractionStrategy by emphasizing speed, precision, and structured JSON output without needing external services. This contrast creates a trace linking the document to LLMExtractionStrategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 2,
      "timestamp": "2025-02-12T14:22:59.097934",
      "negative_results": [
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "explicit",
          "relationship_type": "attribute access",
          "relationship_explanation": "After crawling, the snippet accesses the extracted_content attribute of the result object to retrieve JSON data. This field reflects the output from the extraction strategy.",
          "predicted_trace_chain": "docs/md_v2/basic/content-selection.md -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The documentation shows the final step of processing the crawl output by accessing the extracted_content attribute, thereby linking the crawling process to the final extracted results.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 14,
          "artifact_title": "CrawlResult.links",
          "predicted_relationship": "implicit",
          "relationship_type": "result attribute mapping",
          "relationship_explanation": "Although the snippet does not directly mention the 'links' attribute, the context 'Control which links are included in the results' implies that the links collected and returned by the crawl are stored in the 'links' attribute of the CrawlResult. This attribute is where the effect of the filtering parameters would be reflected.",
          "predicted_trace_chain": "docs/md_v2/advanced/content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult.links",
          "predicted_trace_chain_explanation": "The chain shows that the documentation references the crawl method 'arun()' whose returned CrawlResult contains the 'links' attribute, indirectly capturing the link filtering behavior.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.error_message",
          "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.error_message",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.links",
          "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.links",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "complementary strategy usage",
          "relationship_explanation": "The documentation mentions that the CosineStrategy can be used as a pre\u2010processing step for LLM\u2010based extraction, implying a role for LLMExtractionStrategy in the extraction pipeline.",
          "predicted_trace_chain": "docs/md_v2/extraction/cosine.md -> CosineStrategy -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "This chain indicates that while CosineStrategy is highlighted explicitly, it is often paired with LLMExtractionStrategy to enhance extraction capabilities.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.screenshot",
          "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.screenshot",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 21,
          "artifact_title": "ExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "abstract base class",
          "relationship_explanation": "The documentation mentions the need for an extraction strategy to extract structured data from the page. ExtractionStrategy serves as the abstract interface for such methods and is used by AsyncWebCrawler.arun() to process the page content according to a defined extraction method.",
          "predicted_trace_chain": "docs/md_v2/advanced/session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> ExtractionStrategy",
          "predicted_trace_chain_explanation": "The trace chain indicates that during the execution of the arun() method of AsyncWebCrawler, an ExtractionStrategy compliant method is employed to extract data, fulfilling the concept described in the documentation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 3,
      "timestamp": "2025-02-12T14:28:15.214652",
      "negative_results": [
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "property access",
          "relationship_explanation": "After the crawl, the snippet accesses 'result.extracted_content' to retrieve the extracted articles. This accesses the public attribute of the CrawlResult model that holds the processed extraction output.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The documented example shows that the result from AsyncWebCrawler.arun() (the crawl operation) returns a CrawlResult object whose 'extracted_content' property is used to obtain the final data.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 1,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "default dependency implementation",
          "relationship_explanation": "Although AsyncPlaywrightCrawlerStrategy is not mentioned in the text snippet, AsyncWebCrawler internally instantiates it as the default crawling strategy when no crawler_strategy is provided.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
          "predicted_trace_chain_explanation": "The documentation\u2019s use of AsyncWebCrawler leads to its default dependency, AsyncPlaywrightCrawlerStrategy, which is responsible for the actual crawling tasks even though it is not explicitly imported in the snippet.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "base class",
          "relationship_explanation": "AsyncCrawlerStrategy is the abstract base class for crawler strategies. AsyncPlaywrightCrawlerStrategy extends it; therefore, it is implicitly connected to the documented usage via the default strategy implementation used by AsyncWebCrawler.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "Beginning at the documented AsyncWebCrawler usage, the chain continues with AsyncPlaywrightCrawlerStrategy (the concrete implementation) which in turn extends AsyncCrawlerStrategy, linking the abstract base class to the documented functionality.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "implicit",
          "relationship_type": "attribute access",
          "relationship_explanation": "After the crawl operation, the returned result\u2019s markdown attribute is accessed to retrieve the processed content. This attribute is part of the CrawlResult object.",
          "predicted_trace_chain": "docs/md_v2/basic/browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "The chain begins with the documentation file, proceeds with the instantiation and method call on AsyncWebCrawler, and ends with accessing the markdown attribute from the resulting CrawlResult.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "contrast",
          "relationship_explanation": "The documentation contrasts JsonCssExtractionStrategy with LLM\u2010based methods by mentioning the absence of external API calls. This indirectly references LLMExtractionStrategy, which relies on external API services.",
          "predicted_trace_chain": "docs/md_v2/extraction/css.md -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "By emphasizing the benefit of not requiring external dependencies, the document implicitly contrasts with LLMExtractionStrategy, linking the advantages of JsonCssExtractionStrategy to the drawbacks of LLM-based approaches.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.error_message",
          "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.error_message",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 21,
          "artifact_title": "ExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "abstract base class",
          "relationship_explanation": "The documentation lists 'Extraction Strategy' as a concept for retrieving structured data. The ExtractionStrategy abstract base class defines the interface for extraction strategies used by AsyncWebCrawler.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> ExtractionStrategy",
          "predicted_trace_chain_explanation": "AsyncWebCrawler accepts an extraction_strategy parameter based on the ExtractionStrategy interface, aligning with the documentation's discussion of extraction strategies.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "concrete implementation",
          "relationship_explanation": "The documentation mentions the use of CSS selectors for targeting elements. JsonCssExtractionStrategy is a concrete implementation that uses CSS selectors to extract structured data from a webpage based on a schema.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "predicted_trace_chain_explanation": "In the chain of extraction strategy implementations utilized by AsyncWebCrawler, JsonCssExtractionStrategy specifically leverages CSS selectors, which aligns with the documentation's emphasis on targeting elements for extraction.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        }
      ]
    },
    {
      "run_id": 4,
      "timestamp": "2025-02-12T14:33:19.717838",
      "negative_results": [
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.error_message",
          "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.error_message",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 14,
          "artifact_title": "CrawlResult.links",
          "predicted_relationship": "implicit",
          "relationship_type": "attribute usage",
          "relationship_explanation": "Although the snippet does not mention it directly, the filtered links resulting from the smart link filtering are stored in the 'links' attribute of the CrawlResult. This shows an implicit relationship where the method's filtering parameters affect the contents of 'CrawlResult.links'.",
          "predicted_trace_chain": "docs/md_v2/advanced/content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult.links",
          "predicted_trace_chain_explanation": "The pathway starts at the documentation, then uses the 'AsyncWebCrawler.arun()' method (which handles the filtering options) and finally impacts the 'CrawlResult.links' attribute that holds the final set of filtered links.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 21,
          "artifact_title": "ExtractionStrategy",
          "predicted_relationship": "explicit",
          "relationship_type": "defines the contract for data extraction",
          "relationship_explanation": "The documentation explicitly lists 'Extraction Strategy' as a key concept. The ExtractionStrategy abstract class defines the mechanism for extracting structured data from a page, directly mapping to the documented functionality.",
          "predicted_trace_chain": "docs/md_v2/advanced/session-management-advanced.md -> ExtractionStrategy",
          "predicted_trace_chain_explanation": "Here, the documentation\u2019s mention of extraction strategy is directly traced to the ExtractionStrategy class, which sets the interface for how structured data is to be extracted.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.screenshot",
          "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.screenshot",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 1,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "dependency",
          "relationship_explanation": "Although AsyncPlaywrightCrawlerStrategy is not mentioned in the snippet, AsyncWebCrawler defaults to using it when no crawler_strategy is provided. This makes it an implicit dependency supporting the crawling functionality.",
          "predicted_trace_chain": "docs/md_v2/basic/quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
          "predicted_trace_chain_explanation": "The AsyncWebCrawler class internally instantiates AsyncPlaywrightCrawlerStrategy, linking the documentation's usage example to this implementation class.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "inheritance",
          "relationship_explanation": "AsyncCrawlerStrategy is the abstract base class from which AsyncPlaywrightCrawlerStrategy derives. Since AsyncPlaywrightCrawlerStrategy is used implicitly by AsyncWebCrawler, AsyncCrawlerStrategy is an indirect part of the trace chain.",
          "predicted_trace_chain": "docs/md_v2/basic/quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "This chain shows how the functionality in the documentation is backed by a class hierarchy, with AsyncPlaywrightCrawlerStrategy extending AsyncCrawlerStrategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "explicit",
          "relationship_type": "attribute access",
          "relationship_explanation": "The documentation snippet accesses 'result.extracted_content' to retrieve the JSON-formatted extraction output. This demonstrates an explicit use of the public attribute 'extracted_content' of the CrawlResult object.",
          "predicted_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The trace chain starts at the documentation file and leads directly to the use of the 'extracted_content' attribute, indicating its role in holding the processed extraction data.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "explicit",
          "relationship_type": "attribute access",
          "relationship_explanation": "After the crawl method returns a result, the code explicitly retrieves the 'markdown' attribute from the resulting CrawlResult to get the extracted content.",
          "predicted_trace_chain": "docs/md_v2/basic/browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "This chain shows the result of the 'arun' method (a CrawlResult) and identifies that its 'markdown' field is used directly in the documentation snippet.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 5,
      "timestamp": "2025-02-12T14:38:24.931896",
      "negative_results": [
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.screenshot",
          "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage image as a base64-encoded string that can be decoded into a PNG file as shown in the documentation example.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.screenshot",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "explicit",
          "relationship_type": "attribute access",
          "relationship_explanation": "After the crawling operation, the 'extracted_content' attribute of the CrawlResult object is accessed and parsed with json.loads to obtain the final articles. This shows how the extraction output is exposed in the result.",
          "predicted_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The extraction outcome is obtained as the 'extracted_content' attribute, directly linking the crawling execution to the processing of extracted data in the documentation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "explicit",
          "relationship_type": "attribute access",
          "relationship_explanation": "The snippet accesses the 'markdown' attribute of the result returned by 'arun()', which represents the extracted content.",
          "predicted_trace_chain": "docs/md_v2/basic/browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "This chain shows how the output of the crawling process (via arun()) provides the 'markdown' attribute, which is directly used in the example.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extraction strategy component",
          "relationship_explanation": "The snippet mentions that the Cosine Strategy can be used as a pre\u2010processing step for LLM-based extraction. Although not explicitly invoked in the code sample, this implies a relationship with LLMExtractionStrategy, which provides LLM-based extraction functionality.",
          "predicted_trace_chain": "docs/md_v2/extraction/cosine.md -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "Starting at the document, the mention of LLM-based extraction implicitly connects to the LLMExtractionStrategy, suggesting its use in the processing pipeline even though it is not directly called in the snippet.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.error_message",
          "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.error_message",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.links",
          "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.links",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.fit_markdown",
          "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.fit_markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.html",
          "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.status_code",
          "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.status_code",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.cleaned_html",
          "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.cleaned_html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core methods needed to implement the session management behaviors described in the documentation, including crawl() for executing page actions and hook management for handling session state.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "contrast comparison",
          "relationship_explanation": "The documentation contrasts CSS-based extraction with LLM-based strategies by noting the absence of external dependencies. 'LLMExtractionStrategy' is implicitly referenced as an alternative approach that typically requires API calls, highlighting the advantage of the JSON/CSS extraction method.",
          "predicted_trace_chain": "docs/md_v2/extraction/css.md -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "Although not directly implemented in the documentation snippet, the chain includes 'LLMExtractionStrategy' to represent the contrasting extraction approach, emphasizing that the described method does not rely on external API calls.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 1,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "AsyncWebCrawler, if not provided with a custom crawler strategy, defaults to initializing AsyncPlaywrightCrawlerStrategy. This internal dependency, though not directly mentioned in the snippet, plays a critical role in executing the crawling functionality.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
          "predicted_trace_chain_explanation": "Starting from quickstart.md, the instantiation of AsyncWebCrawler leads to its internal use of AsyncPlaywrightCrawlerStrategy to handle the crawling tasks, forming an indirect trace.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "AsyncPlaywrightCrawlerStrategy extends the abstract class AsyncCrawlerStrategy. This relationship underpins the crawling implementation used by AsyncWebCrawler, ensuring adherence to the defined crawling interface.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "The chain illustrates that from the quickstart documentation, AsyncWebCrawler employs AsyncPlaywrightCrawlerStrategy, which in turn is built as an extension of AsyncCrawlerStrategy, thereby completing the trace of the crawling implementation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.error_message",
          "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.error_message",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    }
  ]
}