[
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "attribute access",
    "relationship_explanation": "After crawling, the extracted content is retrieved via the 'extracted_content' attribute of the CrawlResult, which holds the JSON output from the extraction process.",
    "predicted_trace_chain": "docs/md_v2/basic/content-selection.md -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The final step in the documented example involves accessing the 'extracted_content' attribute from the crawl result to process the extracted data.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 21,
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "explicit",
    "relationship_type": "abstract class definition",
    "relationship_explanation": "The documentation mentions the concept of 'Extraction Strategy' for defining how to extract structured data. This concept maps directly to the ExtractionStrategy abstract base class.",
    "predicted_trace_chain": "docs/md_v2/advanced/session-management-advanced.md -> ExtractionStrategy",
    "predicted_trace_chain_explanation": "The documentation\u2019s discussion on extraction strategies is directly traced to the ExtractionStrategy class, which defines the interface for data extraction methods.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content "
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 1,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "default dependency",
    "relationship_explanation": "AsyncWebCrawler, when instantiated without a custom crawler_strategy, defaults to using AsyncPlaywrightCrawlerStrategy. Although not explicitly mentioned in the text snippet, it underpins the crawling implementation.",
    "predicted_trace_chain": "docs/md_v2/basic/quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "predicted_trace_chain_explanation": "The chain extends from the documentation file to AsyncWebCrawler, which internally instantiates AsyncPlaywrightCrawlerStrategy as its default strategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "IOL"
    ],
    "artifact_code": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "base class",
    "relationship_explanation": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, establishing a base class relationship that defines the abstract crawling interface used by the default strategy.",
    "predicted_trace_chain": "docs/md_v2/basic/quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "Starting from the documentation file, the chain follows AsyncWebCrawler\u2019s default dependency on AsyncPlaywrightCrawlerStrategy, which in turn inherits from AsyncCrawlerStrategy, thus outlining the inheritance hierarchy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "IOL"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 14,
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "implicit",
    "relationship_type": "defines",
    "relationship_explanation": "The 'links' attribute in the CrawlResult model holds the links that are included in the crawl results. Smart link filtering parameters in the documentation are intended to control what ends up in this attribute.",
    "predicted_trace_chain": "docs/md_v2/advanced/content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult.links",
    "predicted_trace_chain_explanation": "The crawling process started by 'arun()' eventually produces a CrawlResult, and its 'links' attribute reflects the impact of any filtering options\u2014thus closing the traceability loop from the documentation to the result model.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "links: Dict[str, List[Dict]] = {}"
  },
  {
    "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "contrast",
    "relationship_explanation": "The documentation highlights the advantage of having 'No External Dependencies' by contrasting CSS-based extraction with LLM\u2010based strategies that require API calls. LLMExtractionStrategy represents the latter approach and is thereby implicitly referenced as an alternative method.",
    "predicted_trace_chain": "docs/md_v2/extraction/css.md -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "Even though the documentation focuses on JsonCssExtractionStrategy, it implicitly sets it apart from LLMExtractionStrategy by emphasizing speed, precision, and structured JSON output without needing external services. This contrast creates a trace linking the document to LLMExtractionStrategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "explicit",
    "relationship_type": "attribute access",
    "relationship_explanation": "After crawling, the snippet accesses the extracted_content attribute of the result object to retrieve JSON data. This field reflects the output from the extraction strategy.",
    "predicted_trace_chain": "docs/md_v2/basic/content-selection.md -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The documentation shows the final step of processing the crawl output by accessing the extracted_content attribute, thereby linking the crawling process to the final extracted results.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 14,
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "implicit",
    "relationship_type": "result attribute mapping",
    "relationship_explanation": "Although the snippet does not directly mention the 'links' attribute, the context 'Control which links are included in the results' implies that the links collected and returned by the crawl are stored in the 'links' attribute of the CrawlResult. This attribute is where the effect of the filtering parameters would be reflected.",
    "predicted_trace_chain": "docs/md_v2/advanced/content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult.links",
    "predicted_trace_chain_explanation": "The chain shows that the documentation references the crawl method 'arun()' whose returned CrawlResult contains the 'links' attribute, indirectly capturing the link filtering behavior.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "links: Dict[str, List[Dict]] = {}"
  },
  {
    "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "complementary strategy usage",
    "relationship_explanation": "The documentation mentions that the CosineStrategy can be used as a pre\u2010processing step for LLM\u2010based extraction, implying a role for LLMExtractionStrategy in the extraction pipeline.",
    "predicted_trace_chain": "docs/md_v2/extraction/cosine.md -> CosineStrategy -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "This chain indicates that while CosineStrategy is highlighted explicitly, it is often paired with LLMExtractionStrategy to enhance extraction capabilities.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 21,
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "abstract base class",
    "relationship_explanation": "The documentation mentions the need for an extraction strategy to extract structured data from the page. ExtractionStrategy serves as the abstract interface for such methods and is used by AsyncWebCrawler.arun() to process the page content according to a defined extraction method.",
    "predicted_trace_chain": "docs/md_v2/advanced/session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> ExtractionStrategy",
    "predicted_trace_chain_explanation": "The trace chain indicates that during the execution of the arun() method of AsyncWebCrawler, an ExtractionStrategy compliant method is employed to extract data, fulfilling the concept described in the documentation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE", "IOL"
    ],
    "artifact_code": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content "
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "property access",
    "relationship_explanation": "After the crawl, the snippet accesses 'result.extracted_content' to retrieve the extracted articles. This accesses the public attribute of the CrawlResult model that holds the processed extraction output.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The documented example shows that the result from AsyncWebCrawler.arun() (the crawl operation) returns a CrawlResult object whose 'extracted_content' property is used to obtain the final data.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 1,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "default dependency implementation",
    "relationship_explanation": "Although AsyncPlaywrightCrawlerStrategy is not mentioned in the text snippet, AsyncWebCrawler internally instantiates it as the default crawling strategy when no crawler_strategy is provided.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "predicted_trace_chain_explanation": "The documentation\u2019s use of AsyncWebCrawler leads to its default dependency, AsyncPlaywrightCrawlerStrategy, which is responsible for the actual crawling tasks even though it is not explicitly imported in the snippet.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "base class",
    "relationship_explanation": "AsyncCrawlerStrategy is the abstract base class for crawler strategies. AsyncPlaywrightCrawlerStrategy extends it; therefore, it is implicitly connected to the documented usage via the default strategy implementation used by AsyncWebCrawler.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "Beginning at the documented AsyncWebCrawler usage, the chain continues with AsyncPlaywrightCrawlerStrategy (the concrete implementation) which in turn extends AsyncCrawlerStrategy, linking the abstract base class to the documented functionality.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "implicit",
    "relationship_type": "attribute access",
    "relationship_explanation": "After the crawl operation, the returned result\u2019s markdown attribute is accessed to retrieve the processed content. This attribute is part of the CrawlResult object.",
    "predicted_trace_chain": "docs/md_v2/basic/browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "The chain begins with the documentation file, proceeds with the instantiation and method call on AsyncWebCrawler, and ends with accessing the markdown attribute from the resulting CrawlResult.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "contrast",
    "relationship_explanation": "The documentation contrasts JsonCssExtractionStrategy with LLM\u2010based methods by mentioning the absence of external API calls. This indirectly references LLMExtractionStrategy, which relies on external API services.",
    "predicted_trace_chain": "docs/md_v2/extraction/css.md -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "By emphasizing the benefit of not requiring external dependencies, the document implicitly contrasts with LLMExtractionStrategy, linking the advantages of JsonCssExtractionStrategy to the drawbacks of LLM-based approaches.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 21,
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "abstract base class",
    "relationship_explanation": "The documentation lists 'Extraction Strategy' as a concept for retrieving structured data. The ExtractionStrategy abstract base class defines the interface for extraction strategies used by AsyncWebCrawler.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> ExtractionStrategy",
    "predicted_trace_chain_explanation": "AsyncWebCrawler accepts an extraction_strategy parameter based on the ExtractionStrategy interface, aligning with the documentation's discussion of extraction strategies.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE", "IOL"
    ],
    "artifact_code": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content "
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "concrete implementation",
    "relationship_explanation": "The documentation mentions the use of CSS selectors for targeting elements. JsonCssExtractionStrategy is a concrete implementation that uses CSS selectors to extract structured data from a webpage based on a schema.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "predicted_trace_chain_explanation": "In the chain of extraction strategy implementations utilized by AsyncWebCrawler, JsonCssExtractionStrategy specifically leverages CSS selectors, which aligns with the documentation's emphasis on targeting elements for extraction.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE", "IOL"
    ],
    "artifact_code": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results"
  },
  {
    "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 14,
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "implicit",
    "relationship_type": "attribute usage",
    "relationship_explanation": "Although the snippet does not mention it directly, the filtered links resulting from the smart link filtering are stored in the 'links' attribute of the CrawlResult. This shows an implicit relationship where the method's filtering parameters affect the contents of 'CrawlResult.links'.",
    "predicted_trace_chain": "docs/md_v2/advanced/content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult.links",
    "predicted_trace_chain_explanation": "The pathway starts at the documentation, then uses the 'AsyncWebCrawler.arun()' method (which handles the filtering options) and finally impacts the 'CrawlResult.links' attribute that holds the final set of filtered links.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "links: Dict[str, List[Dict]] = {}"
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 21,
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "explicit",
    "relationship_type": "defines the contract for data extraction",
    "relationship_explanation": "The documentation explicitly lists 'Extraction Strategy' as a key concept. The ExtractionStrategy abstract class defines the mechanism for extracting structured data from a page, directly mapping to the documented functionality.",
    "predicted_trace_chain": "docs/md_v2/advanced/session-management-advanced.md -> ExtractionStrategy",
    "predicted_trace_chain_explanation": "Here, the documentation\u2019s mention of extraction strategy is directly traced to the ExtractionStrategy class, which sets the interface for how structured data is to be extracted.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "IOL"
    ],
    "artifact_code": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content "
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 1,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "dependency",
    "relationship_explanation": "Although AsyncPlaywrightCrawlerStrategy is not mentioned in the snippet, AsyncWebCrawler defaults to using it when no crawler_strategy is provided. This makes it an implicit dependency supporting the crawling functionality.",
    "predicted_trace_chain": "docs/md_v2/basic/quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "predicted_trace_chain_explanation": "The AsyncWebCrawler class internally instantiates AsyncPlaywrightCrawlerStrategy, linking the documentation's usage example to this implementation class.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "IOL"
    ],
    "artifact_code": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "inheritance",
    "relationship_explanation": "AsyncCrawlerStrategy is the abstract base class from which AsyncPlaywrightCrawlerStrategy derives. Since AsyncPlaywrightCrawlerStrategy is used implicitly by AsyncWebCrawler, AsyncCrawlerStrategy is an indirect part of the trace chain.",
    "predicted_trace_chain": "docs/md_v2/basic/quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "This chain shows how the functionality in the documentation is backed by a class hierarchy, with AsyncPlaywrightCrawlerStrategy extending AsyncCrawlerStrategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "explicit",
    "relationship_type": "attribute access",
    "relationship_explanation": "The documentation snippet accesses 'result.extracted_content' to retrieve the JSON-formatted extraction output. This demonstrates an explicit use of the public attribute 'extracted_content' of the CrawlResult object.",
    "predicted_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The trace chain starts at the documentation file and leads directly to the use of the 'extracted_content' attribute, indicating its role in holding the processed extraction data.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "explicit",
    "relationship_type": "attribute access",
    "relationship_explanation": "After the crawl method returns a result, the code explicitly retrieves the 'markdown' attribute from the resulting CrawlResult to get the extracted content.",
    "predicted_trace_chain": "docs/md_v2/basic/browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "This chain shows the result of the 'arun' method (a CrawlResult) and identifies that its 'markdown' field is used directly in the documentation snippet.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "explicit",
    "relationship_type": "attribute access",
    "relationship_explanation": "After the crawling operation, the 'extracted_content' attribute of the CrawlResult object is accessed and parsed with json.loads to obtain the final articles. This shows how the extraction output is exposed in the result.",
    "predicted_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The extraction outcome is obtained as the 'extracted_content' attribute, directly linking the crawling execution to the processing of extracted data in the documentation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "explicit",
    "relationship_type": "attribute access",
    "relationship_explanation": "The snippet accesses the 'markdown' attribute of the result returned by 'arun()', which represents the extracted content.",
    "predicted_trace_chain": "docs/md_v2/basic/browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "This chain shows how the output of the crawling process (via arun()) provides the 'markdown' attribute, which is directly used in the example.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IOL"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extraction strategy component",
    "relationship_explanation": "The snippet mentions that the Cosine Strategy can be used as a pre\u2010processing step for LLM-based extraction. Although not explicitly invoked in the code sample, this implies a relationship with LLMExtractionStrategy, which provides LLM-based extraction functionality.",
    "predicted_trace_chain": "docs/md_v2/extraction/cosine.md -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "Starting at the document, the mention of LLM-based extraction implicitly connects to the LLMExtractionStrategy, suggesting its use in the processing pipeline even though it is not directly called in the snippet.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "contrast comparison",
    "relationship_explanation": "The documentation contrasts CSS-based extraction with LLM-based strategies by noting the absence of external dependencies. 'LLMExtractionStrategy' is implicitly referenced as an alternative approach that typically requires API calls, highlighting the advantage of the JSON/CSS extraction method.",
    "predicted_trace_chain": "docs/md_v2/extraction/css.md -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "Although not directly implemented in the documentation snippet, the chain includes 'LLMExtractionStrategy' to represent the contrasting extraction approach, emphasizing that the described method does not rely on external API calls.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 1,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "AsyncWebCrawler, if not provided with a custom crawler strategy, defaults to initializing AsyncPlaywrightCrawlerStrategy. This internal dependency, though not directly mentioned in the snippet, plays a critical role in executing the crawling functionality.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "predicted_trace_chain_explanation": "Starting from quickstart.md, the instantiation of AsyncWebCrawler leads to its internal use of AsyncPlaywrightCrawlerStrategy to handle the crawling tasks, forming an indirect trace.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "IOL"
    ],
    "artifact_code": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "AsyncPlaywrightCrawlerStrategy extends the abstract class AsyncCrawlerStrategy. This relationship underpins the crawling implementation used by AsyncWebCrawler, ensuring adherence to the defined crawling interface.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "The chain illustrates that from the quickstart documentation, AsyncWebCrawler employs AsyncPlaywrightCrawlerStrategy, which in turn is built as an extension of AsyncCrawlerStrategy, thereby completing the trace of the crawling implementation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE",
      "IOL"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  }
]