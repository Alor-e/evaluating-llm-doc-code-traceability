[
  {
    "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "base_class",
    "relationship_explanation": "Since the 'AsyncWebCrawler' is known to utilize different strategies and 'JsonCssExtractionStrategy' is one of them, it implies the use of 'AsyncCrawlerStrategy', which defines the abstract interface for crawler strategies.",
    "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "'AsyncWebCrawler' likely relies on the functionality defined in 'AsyncCrawlerStrategy' to accommodate various strategies, including the 'JsonCssExtractionStrategy'.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "returns",
    "relationship_explanation": "While not explicitly mentioned, the code example utilizes the `result1.markdown` attribute returned by `arun()`, which is a likely part of the `CrawlResult` class structure.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The `quickstart.md` example starts with `AsyncWebCrawler`, invokes `arun()`, which provides access to properties of `CrawlResult` such as `html`, from which `markdown` is likely derived.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "document_location": "docs/md_v2/advanced/session-management-advanced.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "The kill_session method is part of the mechanism to clean up sessions as indicated by its name and role within AsyncPlaywrightCrawlerStrategy. This indirectly supports the documentation's session management theme by handling session lifecycle.",
    "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_trace_chain_explanation": "This trace route captures the progression from the document focusing on session management to how AsyncWebCrawler potentially leverages the kill_session feature implemented in AsyncPlaywrightCrawlerStrategy for managing sessions.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]"
  },
  {
    "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "document_location": "docs/md_v2/basic/page-interaction.md",
    "artifact_id": 3,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The `set_hook()` method of `AsyncPlaywrightCrawlerStrategy` can be used to set hooks, potentially related to implementing specific wait conditions based on CSS or JS, as demonstrated in the documentation.",
    "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_trace_chain_explanation": "The `arun()` method, through the `AsyncPlaywrightCrawlerStrategy`, might configure hooks for event handling related to wait conditions specified in the documentation. The `set_hook()` method facilitates setting such hooks.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\")"
  },
  {
    "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "related",
    "relationship_explanation": "Although not directly used, `JsonCssExtractionStrategy` is mentioned in the context hierarchically in discussing extraction techniques using different strategies in the same extraction module.",
    "predicted_trace_chain": "llm.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
    "predicted_trace_chain_explanation": "The document discusses extraction strategies, hinting at the hierarchical relationship by contextualizing `LLMExtractionStrategy` alongside potential alternatives like `JsonCssExtractionStrategy`, showcasing possible variations/expansions.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results"
  },
  {
    "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "document_location": "docs/md_v2/index.md",
    "artifact_id": 20,
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The attribute `success` from the `CrawlResult` is implicitly used as it refers to checking whether the `arun()` method executed successfully. It supports drawing conclusions from the crawl results, indicating the method's reliability and is shown in the code by how results are being processed through `result.markdown`.",
    "predicted_trace_chain": "index.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
    "predicted_trace_chain_explanation": "The use of `AsyncWebCrawler.arun()` method ultimately returns a `CrawlResult`, where attributes like `success` can be evaluated. This constitutes the flow of checking outcomes after invocation, linking the class method and its resultant data model.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "success: bool"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 5,
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "While `arun` is not directly mentioned, the setup implies `AsyncWebCrawler` methods, such as `arun`, will be used to perform crawling within the async context manager.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "predicted_trace_chain_explanation": "The AsyncWebCrawler class's methods, including `arun`, are logically inferred to be part of the operations during usage, given the example's context.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) "
  },
  {
    "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.process_iframes()",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "The presence of the 'process_iframes=True' argument implies the underlying functionality related to iframe processing, which is implemented by 'AsyncPlaywrightCrawlerStrategy.process_iframes()'. However, it's not mentioned directly in the text but is inferred from the description of 'process_iframes=True'.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy.process_iframes()",
    "predicted_trace_chain_explanation": "The pathway shows explicit use of 'arun' which in turn implicitly relies on 'AsyncPlaywrightCrawlerStrategy.process_iframes()' to manage iframe processing, highlighting its implementation role.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "PAL"
    ],
    "artifact_code": ""
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "accesses",
    "relationship_explanation": "Although not explicitly mentioned, the example uses the `result` object to check for `result.html`, indicating indirect access to the `html` attribute from `CrawlResult`. The evidence is seen in checking the `result` object for success and handling the content.",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The `arun` method returns a `CrawlResult`, allowing access to `html` within the result object. The pathway illustrates how the example indirectly uses this attribute after receiving the `CrawlResult` from `arun()`.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "accesses",
    "relationship_explanation": "The example accesses 'result.extracted_content', implying that 'CrawlResult.extracted_content' is an attribute accessed to obtain the JSON converted extracted content.",
    "predicted_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The pathway highlights that after calling 'arun', the 'CrawlResult' object returned holds 'extracted_content'. This is accessed in the example to process the extracted content, thus establishing a correlation between method invocation and data retrieval.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "access",
    "relationship_explanation": "In the example, 'result.extracted_content' is retrieved after executing the crawl, showing the utilization of the 'extracted_content' field of 'CrawlResult'.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The document snippet ends with accessing 'extracted_content', which denotes fetching results post execution. This shows the pathway from calling 'arun()' to getting the crawl results and using extracted fields.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "Although LLMExtractionStrategy is not directly mentioned, it is a type of ExtractionStrategy that can be used within AsyncWebCrawler's extract operation, hinting at potential implicit usage when processing content.",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "AsyncWebCrawler.arun() accepts various strategies for processing content, pointing to potential internal use of LLMExtractionStrategy among others when extracting meaningful data.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The example code sets options related to session processes like process_iframes, which indirectly implicate the use of kill_session within AsyncPlaywrightCrawlerStrategy to manage session lifecycle during a crawl.",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_trace_chain_explanation": "Use of process_iframes in AsyncWebCrawler.arun() likely involves session management features of AsyncPlaywrightCrawlerStrategy, including session cleanup through the kill_session method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]"
  },
  {
    "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "The 'word_count_threshold' argument in the 'arun' call associates with the 'ChunkingStrategy' which implements text chunking strategies pertinent in filtering the content based on word count.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The 'arun' method integrates chunking strategies to determine the application of the 'word_count_threshold', implying the usage of 'ChunkingStrategy' for such functionality.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "document_location": "docs/md_v2/basic/content-selection.md",
    "artifact_id": 21,
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "The behavior to exclude tags and filter media links as described in the snippet likely involves 'ExtractionStrategy'. These strategies are abstracted under the 'AsyncWebCrawler' to manage content extraction.",
    "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> ExtractionStrategy",
    "predicted_trace_chain_explanation": "While the snippet does not directly mention extractions, the functionality to exclude certain tags aligns with methods within 'ExtractionStrategy,' which 'AsyncWebCrawler.arun()' would utilize.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content "
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 15,
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "implicit",
    "relationship_type": "accesses",
    "relationship_explanation": "The 'content' in the result returned by the example corresponds to 'result.markdown', showing interaction with the markdown attribute of a CrawlResult instance.",
    "predicted_trace_chain": "browser-config.md -> CrawlResult -> CrawlResult.markdown",
    "predicted_trace_chain_explanation": "CrawlResult is integral in providing the data structure from which markdown content is extracted as depicted in the code example of the documentation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "markdown: Optional[str] = None"
  },
  {
    "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "document_location": "docs/md_v2/extraction/overview.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "access",
    "relationship_explanation": "The documentation implies accessing extracted content via 'result.extracted_content', hinting that it is part of a CrawlResult, which aligns with 'CrawlResult.html'.",
    "predicted_trace_chain": "overview.md -> CrawlResult -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The documentation steps mention processing results from AsyncWebCrawler.arun(), implying access to result properties like html, thus bridging execution back to CrawlResult's html property.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "The `AsyncWebCrawler` uses a strategy pattern for crawling which defaults to `AsyncPlaywrightCrawlerStrategy`, extending `AsyncCrawlerStrategy`. This is implied as the base abstraction which provides the contract that `AsyncPlaywrightCrawlerStrategy` implements, vital for creating the crawler context.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "The documentation explains an instance of `AsyncWebCrawler` which internally uses `AsyncPlaywrightCrawlerStrategy` by default, hence extending from `AsyncCrawlerStrategy`, showing a necessary inheritance path.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 1,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "default_strategy",
    "relationship_explanation": "The `AsyncWebCrawler` by default uses `AsyncPlaywrightCrawlerStrategy` for the crawling strategy. While not explicitly stated, this class serves as an implicit default strategy implementation when initiating the `AsyncWebCrawler` instance.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "predicted_trace_chain_explanation": "When an `AsyncWebCrawler` instance is created, it uses `AsyncPlaywrightCrawlerStrategy` as the default crawling strategy, making this relationship an implicit default setup in the code structure.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()"
  },
  {
    "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "implicit",
    "relationship_type": "submethod",
    "relationship_explanation": "The `kill_session()` method is an internal component that may be used by the class's broader session management features, which would relate to handling dynamic content in a way that involves session control.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_trace_chain_explanation": "Starting from `browser-config.md`, the `AsyncWebCrawler` class uses `AsyncPlaywrightCrawlerStrategy`, which in turn manages sessions using `kill_session()`. Each step is necessary to trace how session termination could relate to dynamic content processing ensuring resources are properly managed.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]"
  },
  {
    "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 3,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "implicit",
    "relationship_type": "integrates_with",
    "relationship_explanation": "The `set_hook()` method in `AsyncPlaywrightCrawlerStrategy` allows custom hooks which are important in managing events during crawling, potentially including events for dynamic content handling as shown in documentation.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_trace_chain_explanation": "The documentation implicitly uses hooks in `arun()`, which would rely on `set_hook()` methods in its strategy class (`AsyncPlaywrightCrawlerStrategy`) to manage dynamic content-based events, thus forming a complete pathway from configurations to implementation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\")"
  },
  {
    "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 8,
    "artifact_title": "CrawlResult",
    "predicted_relationship": "implicit",
    "relationship_type": "returns",
    "relationship_explanation": "The CrawlResult class is implicitly used as the returned type of AsyncWebCrawler.arun(), as evidenced by accessing 'result.extracted_content'.",
    "predicted_trace_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
    "predicted_trace_chain_explanation": "The arun() method returns a CrawlResult object, which is then used to access extracted content in the example.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} screenshot: Optional[str] = None markdown: Optional[str] = None fit_markdown: Optional[str] = None fit_html: Optional[str] = None extracted_content: Optional[str] = None metadata: Optional[dict] = None error_message: Optional[str] = None session_id: Optional[str] = None response_headers: Optional[dict] = None status_code: Optional[int] = None"
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "attribute access",
    "relationship_explanation": "Although not mentioned explicitly, the code example accesses the 'markdown' attribute of the CrawlResult, indicating its role in processing the result of arun().",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The pathway starts from the documentation example, following the route of AsyncWebCrawler's arun() usage, which returns a CrawlResult object with accessible attributes like 'markdown'.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extending",
    "relationship_explanation": "While 'LLMExtractionStrategy' is not directly referenced in the snippet, since 'CosineStrategy' is a specific type of 'ExtractionStrategy' and the documentation refers to it within a broader content extraction context, inferring 'LLMExtractionStrategy' as an analogous strategy class involved in this domain.",
    "predicted_trace_chain": "cosine.md -> ExtractionStrategy -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "The mention of various strategies in 'cosine.md' implies the role of base and derived strategy implementations. Hence, 'LLMExtractionStrategy', as an implementation or extension of 'ExtractionStrategy', can be seen as integrally related along with 'CosineStrategy'.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 5,
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "implicit",
    "relationship_type": "demonstrates usage",
    "relationship_explanation": "Although not explicitly mentioned, the context manager in the example prepares for actions that would utilize the `arun` method for web crawling operations. The setup implies using AsyncWebCrawler's crawling capabilities, facilitated by `arun()`.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "predicted_trace_chain_explanation": "The document suggests using the crawler, implicitly linking to the method `arun` that performs the key functionality of crawling in `AsyncWebCrawler`. The usage context logically extends from `AsyncWebCrawler` to `arun()` as the expected operation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) "
  },
  {
    "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 1,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "Although not directly mentioned in the snippet, 'CosineStrategy' is likely used in conjunction with 'AsyncPlaywrightCrawlerStrategy' since strategies would be implemented within a broader crawling framework.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> AsyncPlaywrightCrawlerStrategy",
    "predicted_trace_chain_explanation": "The document 'cosine.md' leading to 'CosineStrategy' suggests the strategy's use in broader crawling contexts, like those managed by 'AsyncPlaywrightCrawlerStrategy'. This helps in understanding the application context for the strategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()"
  },
  {
    "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The need to traverse through and extract structured data hints towards asynchronous crawling capabilities, which can be enabled by strategies like 'AsyncCrawlerStrategy' that aid in handling such operations efficiently.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> ExtractionStrategy -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "The schema in the document suggests the necessity for crawling strategies like 'AsyncCrawlerStrategy', highlighting its role in the broader process of extracting structured data via an asynchronous approach, thus supporting the extraction strategies within a larger workflow.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "Although not directly named, LLMExtractionStrategy could be inferred as used within the crawling process for handling dynamic content extraction as the documentation involves complex interactions with JavaScript and potentially extracting structured content, which is typical of what LLMs are used for.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "Based on the advanced configuration settings that include JavaScript execution and dynamic content handling mentioned in the doc, it can be inferred that a powerful extraction strategy like LLMExtractionStrategy might be employed internally by AsyncWebCrawler.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 23,
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "JsonCssExtractionStrategy might be used implicitly for handling CSS selector-based content fetching demonstrated in the 'wait_for' parameter in the code snippet, which suggests waiting for a specific dynamic content area to load.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
    "predicted_trace_chain_explanation": "The documentation includes waiting for a CSS selector indicating its relevance in content extraction scenarios. The strategy using CSS selectors underlies functionality such as that described in the documentation snippet.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): super().__init__(**kwargs) self.schema = schema def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: soup = BeautifulSoup(html, 'html.parser') base_elements = soup.select(self.schema['baseSelector']) results = [] for element in base_elements: item = self._extract_item(element, self.schema['fields']) if item: results.append(item) return results"
  },
  {
    "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 5,
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The mention of handling dynamic content and checking the `result.success` flag implicitly suggests the use of `AsyncWebCrawler.arun()` which performs such actions as part of the crawling and extraction process.",
    "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
    "predicted_trace_chain_explanation": "The `AsyncWebCrawler.arun()` method likely executes strategies like `JsonCssExtractionStrategy` and handles dynamic content and result success checks, which are mentioned as best practices in the documentation.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG]  Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) "
  },
  {
    "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 4,
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "explicit",
    "relationship_type": "implements",
    "relationship_explanation": "Although not directly mentioned, the context of guidance on using strategies for extracting content from dynamic pages implicitly requires the functionality provided by `AsyncWebCrawler`, particularly through the orchestration of different strategies.",
    "predicted_trace_chain": "css.md -> AsyncWebCrawler",
    "predicted_trace_chain_explanation": "The documentation on working with `JsonCssExtractionStrategy` and handling dynamic content requires encapsulating strategies and orchestration which the `AsyncWebCrawler` provides at a higher level. This lists the base mechanism through which strategies are executed.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()"
  },
  {
    "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "document_location": "docs/md_v2/extraction/llm.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "implicit",
    "relationship_type": "potential use",
    "relationship_explanation": "While `kill_session()` is not explicitly mentioned, session management as detailed in documentation could imply its utility in real applications, particularly when handling failed retry attempts.",
    "predicted_trace_chain": "llm.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_trace_chain_explanation": "In scenarios where sessions need to be managed or terminated due to retries or failures, the chain from general crawling (via `AsyncWebCrawler`) to session operations in `AsyncPlaywrightCrawlerStrategy` might engage `kill_session()`.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]"
  },
  {
    "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "document_location": "docs/md_v2/advanced/magic-mode.md",
    "artifact_id": 3,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The use of hooks such as 'remove_overlay_elements' suggests that set_hook might be invoked to modify browser behavior according to given options.",
    "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_trace_chain_explanation": "The ability to modify behavior by setting hooks is indirectly demonstrated by the usage patterns in arun(), which likely involves set_hook operations when executing configurations like 'remove_overlay_elements'.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\")"
  },
  {
    "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 14,
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The usage example in the documentation refers to the 'CrawlResult' object returned by the 'arun' method to access image media and their metadata. Although 'links' as an attribute isn't directly used, it implies interaction with CrawlResult's structure regarding media information.",
    "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
    "predicted_trace_chain_explanation": "The trace starts with the use of the AsyncWebCrawler's 'arun' method which returns a CrawlResult, which inherently includes access to links or media-related elements as depicted in the example. This implicit pathway shows how 'arun' connects its results to CrawlResult properties.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "links: Dict[str, List[Dict]] = {}"
  },
  {
    "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "document_location": "docs/md_v2/advanced/content-processing.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "The Lazy-Loaded content handling by setting CSS selectors and waiting implies a connection to chunking strategies possibly used in AsyncWebCrawler. Although not directly mentioned, this background use case infers involvement of chunking strategy for organizing how data is crawled and processed.",
    "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The documentation points towards a sophisticated handling of webpage content, implicitly signaling the use of strategies like chunking to manage loading and processing efficiently. Starting with AsyncWebCrawler which uses AsyncCrawlerStrategy and then further leading to ChunkingStrategy which handles chunking of content during crawling.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Statement-level",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "Although not explicitly mentioned, the tip on combining with JS execution for handling dynamic content implies a broader crawling mechanism, likely implementing the 'AsyncCrawlerStrategy'.",
    "predicted_trace_chain": "css.md -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "The document hints at broader crawling functionalities that are typically offered by extending 'AsyncCrawlerStrategy', necessary for full-featured web scraping operations.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 4,
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'handle dynamic content' and 'error handling' tips suggest the use of a crawler strategy, which is facilitated by 'AsyncWebCrawler', a class employing various strategies including JSON extraction.",
    "predicted_trace_chain": "css.md -> AsyncWebCrawler",
    "predicted_trace_chain_explanation": "The documentation refers to capabilities that involve managing crawl sessions and error handling, typical uses of 'AsyncWebCrawler', which integrates 'JsonCssExtractionStrategy' to perform these actions.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, always_by_pass_cache: bool = False, base_directory: str = str(Path.home()), **kwargs, ): self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( **kwargs ) self.always_by_pass_cache = always_by_pass_cache # self.crawl4ai_folder = os.path.join(Path.home(), \".crawl4ai\") self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\") os.makedirs(self.crawl4ai_folder, exist_ok=True) os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True) self.ready = False self.verbose = kwargs.get(\"verbose\", False) async def __aenter__(self): await self.crawler_strategy.__aenter__() await self.awarmup() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) async def awarmup(self): if self.verbose: print(\"[LOG] Warming up the AsyncWebCrawler\") await async_db_manager.ainit_db() await self.arun( url=\"https://google.com/\", word_count_threshold=5, bypass_cache=False, verbose=False, ) self.ready = True if self.verbose: print(\"[LOG] AsyncWebCrawler is ready to crawl\") async def arun( self, url: str, word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> CrawlResult: try: extraction_strategy = extraction_strategy or NoExtractionStrategy() extraction_strategy.verbose = verbose if not isinstance(extraction_strategy, ExtractionStrategy): raise ValueError(\"Unsupported extraction strategy\") if not isinstance(chunking_strategy, ChunkingStrategy): raise ValueError(\"Unsupported chunking strategy\") word_count_threshold = max(word_count_threshold, MIN_WORD_THRESHOLD) async_response: AsyncCrawlResponse = None cached = None screenshot_data = None extracted_content = None if not bypass_cache and not self.always_by_pass_cache: cached = await async_db_manager.aget_cached_url(url) if kwargs.get(\"warmup\", True) and not self.ready: return None if cached: html = sanitize_input_encode(cached[1]) extracted_content = sanitize_input_encode(cached[4]) if screenshot: screenshot_data = cached[9] if not screenshot_data: cached = None if not cached or not html: t1 = time.time() if user_agent: self.crawler_strategy.update_user_agent(user_agent) async_response: AsyncCrawlResponse = await self.crawler_strategy.crawl(url, screenshot=screenshot, **kwargs) html = sanitize_input_encode(async_response.html) screenshot_data = async_response.screenshot t2 = time.time() if verbose: print( f\"[LOG] Crawling done for {url}, success: {bool(html)}, time taken: {t2 - t1:.2f} seconds\" ) crawl_result = await self.aprocess_html( url, html, extracted_content, word_count_threshold, extraction_strategy, chunking_strategy, css_selector, screenshot_data, verbose, bool(cached), async_response=async_response, **kwargs, ) crawl_result.status_code = async_response.status_code if async_response else 200 crawl_result.response_headers = async_response.response_headers if async_response else {} crawl_result.success = bool(html) crawl_result.session_id = kwargs.get(\"session_id\", None) return crawl_result except Exception as e: if not hasattr(e, \"msg\"): e.msg = str(e) print(f\"[ERROR]  arun(): Failed to crawl {url}, error: {e.msg}\") return CrawlResult(url=url, html=\"\", markdown = f\"[ERROR] arun(): Failed to crawl {url}, error: {e.msg}\", success=False, error_message=e.msg) async def arun_many( self, urls: List[str], word_count_threshold=MIN_WORD_THRESHOLD, extraction_strategy: ExtractionStrategy = None, chunking_strategy: ChunkingStrategy = RegexChunking(), bypass_cache: bool = False, css_selector: str = None, screenshot: bool = False, user_agent: str = None, verbose=True, **kwargs, ) -> List[CrawlResult]: tasks = [ self.arun( url, word_count_threshold, extraction_strategy, chunking_strategy, bypass_cache, css_selector, screenshot, user_agent, verbose, **kwargs ) for url in urls ] return await asyncio.gather(*tasks) async def aprocess_html( self, url: str, html: str, extracted_content: str, word_count_threshold: int, extraction_strategy: ExtractionStrategy, chunking_strategy: ChunkingStrategy, css_selector: str, screenshot: str, verbose: bool, is_cached: bool, **kwargs, ) -> CrawlResult: t = time.time() # Extract content from HTML try: t1 = time.time() scrapping_strategy = WebScrappingStrategy() # result = await scrapping_strategy.ascrap( result = scrapping_strategy.scrap( url, html, word_count_threshold=word_count_threshold, css_selector=css_selector, only_text=kwargs.get(\"only_text\", False), image_description_min_word_threshold=kwargs.get( \"image_description_min_word_threshold\", IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD ), **kwargs, ) if verbose: print( f\"[LOG]  Content extracted for {url}, success: True, time taken: {time.time() - t1:.2f} seconds\" ) if result is None: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}\") except InvalidCSSSelectorError as e: raise ValueError(str(e)) except Exception as e: raise ValueError(f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\") cleaned_html = sanitize_input_encode(result.get(\"cleaned_html\", \"\")) markdown = sanitize_input_encode(result.get(\"markdown\", \"\")) fit_markdown = sanitize_input_encode(result.get(\"fit_markdown\", \"\")) fit_html = sanitize_input_encode(result.get(\"fit_html\", \"\")) media = result.get(\"media\", []) links = result.get(\"links\", []) metadata = result.get(\"metadata\", {}) if extracted_content is None and extraction_strategy and chunking_strategy: if verbose: print( f\"[LOG] Extracting semantic blocks for {url}, Strategy: {self.__class__.__name__}\" ) # Check if extraction strategy is type of JsonCssExtractionStrategy if isinstance(extraction_strategy, JsonCssExtractionStrategy) or isinstance(extraction_strategy, JsonCssExtractionStrategy): extraction_strategy.verbose = verbose extracted_content = extraction_strategy.run(url, [html]) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) else: sections = chunking_strategy.chunk(markdown) extracted_content = extraction_strategy.run(url, sections) extracted_content = json.dumps(extracted_content, indent=4, default=str, ensure_ascii=False) if verbose: print( f\"[LOG]  Extraction done for {url}, time taken: {time.time() - t:.2f} seconds.\" ) screenshot = None if not screenshot else screenshot if not is_cached: await async_db_manager.acache_url( url, html, cleaned_html, markdown, extracted_content, True, json.dumps(media), json.dumps(links), json.dumps(metadata), screenshot=screenshot, ) return CrawlResult( url=url, html=html, cleaned_html=format_html(cleaned_html), markdown=markdown, fit_markdown=fit_markdown, fit_html= fit_html, media=media, links=links, metadata=metadata, screenshot=screenshot, extracted_content=extracted_content, success=True, error_message=\"\", ) async def aclear_cache(self): await async_db_manager.aclear_db() async def aflush_cache(self): await async_db_manager.aflush_db() async def aget_cache_size(self): return await async_db_manager.aget_total_count()"
  },
  {
    "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "document_location": "docs/md_v2/basic/page-interaction.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'kill_session' method is used internally within 'AsyncPlaywrightCrawlerStrategy' to manage session-related functionality, which supports operations like those shown in 'arun' within an example of dynamic content handling.",
    "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_trace_chain_explanation": "The session management is critical for executing multiple JavaScript operations during crawling, implicitly related to 'arun' through 'AsyncPlaywrightCrawlerStrategy'.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]"
  },
  {
    "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "utilizes",
    "relationship_explanation": "The example checks for `result.success` and `result.screenshot`, implying the use of `CrawlResult` attributes like `html` indirectly when accessing the overall `CrawlResult` object returned by `arun()`.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The `quickstart.md` demonstrates capturing a screenshot where the `CrawlResult` object fields like `html` and `screenshot` are integral to path completion by providing the final results from `arun()` method.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The documentation indirectly references the results of a crawl operation (`result1.markdown[:100]`) which would be part of the 'CrawlResult' data structure. While not explicitly named in the snippet, the example implies the use of 'extracted_content' or similar derived content from the 'CrawlResult' object.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The chaining shows that after using the 'AsyncWebCrawler' and its 'arun' method, the result is processed to access its representation, highlighting 'CrawlResult' and associated functionality, pointing towards data storage and retrieval within a web crawl response.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The parameter 'word_count_threshold' is used in the example, which is often related to chunking strategies that define behavior based on word count, such as ChunkingStrategy implementations.",
    "predicted_trace_chain": "simple-crawling.md -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The usage of 'word_count_threshold' indirectly indicates ChunkingStrategy, as chunking strategies utilize such parameters to determine text segmentation sizes.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 21,
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The documentation mentions content filtering and processing features which are typically handled by extraction strategies, suggesting the use of an ExtractionStrategy.",
    "predicted_trace_chain": "simple-crawling.md -> ExtractionStrategy",
    "predicted_trace_chain_explanation": "Features such as content filtering ('extracted_content') indicate reliance on extraction strategies for processing content during crawling.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ExtractionStrategy(ABC): \"\"\" Abstract base class for all extraction strategies. \"\"\" def __init__(self, **kwargs): self.DEL = \"<|DEL|>\" self.name = self.__class__.__name__ self.verbose = kwargs.get(\"verbose\", False) @abstractmethod def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Extract meaningful blocks or chunks from the given HTML. :param url: The URL of the webpage. :param html: The HTML content of the webpage. :return: A list of extracted blocks or chunks. \"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]: \"\"\" Process sections of text in parallel by default. :param url: The URL of the webpage. :param sections: List of sections (strings) to process. :return: A list of processed JSON blocks. \"\"\" extracted_content = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(self.extract, url, section, **kwargs) for section in sections] for future in as_completed(futures): extracted_content.extend(future.result()) return extracted_content "
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 1,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "Implicitly used as part of the `AsyncWebCrawler` initialization, which creates an instance of `AsyncPlaywrightCrawlerStrategy` when no strategy is provided. This is inferred from the `AsyncWebCrawler`'s constructor logic, suggesting the strategy's dependency.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "predicted_trace_chain_explanation": "The `AsyncWebCrawler` implicitly uses `AsyncPlaywrightCrawlerStrategy` if no other strategy is provided, demonstrating that the strategy is a critical part of the overall crawling setup within the context of the described functionality.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()"
  },
  {
    "sent_document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "document_location": "docs/md_v2/extraction/cosine.md",
    "artifact_id": 6,
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "The 'CosineStrategy' is expected to involve chunking page content as one of its first steps. 'ChunkingStrategy' as a base class is likely providing the interface or methods for this operation, implicating a relationship as a superclass.",
    "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
    "predicted_trace_chain_explanation": "The 'cosine.md' relates to 'CosineStrategy', which likely uses chunking as described in the document. This suggests that 'CosineStrategy' might depend on 'ChunkingStrategy', establishing an inheritance path.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -> list: \"\"\" Abstract method to chunk the given text. \"\"\" pass"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "While not directly mentioned, `AsyncWebCrawler` relies on `AsyncCrawlerStrategy` or a derived strategy to function, because `AsyncWebCrawler` is designed to work with a strategy object, and `AsyncCrawlerStrategy` provides the interface for such strategies.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "The `AsyncWebCrawler` is designed to be used with a crawler strategy (`AsyncCrawlerStrategy` or derivatives like `AsyncPlaywrightCrawlerStrategy`). The trace chain reflects that although `AsyncCrawlerStrategy` is not directly mentioned in the doc snippet, it forms part of the underlying implementation that `AsyncWebCrawler` depends on.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 1,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "implements",
    "relationship_explanation": "The `AsyncWebCrawler` by default uses `AsyncPlaywrightCrawlerStrategy` as the strategy if none is passed, indicated by its constructor initialization in the `AsyncWebCrawler` class. This makes `AsyncPlaywrightCrawlerStrategy` an implicit part of the user flow when following the quickstart guide.",
    "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
    "predicted_trace_chain_explanation": "When following the usage pattern of `AsyncWebCrawler` in the quickstart guide, the default behavior includes using `AsyncPlaywrightCrawlerStrategy`, connecting the quickstart documentation, `AsyncWebCrawler`, and its default strategy.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): def __init__(self, use_cached_html=False, js_code=None, **kwargs): self.use_cached_html = use_cached_html self.user_agent = kwargs.get( \"user_agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" ) self.proxy = kwargs.get(\"proxy\") self.proxy_config = kwargs.get(\"proxy_config\") self.headless = kwargs.get(\"headless\", True) self.browser_type = kwargs.get(\"browser_type\", \"chromium\") self.headers = kwargs.get(\"headers\", {}) self.sessions = {} self.session_ttl = 1800 self.js_code = js_code self.verbose = kwargs.get(\"verbose\", False) self.playwright = None self.browser = None self.sleep_on_close = kwargs.get(\"sleep_on_close\", False) self.hooks = { 'on_browser_created': None, 'on_user_agent_updated': None, 'on_execution_started': None, 'before_goto': None, 'after_goto': None, 'before_return_html': None, 'before_retrieve_html': None } async def __aenter__(self): await self.start() return self async def __aexit__(self, exc_type, exc_val, exc_tb): await self.close() async def start(self): if self.playwright is None: self.playwright = await async_playwright().start() if self.browser is None: browser_args = { \"headless\": self.headless, \"args\": [ \"--disable-gpu\", \"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-blink-features=AutomationControlled\", \"--disable-infobars\", \"--window-position=0,0\", \"--ignore-certificate-errors\", \"--ignore-certificate-errors-spki-list\", # \"--headless=new\", # Use the new headless mode ] } # Add proxy settings if a proxy is specified if self.proxy: proxy_settings = ProxySettings(server=self.proxy) browser_args[\"proxy\"] = proxy_settings elif self.proxy_config: proxy_settings = ProxySettings(server=self.proxy_config.get(\"server\"), username=self.proxy_config.get(\"username\"), password=self.proxy_config.get(\"password\")) browser_args[\"proxy\"] = proxy_settings # Select the appropriate browser based on the browser_type if self.browser_type == \"firefox\": self.browser = await self.playwright.firefox.launch(**browser_args) elif self.browser_type == \"webkit\": self.browser = await self.playwright.webkit.launch(**browser_args) else: self.browser = await self.playwright.chromium.launch(**browser_args) await self.execute_hook('on_browser_created', self.browser) async def close(self): if self.sleep_on_close: await asyncio.sleep(0.5) if self.browser: await self.browser.close() self.browser = None if self.playwright: await self.playwright.stop() self.playwright = None def __del__(self): if self.browser or self.playwright: asyncio.get_event_loop().run_until_complete(self.close()) def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\") async def execute_hook(self, hook_type: str, *args): hook = self.hooks.get(hook_type) if hook: if asyncio.iscoroutinefunction(hook): return await hook(*args) else: return hook(*args) return args[0] if args else None def update_user_agent(self, user_agent: str): self.user_agent = user_agent def set_custom_headers(self, headers: Dict[str, str]): self.headers = headers async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id] def _cleanup_expired_sessions(self): current_time = time.time() expired_sessions = [ sid for sid, (_, _, last_used) in self.sessions.items() if current_time - last_used > self.session_ttl ] for sid in expired_sessions: asyncio.create_task(self.kill_session(sid)) async def smart_wait(self, page: Page, wait_for: str, timeout: float = 30000): wait_for = wait_for.strip() if wait_for.startswith('js:'): # Explicitly specified JavaScript js_code = wait_for[3:].strip() return await self.csp_compliant_wait(page, js_code, timeout) elif wait_for.startswith('css:'): # Explicitly specified CSS selector css_selector = wait_for[4:].strip() try: await page.wait_for_selector(css_selector, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{css_selector}'\") else: raise ValueError(f\"Invalid CSS selector: '{css_selector}'\") else: # Auto-detect based on content if wait_for.startswith('()') or wait_for.startswith('function'): # It's likely a JavaScript function return await self.csp_compliant_wait(page, wait_for, timeout) else: # Assume it's a CSS selector first try: await page.wait_for_selector(wait_for, timeout=timeout) except Error as e: if 'Timeout' in str(e): raise TimeoutError(f\"Timeout after {timeout}ms waiting for selector '{wait_for}'\") else: # If it's not a timeout error, it might be an invalid selector # Let's try to evaluate it as a JavaScript function as a fallback try: return await self.csp_compliant_wait(page, f\"() => {{{wait_for}}}\", timeout) except Error: raise ValueError(f\"Invalid wait_for parameter: '{wait_for}'. \" \"It should be either a valid CSS selector, a JavaScript function, \" \"or explicitly prefixed with 'js:' or 'css:'.\") async def csp_compliant_wait(self, page: Page, user_wait_function: str, timeout: float = 30000): wrapper_js = f\"\"\" async () => {{ const userFunction = {user_wait_function}; const startTime = Date.now(); while (true) {{ if (await userFunction()) {{ return true; }} if (Date.now() - startTime > {timeout}) {{ throw new Error('Timeout waiting for condition'); }} await new Promise(resolve => setTimeout(resolve, 100)); }} }} \"\"\" try: await page.evaluate(wrapper_js) except TimeoutError: raise TimeoutError(f\"Timeout after {timeout}ms waiting for condition\") except Exception as e: raise RuntimeError(f\"Error in wait condition: {str(e)}\") async def process_iframes(self, page): # Find all iframes iframes = await page.query_selector_all('iframe') for i, iframe in enumerate(iframes): try: # Add a unique identifier to the iframe await iframe.evaluate(f'(element) => element.id = \"iframe-{i}\"') # Get the frame associated with this iframe frame = await iframe.content_frame() if frame: # Wait for the frame to load await frame.wait_for_load_state('load', timeout=30000) # 30 seconds timeout # Extract the content of the iframe's body iframe_content = await frame.evaluate('() => document.body.innerHTML') # Generate a unique class name for this iframe class_name = f'extracted-iframe-content-{i}' # Replace the iframe with a div containing the extracted content _iframe = iframe_content.replace('`', '\\`') await page.evaluate(f\"\"\" () => {{ const iframe = document.getElementById('iframe-{i}'); const div = document.createElement('div'); div.innerHTML = `{_iframe}`; div.className = '{class_name}'; iframe.replaceWith(div); }} \"\"\") else: print(f\"Warning: Could not access content frame for iframe {i}\") except Exception as e: print(f\"Error processing iframe {i}: {str(e)}\") # Return the page object return page async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: response_headers = {} status_code = None self._cleanup_expired_sessions() session_id = kwargs.get(\"session_id\") if session_id: context, page, _ = self.sessions.get(session_id, (None, None, None)) if not context: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None, accept_downloads=True, java_script_enabled=True ) await context.add_cookies([{\"name\": \"cookiesEnabled\", \"value\": \"true\", \"url\": url}]) await context.set_extra_http_headers(self.headers) page = await context.new_page() self.sessions[session_id] = (context, page, time.time()) else: context = await self.browser.new_context( user_agent=self.user_agent, viewport={\"width\": 1920, \"height\": 1080}, proxy={\"server\": self.proxy} if self.proxy else None ) await context.set_extra_http_headers(self.headers) if kwargs.get(\"override_navigator\", False) or kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Inject scripts to override navigator properties await context.add_init_script(\"\"\" // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome = { runtime: {}, // Add other properties if necessary }; Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'], }); Object.defineProperty(document, 'hidden', { get: () => false }); Object.defineProperty(document, 'visibilityState', { get: () => 'visible' }); \"\"\") page = await context.new_page() # await stealth_async(page) #, stealth_config) # Add console message and error logging if kwargs.get(\"log_console\", False): page.on(\"console\", lambda msg: print(f\"Console: {msg.text}\")) page.on(\"pageerror\", lambda exc: print(f\"Page Error: {exc}\")) try: if self.verbose: print(f\"[LOG] \ud83d\udd78\ufe0f Crawling {url} using AsyncPlaywrightCrawlerStrategy...\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) if os.path.exists(cache_file_path): html = \"\" with open(cache_file_path, \"r\") as f: html = f.read() # retrieve response headers and status code from cache with open(cache_file_path + \".meta\", \"r\") as f: meta = json.load(f) response_headers = meta.get(\"response_headers\", {}) status_code = meta.get(\"status_code\") response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code ) return response if not kwargs.get(\"js_only\", False): await self.execute_hook('before_goto', page) response = await page.goto( url, wait_until=\"domcontentloaded\", timeout=kwargs.get(\"page_timeout\", 60000) ) # response = await page.goto(\"about:blank\") # await page.evaluate(f\"window.location.href = '{url}'\") await self.execute_hook('after_goto', page) # Get status code and headers status_code = response.status response_headers = response.headers else: status_code = 200 response_headers = {} await page.wait_for_selector('body') await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\") js_code = kwargs.get(\"js_code\", kwargs.get(\"js\", self.js_code)) if js_code: if isinstance(js_code, str): await page.evaluate(js_code) elif isinstance(js_code, list): for js in js_code: await page.evaluate(js) await page.wait_for_load_state('networkidle') # Check for on execution event await self.execute_hook('on_execution_started', page) if kwargs.get(\"simulate_user\", False) or kwargs.get(\"magic\", False): # Simulate user interactions await page.mouse.move(100, 100) await page.mouse.down() await page.mouse.up() await page.keyboard.press('ArrowDown') # Handle the wait_for parameter wait_for = kwargs.get(\"wait_for\") if wait_for: try: await self.smart_wait(page, wait_for, timeout=kwargs.get(\"page_timeout\", 60000)) except Exception as e: raise RuntimeError(f\"Wait condition failed: {str(e)}\") # Update image dimensions update_image_dimensions_js = \"\"\" () => { return new Promise((resolve) => { const filterImage = (img) => { // Filter out images that are too small if (img.width < 100 && img.height < 100) return false; // Filter out images that are not visible const rect = img.getBoundingClientRect(); if (rect.width === 0 || rect.height === 0) return false; // Filter out images with certain class names (e.g., icons, thumbnails) if (img.classList.contains('icon') || img.classList.contains('thumbnail')) return false; // Filter out images with certain patterns in their src (e.g., placeholder images) if (img.src.includes('placeholder') || img.src.includes('icon')) return false; return true; }; const images = Array.from(document.querySelectorAll('img')).filter(filterImage); let imagesLeft = images.length; if (imagesLeft === 0) { resolve(); return; } const checkImage = (img) => { if (img.complete && img.naturalWidth !== 0) { img.setAttribute('width', img.naturalWidth); img.setAttribute('height', img.naturalHeight); imagesLeft--; if (imagesLeft === 0) resolve(); } }; images.forEach(img => { checkImage(img); if (!img.complete) { img.onload = () => { checkImage(img); }; img.onerror = () => { imagesLeft--; if (imagesLeft === 0) resolve(); }; } }); // Fallback timeout of 5 seconds // setTimeout(() => resolve(), 5000); resolve(); }); } \"\"\" await page.evaluate(update_image_dimensions_js) # Wait a bit for any onload events to complete await page.wait_for_timeout(100) # Process iframes if kwargs.get(\"process_iframes\", False): page = await self.process_iframes(page) await self.execute_hook('before_retrieve_html', page) # Check if delay_before_return_html is set then wait for that time delay_before_return_html = kwargs.get(\"delay_before_return_html\") if delay_before_return_html: await asyncio.sleep(delay_before_return_html) # Check for remove_overlay_elements parameter if kwargs.get(\"remove_overlay_elements\", False): await self.remove_overlay_elements(page) html = await page.content() await self.execute_hook('before_return_html', page, html) # Check if kwargs has screenshot=True then take screenshot screenshot_data = None if kwargs.get(\"screenshot\"): # Check we have screenshot_wait_for parameter, if we have simply wait for that time screenshot_wait_for = kwargs.get(\"screenshot_wait_for\") if screenshot_wait_for: await asyncio.sleep(screenshot_wait_for) screenshot_data = await self.take_screenshot(page) if self.verbose: print(f\"[LOG] Crawled {url} successfully!\") if self.use_cached_html: cache_file_path = os.path.join( Path.home(), \".crawl4ai\", \"cache\", hashlib.md5(url.encode()).hexdigest() ) with open(cache_file_path, \"w\", encoding=\"utf-8\") as f: f.write(html) # store response headers and status code in cache with open(cache_file_path + \".meta\", \"w\", encoding=\"utf-8\") as f: json.dump({ \"response_headers\": response_headers, \"status_code\": status_code }, f) async def get_delayed_content(delay: float = 5.0) -> str: if self.verbose: print(f\"[LOG] Waiting for {delay} seconds before retrieving content for {url}\") await asyncio.sleep(delay) return await page.content() response = AsyncCrawlResponse( html=html, response_headers=response_headers, status_code=status_code, screenshot=screenshot_data, get_delayed_content=get_delayed_content ) return response except Error as e: raise Error(f\"[ERROR]  crawl(): Failed to crawl {url}: {str(e)}\") # finally: # if not session_id: # await page.close() # await context.close() async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: semaphore_count = kwargs.get('semaphore_count', 5) # Adjust as needed semaphore = asyncio.Semaphore(semaphore_count) async def crawl_with_semaphore(url): async with semaphore: return await self.crawl(url, **kwargs) tasks = [crawl_with_semaphore(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return [result if not isinstance(result, Exception) else str(result) for result in results] async def remove_overlay_elements(self, page: Page) -> None: \"\"\" Removes popup overlays, modals, cookie notices, and other intrusive elements from the page. Args: page (Page): The Playwright page instance \"\"\" remove_overlays_js = \"\"\" async () => { // Function to check if element is visible const isVisible = (elem) => { const style = window.getComputedStyle(elem); return style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0'; }; // Common selectors for popups and overlays const commonSelectors = [ // Close buttons first 'button[class*=\"close\" i]', 'button[class*=\"dismiss\" i]', 'button[aria-label*=\"close\" i]', 'button[title*=\"close\" i]', 'a[class*=\"close\" i]', 'span[class*=\"close\" i]', // Cookie notices '[class*=\"cookie-banner\" i]', '[id*=\"cookie-banner\" i]', '[class*=\"cookie-consent\" i]', '[id*=\"cookie-consent\" i]', // Newsletter/subscription dialogs '[class*=\"newsletter\" i]', '[class*=\"subscribe\" i]', // Generic popups/modals '[class*=\"popup\" i]', '[class*=\"modal\" i]', '[class*=\"overlay\" i]', '[class*=\"dialog\" i]', '[role=\"dialog\"]', '[role=\"alertdialog\"]' ]; // Try to click close buttons first for (const selector of commonSelectors.slice(0, 6)) { const closeButtons = document.querySelectorAll(selector); for (const button of closeButtons) { if (isVisible(button)) { try { button.click(); await new Promise(resolve => setTimeout(resolve, 100)); } catch (e) { console.log('Error clicking button:', e); } } } } // Remove remaining overlay elements const removeOverlays = () => { // Find elements with high z-index const allElements = document.querySelectorAll('*'); for (const elem of allElements) { const style = window.getComputedStyle(elem); const zIndex = parseInt(style.zIndex); const position = style.position; if ( isVisible(elem) && (zIndex > 999 || position === 'fixed' || position === 'absolute') && ( elem.offsetWidth > window.innerWidth * 0.5 || elem.offsetHeight > window.innerHeight * 0.5 || style.backgroundColor.includes('rgba') || parseFloat(style.opacity) < 1 ) ) { elem.remove(); } } // Remove elements matching common selectors for (const selector of commonSelectors) { const elements = document.querySelectorAll(selector); elements.forEach(elem => { if (isVisible(elem)) { elem.remove(); } }); } }; // Remove overlay elements removeOverlays(); // Remove any fixed/sticky position elements at the top/bottom const removeFixedElements = () => { const elements = document.querySelectorAll('*'); elements.forEach(elem => { const style = window.getComputedStyle(elem); if ( (style.position === 'fixed' || style.position === 'sticky') && isVisible(elem) ) { elem.remove(); } }); }; removeFixedElements(); // Remove empty block elements as: div, p, span, etc. const removeEmptyBlockElements = () => { const blockElements = document.querySelectorAll('div, p, span, section, article, header, footer, aside, nav, main, ul, ol, li, dl, dt, dd, h1, h2, h3, h4, h5, h6'); blockElements.forEach(elem => { if (elem.innerText.trim() === '') { elem.remove(); } }); }; // Remove margin-right and padding-right from body (often added by modal scripts) document.body.style.marginRight = '0px'; document.body.style.paddingRight = '0px'; document.body.style.overflow = 'auto'; // Wait a bit for any animations to complete await new Promise(resolve => setTimeout(resolve, 100)); } \"\"\" try: await page.evaluate(remove_overlays_js) await page.wait_for_timeout(500) # Wait for any animations to complete except Exception as e: if self.verbose: print(f\"Warning: Failed to remove overlay elements: {str(e)}\") async def take_screenshot(self, page: Page) -> str: try: # The page is already loaded, just take the screenshot screenshot = await page.screenshot(full_page=True) return base64.b64encode(screenshot).decode('utf-8') except Exception as e: error_message = f\"Failed to take screenshot: {str(e)}\" print(error_message) # Generate an error image img = Image.new('RGB', (800, 600), color='black') draw = ImageDraw.Draw(img) font = ImageFont.load_default() draw.text((10, 10), error_message, fill=(255, 255, 255), font=font) buffered = BytesIO() img.save(buffered, format=\"JPEG\") return base64.b64encode(buffered.getvalue()).decode('utf-8') finally: await page.close()"
  },
  {
    "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "implicit",
    "relationship_type": "method reference",
    "relationship_explanation": "While not directly mentioned in the example, AsyncPlaywrightCrawlerStrategy.kill_session() is involved in session management which is part of the crawl operation lifecycle.",
    "predicted_trace_chain": "quickstart.md -> async_crawler_strategy.py -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_trace_chain_explanation": "Part of the web crawling operation involves session management handled by AsyncPlaywrightCrawlerStrategy, showing the connectivity in lifecycle handling.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]"
  },
  {
    "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "document_location": "docs/md_v2/basic/quickstart.md",
    "artifact_id": 3,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "implicit",
    "relationship_type": "method reference",
    "relationship_explanation": "Though not explicitly mentioned, AsyncPlaywrightCrawlerStrategy.set_hook() supports the flexibility shown in the snippet for managing customizable hooks during a crawl operation.",
    "predicted_trace_chain": "quickstart.md -> async_crawler_strategy.py -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_trace_chain_explanation": "The example implies potential use of hooks, which the method set_hook() facilitates, highlighting its indirect role in the flexibility of crawling operations.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Method",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\")"
  },
  {
    "sent_document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "document_location": "docs/md_v2/extraction/chunking.md",
    "artifact_id": 22,
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While `FixedLengthWordChunking` is not directly mentioned, `RegexChunking` implements `ChunkingStrategy`, and other strategies like `FixedLengthWordChunking` are likely to extend `ChunkingStrategy`. This implies a conceptual relationship as different implementation strategies for chunking.",
    "predicted_trace_chain": "chunking.md -> RegexChunking -> FixedLengthWordChunking",
    "predicted_trace_chain_explanation": "The documentation describes `RegexChunking`, a class implementing features of the chunking methodology. It suggests a broader context that includes other chunking implementations, leading to an inference of relationship to `FixedLengthWordChunking`, another strategy implementing similar principles.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class FixedLengthWordChunking(ChunkingStrategy): def __init__(self, chunk_size=100, **kwargs): \"\"\" Initialize the fixed-length word chunking strategy with the given chunk size. Args: chunk_size (int): The size of each chunk in words. \"\"\" self.chunk_size = chunk_size def chunk(self, text: str) -> list: words = text.split() return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]"
  },
  {
    "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "document_location": "docs/md_v2/basic/simple-crawling.md",
    "artifact_id": 2,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The documentation highlights options related to session handling, implicitly utilizing the `kill_session` method for managing sessions. This is inferred from the session and task cleanup operations that a strategy like `AsyncPlaywrightCrawlerStrategy` must support within `arun`.",
    "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_trace_chain_explanation": "`AsyncWebCrawler.arun()` interacts with session management indirectly through the strategy class methods such as `kill_session`. Even though not explicitly documented here, understanding session management is critical for hiding intricate details of session cleanup when custom crawl options are applied.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "async def kill_session(self, session_id: str): if session_id in self.sessions: context, page, _ = self.sessions[session_id] await page.close() await context.close() del self.sessions[session_id]"
  },
  {
    "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "document_location": "docs/md_v2/extraction/css.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not explicitly named, 'AsyncWebCrawler' utilizes 'JsonCssExtractionStrategy', which is a form of CrawlerStrategy. Given that 'AsyncWebCrawler' operates using strategies, and 'JsonCssExtractionStrategy' indirectly operates through it, 'AsyncCrawlerStrategy' is necessarily part of the implementation path.",
    "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "AsyncWebCrawler uses strategies to execute crawling tasks. As 'JsonCssExtractionStrategy' tied to AsyncWebCrawler implies a strategy-based execution, the base abstract class 'AsyncCrawlerStrategy' forms part of the hierarchical pathway, linking strategy usage in the AsyncWebCrawler.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 24,
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "While not directly mentioned in the documentation snippet, `LLMExtractionStrategy` is implicitly related as it extends the capabilities of extraction strategies which `JsonCssExtractionStrategy` belongs to. The complexity of the HTML described suggests the use of advanced extraction strategies like LLM for handling intricate data models.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> LLMExtractionStrategy",
    "predicted_trace_chain_explanation": "Starting with `JsonCssExtractionStrategy`, it\u2019s logical to consider `LLMExtractionStrategy` due to its advanced features that could complement the JSON-CSS extraction techniques.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, provider: str = DEFAULT_PROVIDER, api_token: Optional[str] = None, instruction:str = None, schema:Dict = None, extraction_type = \"block\", **kwargs): \"\"\" Initialize the strategy with clustering parameters. :param provider: The provider to use for extraction. :param api_token: The API token for the provider. :param instruction: The instruction to use for the LLM model. \"\"\" super().__init__() self.provider = provider self.api_token = api_token or PROVIDER_MODELS.get(provider, \"no-token\") or os.getenv(\"OPENAI_API_KEY\") self.instruction = instruction self.extract_type = extraction_type self.schema = schema if schema: self.extract_type = \"schema\" self.chunk_token_threshold = kwargs.get(\"chunk_token_threshold\", CHUNK_TOKEN_THRESHOLD) self.overlap_rate = kwargs.get(\"overlap_rate\", OVERLAP_RATE) self.word_token_rate = kwargs.get(\"word_token_rate\", WORD_TOKEN_RATE) self.apply_chunking = kwargs.get(\"apply_chunking\", True) self.base_url = kwargs.get(\"base_url\", None) self.api_base = kwargs.get(\"api_base\", kwargs.get(\"base_url\", None)) self.extra_args = kwargs.get(\"extra_args\", {}) if not self.apply_chunking: self.chunk_token_threshold = 1e9 self.verbose = kwargs.get(\"verbose\", False) if not self.api_token: raise ValueError(\"API token must be provided for LLMExtractionStrategy. Update the config.py or set OPENAI_API_KEY environment variable.\") def extract(self, url: str, ix:int, html: str) -> List[Dict[str, Any]]: # print(\"[LOG] Extracting blocks from URL:\", url) print(f\"[LOG] Call LLM for {url} - block index: {ix}\") variable_values = { \"URL\": url, \"HTML\": escape_json_string(sanitize_html(html)), } prompt_with_variables = PROMPT_EXTRACT_BLOCKS if self.instruction: variable_values[\"REQUEST\"] = self.instruction prompt_with_variables = PROMPT_EXTRACT_BLOCKS_WITH_INSTRUCTION if self.extract_type == \"schema\" and self.schema: variable_values[\"SCHEMA\"] = json.dumps(self.schema, indent=2) prompt_with_variables = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION for variable in variable_values: prompt_with_variables = prompt_with_variables.replace( \"{\" + variable + \"}\", variable_values[variable] ) response = perform_completion_with_backoff( self.provider, prompt_with_variables, self.api_token, base_url=self.api_base or self.base_url, extra_args = self.extra_args ) # , json_response=self.extract_type == \"schema\") try: blocks = extract_xml_data([\"blocks\"], response.choices[0].message.content)['blocks'] blocks = json.loads(blocks) for block in blocks: block['error'] = False except Exception as e: parsed, unparsed = split_and_parse_json_objects(response.choices[0].message.content) blocks = parsed if unparsed: blocks.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": unparsed }) if self.verbose: print(\"[LOG] Extracted\", len(blocks), \"blocks from URL:\", url, \"block index:\", ix) return blocks def _merge(self, documents, chunk_token_threshold, overlap): chunks = [] sections = [] total_tokens = 0 # Calculate the total tokens across all documents for document in documents: total_tokens += len(document.split(' ')) * self.word_token_rate # Calculate the number of sections needed num_sections = math.floor(total_tokens / chunk_token_threshold) if num_sections < 1: num_sections = 1 # Ensure there is at least one section adjusted_chunk_threshold = total_tokens / num_sections total_token_so_far = 0 current_chunk = [] for document in documents: tokens = document.split(' ') token_count = len(tokens) * self.word_token_rate if total_token_so_far + token_count <= adjusted_chunk_threshold: current_chunk.extend(tokens) total_token_so_far += token_count else: # Ensure to handle the last section properly if len(sections) == num_sections - 1: current_chunk.extend(tokens) continue # Add overlap if specified if overlap > 0 and current_chunk: overlap_tokens = current_chunk[-overlap:] current_chunk.extend(overlap_tokens) sections.append(' '.join(current_chunk)) current_chunk = tokens total_token_so_far = token_count # Add the last chunk if current_chunk: sections.append(' '.join(current_chunk)) return sections def run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]: \"\"\" Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy. \"\"\" merged_sections = self._merge( sections, self.chunk_token_threshold, overlap= int(self.chunk_token_threshold * self.overlap_rate) ) extracted_content = [] if self.provider.startswith(\"groq/\"): # Sequential processing with a delay for ix, section in enumerate(merged_sections): extract_func = partial(self.extract, url) extracted_content.extend(extract_func(ix, sanitize_input_encode(section))) time.sleep(0.5) # 500 ms delay between each processing else: # Parallel processing using ThreadPoolExecutor # extract_func = partial(self.extract, url) # for ix, section in enumerate(merged_sections): # extracted_content.append(extract_func(ix, section)) with ThreadPoolExecutor(max_workers=4) as executor: extract_func = partial(self.extract, url) futures = [executor.submit(extract_func, ix, sanitize_input_encode(section)) for ix, section in enumerate(merged_sections)] for future in as_completed(futures): try: extracted_content.extend(future.result()) except Exception as e: if self.verbose: print(f\"Error in thread execution: {e}\") # Add error information to extracted_content extracted_content.append({ \"index\": 0, \"error\": True, \"tags\": [\"error\"], \"content\": str(e) }) return extracted_content"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 11,
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The method 'arun' returns a 'CrawlResult' object, which contains the 'extracted_content' attribute accessed in the example function.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
    "predicted_trace_chain_explanation": "The 'arun' method returns a 'CrawlResult', inside which 'extracted_content' is called implicitly in handling the crawling result.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "extracted_content: Optional[str] = None"
  },
  {
    "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "document_location": "docs/md_v2/basic/browser-config.md",
    "artifact_id": 13,
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The 'arun' method's result could indirectly involve the 'html' attribute of 'CrawlResult'.",
    "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
    "predicted_trace_chain_explanation": "The 'html' content within 'CrawlResult' is likely used implicitly when the result is processed in the example.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "html: str"
  },
  {
    "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "document_location": "docs/md_v2/extraction/css-advanced.md",
    "artifact_id": 0,
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "implicit",
    "relationship_type": "extends",
    "relationship_explanation": "JsonCssExtractionStrategy, by dealing with data extraction, implicitly relies on overall crawler strategies, such as 'AsyncCrawlerStrategy', for retrieving the content to be processed.",
    "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> AsyncCrawlerStrategy",
    "predicted_trace_chain_explanation": "JsonCssExtractionStrategy likely operates on data provided by a broader crawler strategy like AsyncCrawlerStrategy, establishing a chain of reliance.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "class AsyncCrawlerStrategy(ABC): @abstractmethod async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse: pass @abstractmethod async def crawl_many(self, urls: List[str], **kwargs) -> List[AsyncCrawlResponse]: pass @abstractmethod async def take_screenshot(self, **kwargs) -> str: pass @abstractmethod def update_user_agent(self, user_agent: str): pass @abstractmethod def set_hook(self, hook_type: str, hook: Callable): pass"
  },
  {
    "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "document_location": "docs/md_v2/advanced/magic-mode.md",
    "artifact_id": 3,
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "implicit",
    "relationship_type": "uses",
    "relationship_explanation": "The example likely relies on hooks set via `set_hook()` in `AsyncPlaywrightCrawlerStrategy` to manage page events such as removing popups, through the `remove_overlay_elements` setting.",
    "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_trace_chain_explanation": "Hooks are pivotal in altering the page behavior and executing functionality such as removing popups, aligning with the `remove_overlay_elements=True` usage in the sample.",
    "ground_truth_relationship": null,
    "ground_truth_trace_chain": null,
    "traceability_granularity": "Class",
    "confusion_metrics": "False Positive",
    "prediction_details": {
      "matches_ground_truth": false,
      "relationship_match": false
    },
    "error_group": [
      "IAE"
    ],
    "artifact_code": "def set_hook(self, hook_type: str, hook: Callable): if hook_type in self.hooks: self.hooks[hook_type] = hook else: raise ValueError(f\"Invalid hook type: {hook_type}\")"
  }
]