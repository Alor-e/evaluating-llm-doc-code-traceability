{
  "dataset": "crawl4ai",
  "run_id": 5,
  "timestamp": "2025-02-12T14:00:35.332627",
  "results": [
    {
      "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet explicitly shows the usage of the `arun()` method by calling it with an URL, which is directly related to invoking the web crawling process as demonstrated in the text.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The trace starts with the documentation file 'output-formats.md', which directly references and provides code example of using the `AsyncWebCrawler.arun()` method.",
      "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 12,
      "artifact_title": "CrawlResult.fit_markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "attributes",
      "relationship_explanation": "In the provided code example, there is a print statement displaying `fit_markdown`, which is a class-level attribute of `CrawlResult`, implicitly demonstrating its use in displaying the main content extracted by the process.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult.fit_markdown",
      "predicted_trace_chain_explanation": "The documentation describes retrieving and utilizing `fit_markdown` after calling `arun()` on `AsyncWebCrawler`. The return value of `arun()` is a `CrawlResult` object from which `fit_markdown` is accessed, hence establishing the trace pathway through this sequence of events.",
      "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.fit_markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation directly instructs users to import `AsyncWebCrawler` and create an instance of it within an async context manager, indicating that this class is explicitly used in the provided code snippet.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation snippet directly references `AsyncWebCrawler` as the main class to use for creating a web crawler instance, making it the endpoint of the trace chain from the documentation.",
      "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "While not directly mentioned, `AsyncWebCrawler` relies on `AsyncCrawlerStrategy` or a derived strategy to function, because `AsyncWebCrawler` is designed to work with a strategy object, and `AsyncCrawlerStrategy` provides the interface for such strategies.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "The `AsyncWebCrawler` is designed to be used with a crawler strategy (`AsyncCrawlerStrategy` or derivatives like `AsyncPlaywrightCrawlerStrategy`). The trace chain reflects that although `AsyncCrawlerStrategy` is not directly mentioned in the doc snippet, it forms part of the underlying implementation that `AsyncWebCrawler` depends on.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The `AsyncWebCrawler` by default uses `AsyncPlaywrightCrawlerStrategy` as the strategy if none is passed, indicated by its constructor initialization in the `AsyncWebCrawler` class. This makes `AsyncPlaywrightCrawlerStrategy` an implicit part of the user flow when following the quickstart guide.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "When following the usage pattern of `AsyncWebCrawler` in the quickstart guide, the default behavior includes using `AsyncPlaywrightCrawlerStrategy`, connecting the quickstart documentation, `AsyncWebCrawler`, and its default strategy.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions using the `arun()` method to crawl a URL with JavaScript code and waiting conditions.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation text directly references the `arun()` method of the `AsyncWebCrawler` class, outlining its usage for handling JavaScript execution and waiting for content to load.",
      "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "contains",
      "relationship_explanation": "Although the documentation does not explicitly state the `AsyncWebCrawler`, the `arun()` method's usage implies its involvement as it belongs to this class.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The relationship chain starts with the documentation referencing the method `arun()`, which is a part of the `AsyncWebCrawler` class.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The `AsyncWebCrawler` is designed to use a `crawler_strategy`, which defaults to `AsyncPlaywrightCrawlerStrategy`. This implies methods like `arun()` eventually interact with this class for operations like JavaScript execution.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The document traces functionality back to the `AsyncWebCrawler`, which utilizes `AsyncPlaywrightCrawlerStrategy` as its default crawling strategy, affecting how actions described in `arun()` are implemented.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
      "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly mentions and uses the 'CosineStrategy' class. It provides examples of how this class should be initialized with parameters like 'word_count_threshold', 'top_k', and others, indicating direct usage of this class in practice.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The document 'cosine.md' directly references the 'CosineStrategy' class, illustrating how it should be configured for optimal use in different contexts, such as adjusting word count thresholds or handling various content types.",
      "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "CosineStrategy is likely a subclass of the more abstract 'ExtractionStrategy' since it is implementing extraction logic as inferred by its initialization and parameters in the examples. This piece of the hierarchy can be deduced based on typical design patterns in strategy implementations.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "Starting from 'cosine.md', which mentions 'CosineStrategy', the trace continues implicitly to 'ExtractionStrategy', which is suggested as the parent class given the context of its function and usual design pattern, forming the chain through class inheritance.",
      "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation directly mentions the 'arun' method being used to perform a crawling operation with domain-based filtering. The example shows 'arun' as being key to initiating the crawling with specific arguments for excluding domains.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The 'content-selection.md' document indicates a direct example usage of 'arun', which directly ties back to the ability to perform web crawling with optional filters for domain exclusion as part of the method parameters.",
      "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'arun' method in the 'AsyncWebCrawler' class is explicitly called in the documentation snippet's example. This indicates that the method is used to initiate a web crawling process with specific anti-detection features.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation directly shows the use of AsyncWebCrawler.arun(), making it the target artifact in the traceability chain.",
      "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'AsyncWebCrawler' class uses a 'crawler_strategy', which by default is 'AsyncPlaywrightCrawlerStrategy', implementing 'AsyncCrawlerStrategy'. This base class provides the abstract definition for the strategy methods likely used in tasks like masking automation signals.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "Starting from the documentation example, the arun() method is part of AsyncWebCrawler which uses AsyncCrawlerStrategy interfaces for implementing crawls including stealth features.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The 'AsyncPlaywrightCrawlerStrategy' class extends 'AsyncCrawlerStrategy' and implements the crawling functionalities. While not directly mentioned, this strategy is the default for 'AsyncWebCrawler', used to possibly set the anti-detection features depending on the user simulation and navigator overrides.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The introduction of simulate_user=True and override_navigator=True implies features likely handled by AsyncPlaywrightCrawlerStrategy, which extends the abstract AsyncCrawlerStrategy.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "documented_class",
      "relationship_explanation": "The text snippet explicitly mentions the 'JsonCssExtractionStrategy' as a class designed for extracting data using CSS selectors. This is directly aligned with the provided code example.",
      "predicted_trace_chain": "overview.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation in 'overview.md' describes the 'JsonCssExtractionStrategy', guiding users on how to implement CSS-based data extraction. This relates directly to the class definition that provides this functionality.",
      "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "usage_example",
      "relationship_explanation": "In the provided code example, 'arun' is implicitly used as the method to execute the web crawling task with the 'extraction_strategy' parameter set to an instance of 'JsonCssExtractionStrategy'.",
      "predicted_trace_chain": "overview.md -> JsonCssExtractionStrategy -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "'JsonCssExtractionStrategy' is designed to be used as a part of the extraction strategy in 'arun', showcasing a pathway from strategy definition to its utilization in a web crawler execution.",
      "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
      "ground_truth_trace_chain": "overview.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `arun` method from `AsyncWebCrawler` is explicitly called in the provided snippet to initiate the crawling process for media extraction from a URL.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation describes using the `arun` method from the `AsyncWebCrawler` class, which is responsible for handling web crawling, therefore establishing a direct trace chain from the description on how to utilize the method for media-specific operations.",
      "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 16,
      "artifact_title": "CrawlResult.media",
      "predicted_relationship": "explicit",
      "relationship_type": "attribute_usage",
      "relationship_explanation": "The code snippet in the documentation demonstrates accessing the `media` attribute from the `CrawlResult` object returned by the `arun` method to access different types of media.",
      "predicted_trace_chain": "content-selection.md -> CrawlResult -> CrawlResult.media",
      "predicted_trace_chain_explanation": "The `CrawlResult` class embodies the result structure of the crawling process including media results, hence directly mapping the documentation's retrieval of media attributes from the instance of `CrawlResult` to its implementation.",
      "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult.media",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 22,
      "artifact_title": "RegexChunking",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly mentions adding a chunking strategy called `RegexChunking` and provides an example of how RegexChunking is used with a regex pattern to split text.",
      "predicted_trace_chain": "quickstart.md -> RegexChunking",
      "predicted_trace_chain_explanation": "The documentation in quickstart.md directly references RegexChunking as a chunking strategy that can be used, indicating its role in the system.",
      "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
      "ground_truth_trace_chain": "quickstart.md -> ChunkingStrategy -> RegexChunking",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "demonstrated usage",
      "relationship_explanation": "The example in the documentation shows the `arun` method being invoked on an `AsyncWebCrawler` instance. While `arun` is not directly discussed, it is crucial in demonstrating how the RegexChunking strategy is applied in practice.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The quickstart.md shows AsyncWebCrawler being used in an example that indirectly demonstrates the arun method, part of the invocation chain for applying RegexChunking.",
      "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "demonstrated usage",
      "relationship_explanation": "In the documentation example, an `AsyncWebCrawler` instance is created, showing how the user sets up the crawler with verbosity enabled but without much explanation on the class's structure.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document implicitly references AsyncWebCrawler as the orchestrator to use different strategies, setting the context for arun method.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "ChunkingStrategy",
      "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
      "ground_truth_trace_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "usage",
      "relationship_explanation": "The AsyncWebCrawler class is explicitly mentioned as it is used in example code snippets to perform browser-specific crawling with parameters such as 'browser_type'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document explains browser configuration by showing how to instantiate the AsyncWebCrawler with different 'browser_type' parameters.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The arun() method is implicitly related via the example code where AsyncWebCrawler's instances call it to perform crawling operations.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document example uses 'arun()' on the 'AsyncWebCrawler' instance to demonstrate the browser-specific crawling. 'arun()' is a method of 'AsyncWebCrawler', thus forming a natural chain.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "AsyncWebCrawler utilizes AsyncPlaywrightCrawlerStrategy which is a subclass of AsyncCrawlerStrategy. This establishes an implicit connection since AsyncWebCrawler relies on this strategy pattern.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "AsyncWebCrawler is shown in the documentation. It uses AsyncPlaywrightCrawlerStrategy, which in turn extends AsyncCrawlerStrategy, thereby forming a chain of dependency.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "This strategy does the actual operation as seen in the example by implementing methods that can be invoked via AsyncWebCrawler for diverse browser operations.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The AsyncWebCrawler's functionality presented in examples relies on the given browser interactions which are handled by AsyncPlaywrightCrawlerStrategy.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions calling the method 'arun' on the 'crawler' object, demonstrating its usage for converting HTML to text configurations.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation in output-formats.md demonstrates usage of the 'arun' method for HTML to text conversion tasks, making an explicit mention of the method and its parameters.",
      "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "While not directly stated, the 'arun' method is part of the 'AsyncWebCrawler' class. The documentation implicitly relates to 'AsyncWebCrawler' as the method belongs to this class.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The 'arun' method is directly called in the documentation, which implies its belonging to 'AsyncWebCrawler'. The class is key in understanding where the method is implemented as part of the HTML to text conversion setup.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The 'AsyncWebCrawler' uses a 'crawler_strategy' that defaults to 'AsyncPlaywrightCrawlerStrategy'. The documentation does not explicitly mention this, but utilizing 'arun' indirectly relies on the strategy which is a subclass of 'AsyncCrawlerStrategy'.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "Following the usage of 'arun', we deduce 'AsyncWebCrawler' employs a strategy pattern. The default strategy, 'AsyncPlaywrightCrawlerStrategy', is critical for executing different crawling tasks.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "base",
      "relationship_explanation": "'AsyncPlaywrightCrawlerStrategy', the default crawling strategy in 'AsyncWebCrawler', extends 'AsyncCrawlerStrategy', indicating a foundational role in providing abstract methods for various crawling operations.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "Tracing the pathway through 'AsyncWebCrawler' to its strategy employs 'AsyncCrawlerStrategy' as an abstract base for defining core crawling functionalities implemented by 'AsyncPlaywrightCrawlerStrategy'.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly uses the 'AsyncWebCrawler' class in code examples to demonstrate setting a custom user agent and headers.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation snippet directly illustrates the instantiation of the 'AsyncWebCrawler' class, making it an explicit reference point for how identity management is implemented in a browser context.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "invokes",
      "relationship_explanation": "The 'arun' method is shown in the documentation as the method called on the 'AsyncWebCrawler' instance to execute a web crawling task with custom settings, implying its significant role in carrying out the primary functionality.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' class directly leads to the 'arun' method invocation as shown in the usage example. This method is integral to executing the web crawl after setting the user agent and headers.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "Given the hierarchical structure in the codebase, 'AsyncWebCrawler' likely depends on 'AsyncCrawlerStrategy' as part of its initialization or operations strategy, though not directly mentioned, it indicates a typical inheritance relationship.",
      "predicted_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' serves as the operational class, and usually, such structures extend a strategy interface like 'AsyncCrawlerStrategy' to define their crawling strategy logic.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "This strategy class is likely used by 'AsyncWebCrawler' as its default strategy, implementing methods required by 'AsyncCrawlerStrategy', implied by its usage context.",
      "predicted_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "From 'AsyncWebCrawler', the strategy pathway naturally extends towards 'AsyncPlaywrightCrawlerStrategy', assuming the latter as the concrete implementation of the abstract methods defined in 'AsyncCrawlerStrategy'.",
      "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "usage",
      "relationship_explanation": "The documentation snippet demonstrates the use of the AsyncWebCrawler class through the context manager pattern, indicating its role directly in handling asynchronous web crawling operations which fits the intended usage as per documentation.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document 'quickstart.md' explains an example usage of AsyncWebCrawler, which serves as the central class for orchestrating the web crawling operation.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "method invocation",
      "relationship_explanation": "The snippet includes a function call to `arun()` method of AsyncWebCrawler, showing its functionality for performing specific crawl tasks, such as handling overlay removal and taking screenshots.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The method arun() is invoked on an instance of AsyncWebCrawler, which is demonstrated in the documentation via a usage example, highlighting its role in executing a crawl operation.",
      "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "implicit",
      "relationship_type": "method reference",
      "relationship_explanation": "While not directly mentioned in the example, AsyncPlaywrightCrawlerStrategy.kill_session() is involved in session management which is part of the crawl operation lifecycle.",
      "predicted_trace_chain": "quickstart.md -> async_crawler_strategy.py -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "Part of the web crawling operation involves session management handled by AsyncPlaywrightCrawlerStrategy, showing the connectivity in lifecycle handling.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Method",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 3,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
      "predicted_relationship": "implicit",
      "relationship_type": "method reference",
      "relationship_explanation": "Though not explicitly mentioned, AsyncPlaywrightCrawlerStrategy.set_hook() supports the flexibility shown in the snippet for managing customizable hooks during a crawl operation.",
      "predicted_trace_chain": "quickstart.md -> async_crawler_strategy.py -> AsyncPlaywrightCrawlerStrategy.set_hook()",
      "predicted_trace_chain_explanation": "The example implies potential use of hooks, which the method set_hook() facilitates, highlighting its indirect role in the flexibility of crawling operations.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Method",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, providing implementations for the asynchronous crawling actions demonstrated in the snippet.",
      "predicted_trace_chain": "quickstart.md -> async_crawler_strategy.py -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "This class implements the strategy pattern used for performing async web crawling within AsyncWebCrawler, as demonstrated in 'quickstart.md'.",
      "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation describes the use of 'await crawler.arun(...)'. Here, 'crawler' is an instance of a class that supports the 'arun' method, indicating the use of 'AsyncWebCrawler' as it provides this method. This is evidence that 'AsyncWebCrawler' is utilized in the context provided.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation addresses interacting with pages using a strategy available through the 'arun' method, which is part of the 'AsyncWebCrawler' class.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "extends",
      "relationship_explanation": "The documentation mentions classes 'JsonCssExtractionStrategy' and 'LLMExtractionStrategy', which imply functionality for extraction strategy. These classes extend 'ExtractionStrategy', a base class for different extraction strategies.",
      "predicted_trace_chain": "page-interaction.md -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "Since 'JsonCssExtractionStrategy' and 'LLMExtractionStrategy' are derived from 'ExtractionStrategy', understanding the base concept is crucial to comprehending the specific strategies discussed.",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
      "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'JsonCssExtractionStrategy' is explicitly described in the snippet as an extraction strategy that utilizes a schema for extraction, such as selecting HTML elements using CSS selectors. This indicates it's implementing an extraction process as mentioned.",
      "predicted_trace_chain": "page-interaction.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "Direct mention in the documentation referring to JSON + CSS extraction technique with schema-based approach, making it a key element of the snippet.",
      "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
      "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "LLMExtractionStrategy is explicitly mentioned to demonstrate the extraction of content using an LLM provider and a schema, implementing a distinct form of extraction strategy.",
      "predicted_trace_chain": "page-interaction.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation directly uses this strategy for content analysis as shown, specifying its implementation in extracting complex content.",
      "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
      "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet demonstrates the use of 'await crawler.arun(...)', showing an example usage of this method to facilitate actions which include page interactions and data extraction.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "Detailed interaction with webpage actions and extraction utilizes 'arun()', highlighting its role within 'AsyncWebCrawler'.",
      "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
      "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 26,
      "artifact_title": "RegexChunking",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly mentions the `RegexChunking` class as a chunking strategy that splits text using regular expressions. It details the class usage in an example block demonstrating how to use the class.",
      "predicted_trace_chain": "chunking.md -> RegexChunking",
      "predicted_trace_chain_explanation": "The documentation snippet from chunking.md directly describes the `RegexChunking` class and provides an example of its instantiation and usage, specifically showing its method of splitting text based on regular expressions.",
      "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> RegexChunking",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 22,
      "artifact_title": "FixedLengthWordChunking",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "While `FixedLengthWordChunking` is not directly mentioned, `RegexChunking` implements `ChunkingStrategy`, and other strategies like `FixedLengthWordChunking` are likely to extend `ChunkingStrategy`. This implies a conceptual relationship as different implementation strategies for chunking.",
      "predicted_trace_chain": "chunking.md -> RegexChunking -> FixedLengthWordChunking",
      "predicted_trace_chain_explanation": "The documentation describes `RegexChunking`, a class implementing features of the chunking methodology. It suggests a broader context that includes other chunking implementations, leading to an inference of relationship to `FixedLengthWordChunking`, another strategy implementing similar principles.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 6,
      "artifact_title": "ChunkingStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "interface",
      "relationship_explanation": "The `RegexChunking` implements a strategy for chunking. As it is expected to inherit from an abstract `ChunkingStrategy` class, there is an implicit relationship that `RegexChunking` provides the specific implementation details for the abstract method it inherits.",
      "predicted_trace_chain": "chunking.md -> RegexChunking -> ChunkingStrategy",
      "predicted_trace_chain_explanation": "The detailed description in the document of `RegexChunking` assumes that it implements or extends a base class or interface like `ChunkingStrategy`, forming the conceptual foundation for how specific chunking techniques are defined and applied.",
      "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The AsyncWebCrawler class is directly used in the documentation as an instance is created and its method 'arun' is called. This shows that the class is responsible for crawling based on the provided URL and parameters, including handling media processing.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation specifies the use of an object called 'crawler' which directly refers to an instance of the AsyncWebCrawler class. This is confirmed by the context where 'crawler.arun()' is used in the code examples to demonstrate media processing capabilities.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The method 'arun' of the AsyncWebCrawler class is explicitly used in the documentation. The examples provided demonstrate how the method is called with parameters like URLs and options to handle media and lazy-loaded content, reflecting the functionality discussed.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation directly involves calling 'arun' from the AsyncWebCrawler instance to achieve the media processing operations. This connection shows it utilizes this method for executing crawl operations with various configurations including media handling.",
      "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "explicit",
      "relationship_type": "interfaces",
      "relationship_explanation": "The CrawlResult class is explicitly referenced since results of the crawls are stored in a CrawlResult object as hinted in the documentation with 'result.media[\"images\"]'. This indicates the object structure that holds the media output after processing.",
      "predicted_trace_chain": "content-processing.md -> CrawlResult",
      "predicted_trace_chain_explanation": "The output of 'arun' returns a CrawlResult instance, as inferred from its usage in accessing 'result.media'. This indicates a direct pathway from executing a crawl via the arun method to storing its results within a CrawlResult object.",
      "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 16,
      "artifact_title": "CrawlResult.media",
      "predicted_relationship": "explicit",
      "relationship_type": "contains",
      "relationship_explanation": "The 'media' attribute of the CrawlResult class is directly referenced in the documentation. The example accesses media information within the 'result' object, which indicates the media processing output stored in this attribute.",
      "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
      "predicted_trace_chain_explanation": "Upon receiving a CrawlResult from 'arun', accessing 'media' field demonstrates that this is where processed media data is stored, as evidenced by statements like 'for image in result.media[\"images\"]:'.",
      "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.media",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The text provides a code example that directly uses `crawler.arun()` method, which is part of the `AsyncWebCrawler` class. Evidence can be found in the line `result = await crawler.arun(...)`.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation snippet shows how to use the `arun` method of the `AsyncWebCrawler` class to perform a web crawl using CSS selectors. Thus, the `AsyncWebCrawler` class is directly relevant and necessary to understand the operation described in the documentation.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The `AsyncWebCrawler` class uses a strategy to perform crawling. The `AsyncCrawlerStrategy` is the base class that provides the definition for such strategies. Even though it's not directly mentioned, it's implicitly used as it establishes the interface that the `AsyncWebCrawler` requires.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "The `AsyncWebCrawler` class operates using a `crawler_strategy`, which is an instance of a class that implements `AsyncCrawlerStrategy`. Thus, `AsyncCrawlerStrategy` is an essential part of the trace chain because it defines the methods expected to be usable by `AsyncWebCrawler`.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The `AsyncPlaywrightCrawlerStrategy` class extends `AsyncCrawlerStrategy` and could be used by `AsyncWebCrawler` for performing crawling actions. This extension provides concrete implementations needed for executing the example.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The example in the documentation could be leveraging the `AsyncPlaywrightCrawlerStrategy` as the concrete implementation of `AsyncCrawlerStrategy` used by `AsyncWebCrawler`. This allows the described functionality (using CSS selectors) to be performed as implied by the usage patterns possible in the `AsyncPlaywrightCrawlerStrategy`.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "`arun()` is the method called in the example shown in the text to perform web crawling with CSS selectors. It explicitly implements the functionality documented.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The `arun()` method is the primary interface shown in the documentation for executing a crawl with CSS selectors, making it a crucial last step in the trace from concept to execution.",
      "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet provides an example using `crawler.arun` to retrieve a cleaned HTML result, indicating that `AsyncWebCrawler` is the class implementing the `arun` method, as evidenced by its signature and context of usage.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document discusses the use case for returning cleaned HTML, which suggests the usage of the `arun` method of the `AsyncWebCrawler` class. This is explicitly reflected in the code example provided in the documentation.",
      "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The method `arun` is used directly in the documentation example to demonstrate how to obtain and print `cleaned_html`. The code snippet in the documentation matches the signature and functional description provided in the `AsyncWebCrawler.arun()` method.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document focuses on demonstrating how to get `cleaned_html` using the `arun` method of `AsyncWebCrawler`, making it clear how the documentation relates to the specific method, as part of showing sanitized HTML retrieval in the workflow.",
      "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 9,
      "artifact_title": "CrawlResult.cleaned_html",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation shows `cleaned_html` being accessed in the example. This indicates the `CrawlResult` class attribute `cleaned_html` is part of the resulting object from the `arun` method, showing the retrieval of cleaned HTML as an output.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.cleaned_html",
      "predicted_trace_chain_explanation": "The document describes a process where the `arun` method outputs a result object that includes `cleaned_html`. The attribute `cleaned_html` within `CrawlResult` is part of this output, depicting the final step in obtaining the cleaned HTML from the `AsyncWebCrawler` process outlined.",
      "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.cleaned_html",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The text snippet explicitly imports and uses 'AsyncWebCrawler' as indicated by 'from crawl4ai import AsyncWebCrawler'. It's used in the example where an instance of 'AsyncWebCrawler' is created and used to run the 'arun' method.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation snippet demonstrates the use of the 'AsyncWebCrawler' class for performing web crawling tasks asynchronously. It's a direct correspondence with the supplied code example.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "method usage",
      "relationship_explanation": "The example in the documentation calls the 'arun' method of the 'AsyncWebCrawler'. This is implicit in the task the 'AsyncWebCrawler' executes within the 'extract_tech_content' asynchronous function.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The 'arun' method of 'AsyncWebCrawler' is used to execute the web crawling operation, playing a crucial role in extracting content as directed by the 'LLMExtractionStrategy'. It showcases method invocation within the class discussed.",
      "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'LLMExtractionStrategy' is explicitly mentioned in the snippet's code as part of the extraction process, highlighting its usage for extracting technology-related content using a large language model provider.",
      "predicted_trace_chain": "llm.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "LLMExtractionStrategy's role is essential as defined by user instruction to extract technology content, which is a direct application of this strategy. This demonstrates the specific configuration and utilization relationship with 'AsyncWebCrawler'.",
      "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
      "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
      "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
      "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 16,
      "artifact_title": "CrawlResult.media",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly references the 'result.media' attribute, demonstrating how it stores video and audio data for further processing.",
      "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
      "predicted_trace_chain_explanation": "The documentation describes handling video and audio data extracted during web crawling, which corresponds to 'result.media' within the 'CrawlResult' class, suggesting this attribute is used for accessing media content. This utilizes 'CrawlResult' which makes 'CrawlResult.media' essential.",
      "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.media",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `arun` method is explicitly mentioned in the example provided in the documentation. It is used to perform a crawl operation with specific options passed as arguments, such as `url`, `word_count_threshold`, and other processing flags. The documentation provides a direct example of how to use this method.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation snippet discusses how to use the `arun` method by providing a code example where `arun` is called with multiple parameters. This establishes a direct trace from the documentation to the method, illustrating its usage for users.",
      "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "Although not directly mentioned, `AsyncWebCrawler` internally uses an instance of a strategy derived from `AsyncCrawlerStrategy`. The usage of options like `remove_overlay_elements` and processing of iframes likely link back to methods defined or enforced by the strategy pattern implemented in `AsyncCrawlerStrategy`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "The chain is established through implicit knowledge: `AsyncWebCrawler.arun()` utilizes a strategy instance which inherits from `AsyncCrawlerStrategy`. This indirect relationship is necessary to understand how `arun` manages and delegates various crawling tasks like handling overlays and iframes.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation highlights options related to session handling, implicitly utilizing the `kill_session` method for managing sessions. This is inferred from the session and task cleanup operations that a strategy like `AsyncPlaywrightCrawlerStrategy` must support within `arun`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "`AsyncWebCrawler.arun()` interacts with session management indirectly through the strategy class methods such as `kill_session`. Even though not explicitly documented here, understanding session management is critical for hiding intricate details of session cleanup when custom crawl options are applied.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly mentions the `arun()` method and provides a usage example: `result = await crawler.arun(url=\"https://example.com\")`. This shows that `arun()` is used to make asynchronous web crawling requests.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation in simple-crawling.md directly references and provides usage for the `arun()` method, making it the starting point in the trace.",
      "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "explicit",
      "relationship_type": "returns",
      "relationship_explanation": "The `arun()` method is documented to return a `CrawlResult` object, as seen in the line: 'The `arun()` method returns a `CrawlResult` object with several useful properties.'",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
      "predicted_trace_chain_explanation": "The starting point is simple-crawling.md referencing `arun()`, which in turn returns the `CrawlResult` object, thereby forming a logical flow: documentation to method to class.",
      "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 13,
      "artifact_title": "CrawlResult.html",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute",
      "relationship_explanation": "In the documentation, `result.html` is used to retrieve the raw HTML, which suggests that `html` is an attribute of `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
      "predicted_trace_chain_explanation": "Starting from simple-crawling.md, `arun()` returns a `CrawlResult` containing the `html` attribute which is used in the documented examples.",
      "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.html",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 9,
      "artifact_title": "CrawlResult.cleaned_html",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute",
      "relationship_explanation": "The documentation example calls `result.cleaned_html` to access cleaned HTML content, indicating that `cleaned_html` is an attribute of `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.cleaned_html",
      "predicted_trace_chain_explanation": "The documentation specifies .cleaned_html after receiving a `CrawlResult` from `arun()`, thus completing the trace pathway through the object's attributes.",
      "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.cleaned_html",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 15,
      "artifact_title": "CrawlResult.markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute",
      "relationship_explanation": "The documentation utilizes `result.markdown` to access a markdown version of content, pointing to `markdown` as an attribute of `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
      "predicted_trace_chain_explanation": "The document mentions collecting markdown from the `CrawlResult`. This establishes the full trace from the document, through `arun()`, to the `CrawlResult` attribute.",
      "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 12,
      "artifact_title": "CrawlResult.fit_markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute",
      "relationship_explanation": "When the documentation lists `result.fit_markdown` to get the most relevant content, it implies `fit_markdown` is an attribute of `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.fit_markdown",
      "predicted_trace_chain_explanation": "By mentioning the retrieval of `fit_markdown` from a `CrawlResult`, the text reflects a clear path through chained relationships, terminating at the class attribute.",
      "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.fit_markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 20,
      "artifact_title": "CrawlResult.success",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute",
      "relationship_explanation": "Checking `result.success` as indicated in the documentation example reveals `success` as a boolean attribute in `CrawlResult` to verify crawl completion.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
      "predicted_trace_chain_explanation": "The provided code in the document assesses success using this attribute, showing its connection through the pathway from `arun()` returning `CrawlResult`.",
      "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 19,
      "artifact_title": "CrawlResult.status_code",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute",
      "relationship_explanation": "Example in documentation uses `result.status_code` to obtain HTTP status codes, identifying `status_code` as an attribute of `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.status_code",
      "predicted_trace_chain_explanation": "Described retrieval of `status_code` reflects the logical progression from the document, via method, to the specific class attribute.",
      "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.status_code",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 16,
      "artifact_title": "CrawlResult.media",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute",
      "relationship_explanation": "Utilization of `result.media` maps to the `media` attribute within `CrawlResult`, collecting multimedia from a response, as per the documentation.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.media",
      "predicted_trace_chain_explanation": "From document to attribute, it follows that `media` is accessed through `CrawlResult`, after retrieving it through the `arun()` method.",
      "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.media",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 14,
      "artifact_title": "CrawlResult.links",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute",
      "relationship_explanation": "Documentation example fetching `result.links` indicates `links` is an attribute in `CrawlResult` to collate URLs encountered.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
      "predicted_trace_chain_explanation": "The descriptor leads from the document calling `arun()` to its returned `CrawlResult`, enabling access to the `links` attribute.",
      "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.links",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly discusses the 'JsonCssExtractionStrategy' by name as a feature of Crawl4AI. It describes its function and how it allows users to extract structured data from web pages using CSS selectors based on a predefined schema.",
      "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document 'css.md' directly discusses 'JsonCssExtractionStrategy', detailing how it is used and its specific functionality within Crawl4AI for extracting data from web pages.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation provides an example of using 'JsonCssExtractionStrategy' with the 'AsyncWebCrawler'. It describes how to implement this strategy with AsyncWebCrawler, suggesting a usage relationship where AsyncWebCrawler interacts with JsonCssExtractionStrategy for data extraction.",
      "predicted_trace_chain": "css.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The discussion in 'css.md' describes how to use 'JsonCssExtractionStrategy' with 'AsyncWebCrawler', indicating that AsyncWebCrawler is a client or user of this strategy, forming a direct pathway from use of CSS strategy in the document to the AsyncWebCrawler class.",
      "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
      "ground_truth_trace_chain": "css.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "While not explicitly named, 'AsyncWebCrawler' utilizes 'JsonCssExtractionStrategy', which is a form of CrawlerStrategy. Given that 'AsyncWebCrawler' operates using strategies, and 'JsonCssExtractionStrategy' indirectly operates through it, 'AsyncCrawlerStrategy' is necessarily part of the implementation path.",
      "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "AsyncWebCrawler uses strategies to execute crawling tasks. As 'JsonCssExtractionStrategy' tied to AsyncWebCrawler implies a strategy-based execution, the base abstract class 'AsyncCrawlerStrategy' forms part of the hierarchical pathway, linking strategy usage in the AsyncWebCrawler.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly mentions `CosineStrategy` and demonstrates how to create a strategy instance with specific parameters like `linkage_method`, `max_dist`, and `model_name`. This indicates that `CosineStrategy` implements custom clustering as described.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The trace chain starts with the `cosine.md` documentation which directly references the `CosineStrategy` class, indicating its use in implementing custom clustering features.",
      "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation includes a `Content Filtering Pipeline` example that shows the `extract_pricing_features` function using the `AsyncWebCrawler` class to perform a web crawl with the `CosineStrategy` as an `extraction_strategy`. This implies that `AsyncWebCrawler` uses strategies like `CosineStrategy` for its crawling operations.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The `cosine.md` document implicitly refers to `AsyncWebCrawler` through its usage in the example function `extract_pricing_features`, demonstrating how this class integrates with extraction strategies like `CosineStrategy`.",
      "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
      "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The example function in the documentation calls the `arun` method on an instance of `AsyncWebCrawler`, specifying the `CosineStrategy` as the `extraction_strategy`. This illustrates the role of `arun()` in executing a crawl operation with a given strategy.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The `cosine.md` document indirectly references `AsyncWebCrawler.arun()` through its usage in the `Content Filtering Pipeline` example, showing its operational integration with `AsyncWebCrawler` to execute crawling tasks with a specified extraction strategy.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
      "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "accesses",
      "relationship_explanation": "Within the `Content Filtering Pipeline`, the example function accesses `result.extracted_content` to manipulate the returned crawl data, showing how this attribute of `CrawlResult` is fundamental to processing the output of a crawl executed by `AsyncWebCrawler`.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "In the `cosine.md` document, `CrawlResult.extracted_content` is indirectly referenced in the content extraction post-processing step, which is part of the method chain `arun()` returns through `CrawlResult`, illustrating the completion of a crawling and extraction process.",
      "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
      "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
      "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The code snippet directly mentions using the 'arun' method of 'crawler', which corresponds to 'AsyncWebCrawler.arun()'. This is the main action point in the sample code where the URL is processed with exclusion parameters for links.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation in 'content-processing.md' directly demonstrates usage of 'AsyncWebCrawler.arun()' through the provided code sample. This ties the documentation directly to the implementation, showing how 'arun' is meant to be used within a broader context of filtering links.",
      "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "'AsyncWebCrawler' is the class context in which 'arun' method is called. It implicitly supports the functionalities demonstrated in the documentation by providing the 'arun' method used in the code sample.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation in 'content-processing.md' involves 'AsyncWebCrawler' as it details a feature (link filtering) that relies on method implementations within this class, identified through the 'arun' call.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "base",
      "relationship_explanation": "The 'AsyncWebCrawler' likely uses an 'AsyncCrawlerStrategy' given the structure and naming conventions, providing a strategy-oriented approach for handling various crawling configurations, which indirectly supports the URL processing with options specified in the documentation.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "From 'content-processing.md' the documentation connects to 'AsyncWebCrawler', which is likely designed to operate with 'AsyncCrawlerStrategy' configurations, linking the strategy with the capabilities described in filtering links through 'AsyncWebCrawler'.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "demonstrates usage",
      "relationship_explanation": "The documentation snippet explicitly demonstrates how to use the `AsyncWebCrawler` class with different browser types such as 'firefox', 'webkit', and 'chromium'. This is shown in the provided code examples where `AsyncWebCrawler` is instantiated with different initialization parameters.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation directly provides a usage example of the `AsyncWebCrawler` class, indicating how to configure it to use different browser engines, which is a primary functionality aspect.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "method usage",
      "relationship_explanation": "The method `arun` of the `AsyncWebCrawler` class is used in the code examples provided in the documentation snippet. While the method itself is not explicitly discussed, the examples demonstrate its usage for crawling a URL.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "Starting with the `AsyncWebCrawler` from the documentation, `arun()` is the method used to execute the core functionality of the class\u2014performing a web crawl. The chain illustrates how this method is critical to carrying out the demonstrated task.",
      "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "base class",
      "relationship_explanation": "The `AsyncWebCrawler` utilizes a strategy pattern, with `AsyncPlaywrightCrawlerStrategy` as the default. `AsyncCrawlerStrategy` is the base class for `AsyncPlaywrightCrawlerStrategy`, forming an underpinning part of the broader strategy system used by `AsyncWebCrawler`.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "The `AsyncWebCrawler` indirectly relies on implementations of `AsyncCrawlerStrategy` through its default strategy, `AsyncPlaywrightCrawlerStrategy`. The connection is fundamental, as these strategies define how the browser-related operations are executed.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "default strategy",
      "relationship_explanation": "The `AsyncWebCrawler` class uses `AsyncPlaywrightCrawlerStrategy` as its default strategy for crawling, as indicated in its `__init__` method. This is critical when using `AsyncWebCrawler` with browsers, aligning with the browser selection context provided in the documentation.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "`AsyncWebCrawler` is designed to be strategy-driven, and it defaults to using `AsyncPlaywrightCrawlerStrategy` for browser-based operations. This chain shows the logical progression from the web crawler itself to the specific strategy it uses under default conditions.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/index.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet explicitly uses 'AsyncWebCrawler' by creating an instance with AsyncWebCrawler(verbose=True) within the example code. This demonstrates usage for asynchronous web crawling.",
      "predicted_trace_chain": "index.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation demonstrates how to use the 'AsyncWebCrawler' class, establishing the direct relationship between the document and the class instance creation.",
      "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
      "ground_truth_trace_chain": "index.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/index.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "While 'arun()' isn't directly mentioned in the text explaining its use, the example code calls await crawler.arun(url=\"...\"), which demonstrates its use within the AsyncWebCrawler.",
      "predicted_trace_chain": "index.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The example code calls the 'arun' method on an instance of 'AsyncWebCrawler', showing how to execute a web scraping operation.",
      "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
      "ground_truth_trace_chain": "index.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/index.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
      "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/index.md",
      "artifact_title": "CrawlResult.markdown",
      "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
      "ground_truth_trace_chain": "index.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/index.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
      "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/index.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
      "ground_truth_trace_chain": "index.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "usage",
      "relationship_explanation": "The documentation snippet uses AsyncWebCrawler in the example, indicating this class is used for managing the crawling task including the setup and teardown of the crawling environment.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation directly shows the use of the AsyncWebCrawler class to perform crawling operations with JavaScript execution and LLM extraction strategies.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "usage",
      "relationship_explanation": "The method arun is explicitly used in the example to perform a single URL crawl, demonstrating how parameters like js_code and extraction_strategy are passed.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "AsyncWebCrawler is instantiated and its method arun is used to initiate the crawling process as described directly in the documentation example.",
      "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "usage",
      "relationship_explanation": "The code example explicitly utilizes LLMExtractionStrategy by instantiating it and passing it as a parameter for extraction purposes.",
      "predicted_trace_chain": "llm.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "LLMExtractionStrategy is directly instantiated within the documentation's code example for extracting summarized content from dynamic web pages.",
      "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
      "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
      "ground_truth_trace_chain": "llm.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
      "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
      "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet explicitly demonstrates the use of the `AsyncWebCrawler` class, as indicated by its instantiation in the example code `async with AsyncWebCrawler(...) as crawler:`.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The provided code sample explicitly uses `AsyncWebCrawler`, making it directly relevant to the documentation on combining proxy settings with Magic Mode.",
      "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "calls",
      "relationship_explanation": "The snippet shows a usage example where the method `arun()` is called on an instance of `AsyncWebCrawler`. Although `arun` is not explicitly discussed, it is demonstrated in the line `result = await crawler.arun(...)`.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The method `arun()` is part of the `AsyncWebCrawler` class, showing the method\u2019s expected usage when crawling a URL with optional proxy and Magic Mode settings, thus directly related to the snippet content.",
      "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
      "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly describes the `JsonCssExtractionStrategy`, especially the details about the `schema` which is used to define extraction tasks. The mention of `baseSelector`, `fields`, and the nature of data types matches the purpose and functionality of the `JsonCssExtractionStrategy` class.",
      "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document (css.md) elaborates on how CSS selectors are utilized for efficient data extraction. This directly relates to the `JsonCssExtractionStrategy` class which implements CSS selector-based data extraction as per the schema detailed in the document.",
      "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The `JsonCssExtractionStrategy` is a specific implementation of `ExtractionStrategy` as suggested by the text's focus on using CSS selectors for extraction. The document indirectly implies the use of `ExtractionStrategy` as the base class, leading us to understand the typical structure of the extraction strategy implementations.",
      "predicted_trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The pathway begins with the documentation (css.md) describing the functionality of using CSS selectors for data extraction. This fits within the broader taxonomy of extraction strategies, implying its extension of the abstract base class `ExtractionStrategy`, before narrowing into the specific implementation, `JsonCssExtractionStrategy`. Each step refines from the general strategy pattern to its specialized application.",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `arun` method is explicitly mentioned and demonstrated in the documentation as the method being used to perform crawling and extract metadata.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation refers to using an instance of `AsyncWebCrawler` to call the `arun` method for performing the crawl. This relationship is direct as it uses the `arun` method as part of the documented use case.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `CrawlResult` class is explicitly used in the documentation example to hold the result of the `arun` method. The result (`metadata`) is accessed from the returned `CrawlResult` object, and shown in the example.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
      "predicted_trace_chain_explanation": "The `arun` method of `AsyncWebCrawler` returns a `CrawlResult` object, which is then utilized in the documentation. The documentation describes accessing metadata through this object, detailing the chain of usage from the method to the class.",
      "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 17,
      "artifact_title": "CrawlResult.metadata",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "Within the example usage, the `metadata` attribute of a `CrawlResult` instance is explicitly accessed to retrieve various pieces of metadata such as title, description, and others, as shown in the text.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.metadata",
      "predicted_trace_chain_explanation": "The documentation's example shows how to obtain metadata using the `.metadata` attribute on a `CrawlResult` object obtained from the `arun` method. Thus, the pathway illustrates the extraction and usage of metadata in the described process.",
      "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.metadata",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The example code snippet in the documentation directly mentions the use of the 'arun' method to perform crawling with specified timing controls like 'page_timeout' and 'delay_before_return_html'. This indicates direct usage of 'AsyncWebCrawler.arun()' method.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation snippet describes an example of using the method 'arun' from 'AsyncWebCrawler'. This is a direct demonstration, showing how the 'arun' method is a part of 'AsyncWebCrawler', providing a functional usage example, implying direct invocation of this method in the context of controlling crawling interactions.",
      "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "While 'AsyncWebCrawler.arun()' is explicitly mentioned and demonstrated in the text, 'AsyncWebCrawler' itself is implicated as it is the class that provides the 'arun' method. This hints at the role that 'AsyncWebCrawler' plays in handling the timing control functionalities described in the snippet.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "Although the documentation does not directly mention 'AsyncWebCrawler', the tracing to 'arun' implies coverage of 'AsyncWebCrawler' because 'arun' is a method within this class, thus understanding 'AsyncWebCrawler' is necessary to grasp how it supports timing control features through its 'arun' method.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
      "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation provides an example using the `AsyncWebCrawler` class directly in the code snippet to perform web crawling and extract data from a webpage. This is evident from the line `async with AsyncWebCrawler(verbose=True) as crawler:`.",
      "predicted_trace_chain": "css.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document 'css.md' directly mentions and demonstrates the use of 'AsyncWebCrawler' as the main class for orchestrating the crawling and extraction process, making it pivotal in the traceability path.",
      "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
      "ground_truth_trace_chain": "css.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "initializes",
      "relationship_explanation": "The code snippet initializes a `JsonCssExtractionStrategy` with a schema to extract structured data using CSS selectors. This is indicated by the line `extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)`.",
      "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document 'css.md' directly discusses setting up a `JsonCssExtractionStrategy`, clearly specifying its role in defining the schema for data extraction, an essential step in the crawling process.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "invokes",
      "relationship_explanation": "The method `AsyncWebCrawler.arun()` is explicitly called in the provided snippet to perform the crawling operation: `result = await crawler.arun(...)`. This method is crucial for initiating the web retrieval and data extraction process.",
      "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "Starting from 'css.md', 'AsyncWebCrawler' is used to create the crawler instance, which then calls 'arun()'. This method is essential for executing the actual crawling and extraction workflow.",
      "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
      "ground_truth_trace_chain": "css.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "explicit",
      "relationship_type": "accesses",
      "relationship_explanation": "The code accesses `result.extracted_content` to parse and print the extracted cryptocurrency prices with `crypto_prices = json.loads(result.extracted_content)`, demonstrating the use of this attribute from the `CrawlResult` class.",
      "predicted_trace_chain": "css.md -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "The chain starts from the document, progresses through the 'arun()' method which returns a `CrawlResult`. Here, 'extracted_content' is accessed to obtain the parsed data, completing the extraction workflow.",
      "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
      "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
      "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
      "ground_truth_trace_chain": "css.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
      "ground_truth_trace_chain": "css.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
      "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "demonstrates",
      "relationship_explanation": "The documentation snippet provides an example of how `JsonCssExtractionStrategy` can be used to extract structured data from a complex HTML document, specifically referencing its ability to handle nested elements typical of e-commerce site layouts.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation directly discusses and demonstrates various features of `JsonCssExtractionStrategy`, explaining its application in this context.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "While not directly mentioned in the documentation snippet, `LLMExtractionStrategy` is implicitly related as it extends the capabilities of extraction strategies which `JsonCssExtractionStrategy` belongs to. The complexity of the HTML described suggests the use of advanced extraction strategies like LLM for handling intricate data models.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "Starting with `JsonCssExtractionStrategy`, it\u2019s logical to consider `LLMExtractionStrategy` due to its advanced features that could complement the JSON-CSS extraction techniques.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation demonstrates the use of JavaScript execution commands during the crawling process, which aligns with functionalities provided by `AsyncPlaywrightCrawlerStrategy`, specifically through its `crawl` method that enables JavaScript to be executed on a page.",
      "predicted_trace_chain": "page-interaction.md -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The documentation snippet details the use of JavaScript execution for crawling different URL states, indicative of the capabilities provided by `AsyncPlaywrightCrawlerStrategy`, which is the class responsible for this functionality. Although `AsyncPlaywrightCrawlerStrategy` isn't explicitly mentioned, it is responsible for the described actions.",
      "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `arun()` method is used explicitly in the examples where JavaScript code is executed during crawling. This directly correlates to the method being responsible for executing such logic via the crawler strategy.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation snippet calls `arun()` method directly, which then utilizes an instance of `AsyncPlaywrightCrawlerStrategy` to execute JavaScript commands as part of URL crawling.",
      "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
      "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly demonstrates setting up a proxy with the `AsyncWebCrawler` class during instantiation.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document provides an example of using the `AsyncWebCrawler` class directly with proxy setup, establishing a clear usage relationship as outlined in the proxy-security documentation.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "Even though the method `arun()` is not explicitly mentioned, the snippet demonstrates using the method within the `AsyncWebCrawler` context, indicating its role in initiating a crawl operation.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The snippet demonstrates using `arun()` within an instance of `AsyncWebCrawler`, implicitly indicating the workflow from instantiation to the method being a necessary step in the crawling process.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
      "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions the use of the CosineStrategy class for different extraction strategies. Each use case initializes this strategy with different parameters for specific content extraction tasks.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "CosineStrategy is directly mentioned and used within the documented scenarios for content extraction tasks. It is a central element in the described use cases, forming the basis for article content, product review analysis, and technical documentation strategies.",
      "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "calls",
      "relationship_explanation": "The documentation snippet includes an example where the AsyncWebCrawler's arun method is used to invoke a crawl operation using the CosineStrategy. Although arun is not explicitly described in the text, its use is demonstrated in example code.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "This trace shows the way the AsyncWebCrawler.arun() method is used alongside CosineStrategy, indicating how these components interact in real use cases described in the documentation. The snippet demonstrates invoking arun with a specified strategy.",
      "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
      "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "inherits",
      "relationship_explanation": "CosineStrategy, which is highlighted in the documentation, inherits from ExtractionStrategy. This base class is not directly mentioned in the snippet but is crucial as it defines the interface and foundational behavior for all extraction strategies, including CosineStrategy.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "The trace is extended to include the relationship between CosineStrategy and its superclass ExtractionStrategy, highlighting the base functionality enabled by ExtractionStrategy. This relationship is critical as it demonstrates the strategy design pattern used in the implementation.",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
      "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
      "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
      "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 25,
      "artifact_title": "NlpSentenceChunking",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation snippet explicitly introduces the 'NlpSentenceChunking' class, explaining that it uses NLP models to split text into sentences, indicating that it implements sentence chunking based on natural language processing techniques.",
      "predicted_trace_chain": "chunking.md -> NlpSentenceChunking",
      "predicted_trace_chain_explanation": "The trace starts at the documentation file 'chunking.md', which directly mentions and describes the 'NlpSentenceChunking' class, forming a one-to-one relationship between the document and the code artifact.",
      "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> NlpSentenceChunking",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 6,
      "artifact_title": "ChunkingStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The 'NlpSentenceChunking' class, according to the naming convention and typical class structure, is likely a subclass of 'ChunkingStrategy'. This relationship is implicit as the documentation provides a class snippet that hints at inheritance by highlighting chunking ability based on its purpose.",
      "predicted_trace_chain": "chunking.md -> NlpSentenceChunking -> ChunkingStrategy",
      "predicted_trace_chain_explanation": "The chain begins from the chunking documentation, explicitly linking to 'NlpSentenceChunking', which, by inference of typical programming patterns and naming, extends the 'ChunkingStrategy', outlining a pathway from the documentation to the base strategy class.",
      "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `AsyncWebCrawler` class is directly used in the provided code snippet (`async with AsyncWebCrawler(verbose=True) as crawler`). This indicates that the functionality explained in the documentation relies on this class to perform web crawling.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document introduces `AsyncWebCrawler` to highlight its role in enabling advanced session-based crawling with dynamic content. This serves as the foundation for the discussed crawling strategy.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "Although not directly mentioned, `AsyncWebCrawler` is initialized with a strategy, which is implemented by `AsyncPlaywrightCrawlerStrategy`. This is critical to enabling the crawling operations described, particularly for handling JavaScript and dynamic content.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The documentation implicitly requires the use of `AsyncPlaywrightCrawlerStrategy` via `AsyncWebCrawler` to enable advanced features like dynamic content handling. This strategy provides the necessary implementations to interact with Playwright's functions.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `JsonCssExtractionStrategy` is explicitly instantiated and utilized in the code snippet (`extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)`). This strategy is tasked with extracting data from the crawled pages as described in the documentation.",
      "predicted_trace_chain": "quickstart.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document demonstrates the usage of `JsonCssExtractionStrategy` to extract structured data from dynamic web content. This highlights its role in data extraction within the described crawling process.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
      "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `kill_session` method of `AsyncPlaywrightCrawlerStrategy` is explicitly called in the code (`await crawler.crawler_strategy.kill_session(session_id)`). It stops a session which is necessary for handling dynamic pagination in the crawling process.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "The document outlines the termination of sessions using `kill_session` to reset or stop current sessions, ensuring the crawler handles state changes and dynamic data effectively during the crawling process.",
      "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
      "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncWebCrawler.arun()",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 12,
      "artifact_title": "CrawlResult.fit_markdown",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly demonstrates the use of 'fit_markdown' from CrawlResult as ideal for handling blog post and news article content, showing its utility within usage examples.",
      "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.fit_markdown",
      "predicted_trace_chain_explanation": "The document 'content-processing.md' discusses best practices for processing content, leading to the use of the 'CrawlResult' class where 'fit_markdown' is accessed as an attribute to handle formatted markdown content.",
      "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.fit_markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 16,
      "artifact_title": "CrawlResult.media",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet shows how to handle media by using 'media' from CrawlResult to filter images based on their relevance score, suggesting its role in media processing.",
      "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.media",
      "predicted_trace_chain_explanation": "The document addresses how to manage media content processing separated from the primary content, which traces to the 'media' attribute in 'CrawlResult' to filter images.",
      "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.media",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 14,
      "artifact_title": "CrawlResult.links",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explains how to select internal links related to content using 'links' from CrawlResult, demonstrating direct application within practical examples.",
      "predicted_trace_chain": "content-processing.md -> CrawlResult -> CrawlResult.links",
      "predicted_trace_chain_explanation": "Content processing involves link analysis, traced to the 'links' attribute of 'CrawlResult', showing specific usage in selecting content-related internal links.",
      "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.links",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet includes a direct call to 'arun', which is a method of 'AsyncWebCrawler', for cleaning content purposefully, seen in a usage example.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation provides a practical example demonstrating how to call 'arun' from 'AsyncWebCrawler' for custom content cleaning, well-integrated within content optimization practices.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet directly shows usage of the AsyncWebCrawler class in a sample script, highlighting how to instantiate and use it within an asynchronous context.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The AsyncWebCrawler is directly referred to in the documentation, making it the endpoint of the trace chain for this document.",
      "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "Within the example script, the AsyncWebCrawler is employed to call the 'arun' method, indicating its role in executing the crawling task.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation leads from the instantiation of AsyncWebCrawler to its method 'arun()', demonstrating its usage in the crawling process.",
      "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "implicit",
      "relationship_type": "returns",
      "relationship_explanation": "The 'arun' method is inferred to return a CrawlResult object, as indicated by the print statement within the documentation snippet that accesses the 'markdown' attribute, which is part of a CrawlResult.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
      "predicted_trace_chain_explanation": "The methodology in the document describes a flow from AsyncWebCrawler to calling 'arun()', which returns CrawlResult, forming the final node in this trace chain.",
      "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 15,
      "artifact_title": "CrawlResult.markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "accesses",
      "relationship_explanation": "The 'result.markdown' access in the snippet suggests that the 'markdown' attribute is part of the returned CrawlResult instance, demonstrating direct access to the value stored in this attribute.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
      "predicted_trace_chain_explanation": "This step connects usage patterns in the code example, culminating in the access of the 'markdown' attribute on the CrawlResult returned by the 'arun()' method.",
      "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "usage",
      "relationship_explanation": "The documentation snippet is explicitly using the class `AsyncWebCrawler` in the example provided to demonstrate basic configuration settings such as `headless`, `verbose`, and `sleep_on_close`.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation snippet directly references and utilizes the `AsyncWebCrawler` class, indicating its central role in setting up and managing the basic configuration for asynchronous web crawling as described.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "invocation",
      "relationship_explanation": "The method `arun` of `AsyncWebCrawler` is implicitly invoked in the example snippet to perform the crawling action (`result = await crawler.arun(url=\"https://example.com\")`).",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "Starting from the documentation reference to `AsyncWebCrawler`, it shows an example usage where `arun()` is called on an instance of `AsyncWebCrawler`. This demonstrates how methods within the `AsyncWebCrawler` class are utilized in practice, forming an essential part of the usage pathway.",
      "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "Although not directly referenced in the snippet, the class `AsyncWebCrawler` is initialized to use `AsyncPlaywrightCrawlerStrategy` as its default strategy, which becomes part of its `crawl` method calls inherently. This connection is necessary for fully understanding the implementation of asynchronous operations within `AsyncWebCrawler`.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The documented example in `browser-config.md` involves configuring `AsyncWebCrawler`, which by default integrates `AsyncPlaywrightCrawlerStrategy`. Thus, learning about `AsyncPlaywrightCrawlerStrategy` is crucial for understanding what strategy `AsyncWebCrawler` employs during operations like invoking `arun()`.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly refers to using 'crawler.arun()' which directly indicates the usage of the 'AsyncWebCrawler' class as it carries out the 'arun' method for executing the crawl.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation specifically provides a code example using 'crawler.arun()', pointing directly to the 'AsyncWebCrawler' class in which this method is defined.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The snippet's code example demonstrates calling 'crawler.arun()' with multiple relevant parameters like 'url', 'word_count_threshold', and 'remove_overlay_elements', directly involving the 'arun' method to implement the functionality described for cleaning content.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The detailed code example in the snippet directly matches the 'arun' method signature and usage, tracing from the 'AsyncWebCrawler' to the specific method reflecting practical implementation.",
      "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 9,
      "artifact_title": "CrawlResult.cleaned_html",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet refers to 'print(result.cleaned_html)', which implies the 'CrawlResult' model's 'cleaned_html' attribute generated by the 'arun()' method's return, contributing to showing clean HTML.",
      "predicted_trace_chain": "content-processing.md -> CrawlResult.cleaned_html",
      "predicted_trace_chain_explanation": "The 'cleaned_html' attribute is the conclusion of the 'arun' process, providing clean page content, represented in the documentation as output via 'result.cleaned_html'.",
      "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.cleaned_html",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 15,
      "artifact_title": "CrawlResult.markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet includes 'print(result.markdown)', which implicitly traces to 'CrawlResult.markdown', showcasing its role in providing a markdown version of the cleaned content, derived from the crawl result returned by the 'arun' method.",
      "predicted_trace_chain": "content-processing.md -> CrawlResult.markdown",
      "predicted_trace_chain_explanation": "The 'markdown' attribute is part of the output from the crawler as in 'result.markdown', indicating its relevance in showcasing markdown output in the documentation example.",
      "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The text explicitly demonstrates the usage of the 'arun' method of 'AsyncWebCrawler' to execute JavaScript on a webpage before crawling it. The method name 'arun' is directly mentioned and used in the example code provided.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation provides an example specifically using 'arun' to execute JavaScript code on a webpage, marking it as an explicit trace to the 'AsyncWebCrawler.arun()' function.",
      "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "Although not directly mentioned in the text, 'AsyncWebCrawler' utilizes 'AsyncCrawlerStrategy', and its default implementation is 'AsyncPlaywrightCrawlerStrategy'. This is inferred from the context as 'AsyncPlaywrightCrawlerStrategy' implements the async strategy pattern that supports JavaScript execution, a functionality showcased by the 'arun' method in the example.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "'AsyncWebCrawler.arun()' uses 'AsyncCrawlerStrategy', particularly in its default design, which is usually handled by 'AsyncPlaywrightCrawlerStrategy'. This chain is necessary to understand how JavaScript execution is supported and managed by 'arun' within 'AsyncWebCrawler'.",
      "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'LLMExtractionStrategy' class is explicitly mentioned in the documentation snippet as being used to support multiple LLM providers for extraction, demonstrated through examples using different provider names and API keys.",
      "predicted_trace_chain": "quickstart.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation discusses how to use the LLM extraction strategy, and the class LLMExtractionStrategy directly implements this functionality, making it the endpoint of this trace.",
      "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
      "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
      "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The text snippet mentions instantiating an 'AsyncWebCrawler' with custom headers, directly relating to the class 'AsyncWebCrawler'. This demonstrates the usage of 'AsyncWebCrawler' for initializing a crawler with specific headers.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document 'proxy-security.md' directly references the 'AsyncWebCrawler' class in the code example, showing its use in the initialization with custom headers.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "Within the code snippet, the 'arun' method of an 'AsyncWebCrawler' instance is called to perform a crawling operation. This suggests that the method is part of the public interface of the 'AsyncWebCrawler' class for executing the crawl.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document shows using 'arun()' method within an 'AsyncWebCrawler' context, establishing a direct relationship where 'proxy-security.md' illustrates 'arun()' usage as an operation feature of the 'AsyncWebCrawler'.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
      "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions and demonstrates the use of `AsyncWebCrawler` in the `main` function, showing how to use it with the `extract_with_retry` function for handling retries during extraction.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The `llm.md` file directly refers to `AsyncWebCrawler` within the code example as the instantiated class used for web crawling operations.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "creates",
      "relationship_explanation": "The snippet directly refers to `LLMExtractionStrategy` as a parameter when creating an instance, highlighting its role in the strategy pattern used for extracting content through LLM APIs.",
      "predicted_trace_chain": "llm.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The `llm.md` document explicitly creates an instance of `LLMExtractionStrategy` as part of the retry logic showcased in the example.",
      "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "calls",
      "relationship_explanation": "Within the retry function `extract_with_retry`, `arun()` method of `AsyncWebCrawler` is called to execute the web crawling and extraction process based on the given URL and strategy, demonstrating its use implicitly.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The method `arun()` from `AsyncWebCrawler` is a crucial step called by `extract_with_retry` in the `llm.md` to perform the actual extraction process.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
      "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
      "ground_truth_trace_chain": "llm.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
      "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
      "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly mentions the 'kill_session' method as a way to clean up session resources in the 'Resource Management' section of the provided snippet.",
      "predicted_trace_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "The session management documentation provides a coding example where the 'kill_session' method is utilized, indicating direct usage and therefore implying its implementation as a part of ActivePlaywrightCrawlerStrategy.",
      "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
      "ground_truth_trace_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'arun' method of the 'AsyncWebCrawler' is explicitly called in examples for managing sessions in the 'State Management' section. This indicates its use in executing crawl operations with specific session IDs.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation demonstrates usage of 'AsyncWebCrawler.arun' to manage state transitions (e.g., from login to dashboard) during a web crawl, explicitly showing the method in two separate code examples related to session management.",
      "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
      "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "inherits",
      "relationship_explanation": "The 'AsyncWebCrawler' implicitly relies on 'AsyncPlaywrightCrawlerStrategy', as it is used as a default strategy according to its class definition. The snippet does not directly mention AsyncPlaywrightCrawlerStrategy, but it is integral given its involvement in session handling via 'kill_session'.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' internally uses methods from the 'AsyncPlaywrightCrawlerStrategy', which directly supports actions like session management. Although not directly quoted from the snippet, the linkage is inferred as 'AsyncPlaywrightCrawlerStrategy' extends necessary functionalities to the web crawler.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
      "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
      "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
      "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
      "ground_truth_trace_chain": "session-management.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The method `arun()` is explicitly called in the provided code snippet. It is used to perform a web crawling operation asynchronously on a given URL.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document refers directly to using `arun()` method for obtaining crawling results from a URL, demonstrating its application in generating markdown content.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 15,
      "artifact_title": "CrawlResult.markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "extracted",
      "relationship_explanation": "The `markdown` attribute of the `CrawlResult` class is accessed after calling `arun()`, suggesting that it holds the markdown version of the crawled HTML. This is inferred from the example where `result.markdown` is printed.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult.markdown",
      "predicted_trace_chain_explanation": "The `arun()` method, part of the `AsyncWebCrawler`, returns a `CrawlResult` object, which includes the `markdown` attribute accessed in the example to output the markdown content.",
      "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The example code snippet from the documentation uses 'arun()' method of the 'AsyncWebCrawler' class to initiate crawling on a URL (e.g., 'https://example.com').",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation snippet explicitly demonstrates the usage of the 'arun()' method to perform crawling.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_id": 14,
      "artifact_title": "CrawlResult.links",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation demonstrates accessing the 'links' attribute of the result from 'arun()', using it to classify and analyze links such as 'internal' and 'external'.",
      "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult.links",
      "predicted_trace_chain_explanation": "The 'arun()' method returns a 'CrawlResult' object, which has a 'links' attribute used for link classification as shown in the documentation.",
      "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult.links",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
      "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
      "document_location": "docs/md_v2/advanced/content-processing.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
      "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly mentions the usage of 'AsyncWebCrawler' in the example function 'crawl_with_advanced_config'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document 'browser-config.md' contains a Python code example demonstrating the instantiation and usage of the 'AsyncWebCrawler' class.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The method 'arun' is called on an instance of 'AsyncWebCrawler' to perform the crawl operation as demonstrated in the provided code.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The 'arun' method is invoked after creating an 'AsyncWebCrawler' instance, thereby concluding the crawl operation as shown in the example.",
      "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'AsyncWebCrawler' uses 'AsyncPlaywrightCrawlerStrategy' implicitly since it's the default strategy used when no strategy is provided during the initialization of 'AsyncWebCrawler'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "Since 'AsyncWebCrawler' depends on a crawler strategy and defaults to 'AsyncPlaywrightCrawlerStrategy', there's an implicit pathway from the documentation example to this strategy.",
      "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The method 'arun' returns a 'CrawlResult' object, which contains the 'extracted_content' attribute accessed in the example function.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "The 'arun' method returns a 'CrawlResult', inside which 'extracted_content' is called implicitly in handling the crawling result.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 13,
      "artifact_title": "CrawlResult.html",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'arun' method's result could indirectly involve the 'html' attribute of 'CrawlResult'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
      "predicted_trace_chain_explanation": "The 'html' content within 'CrawlResult' is likely used implicitly when the result is processed in the example.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult.screenshot",
      "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult.screenshot",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "demonstrated usage",
      "relationship_explanation": "The 'AsyncWebCrawler' class is explicitly instantiated in the example shown in the documentation snippet, which instructs how to enable verbose logging when using this class.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' class is directly demonstrated in the example snippet from 'simple-crawling.md', showing how to activate verbose logging by passing a keyword argument, thus being the focal point of the snippet.",
      "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "demonstrated usage",
      "relationship_explanation": "The method 'arun()' of the 'AsyncWebCrawler' class is used in the example to perform a web crawl. Although not explicitly discussed, its usage is demonstrated in the snippet showing a practical implementation of the method.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The method 'arun()' is part of the implementation steps directly shown in 'simple-crawling.md', used within the 'AsyncWebCrawler' object to achieve the crawling task, completing the example provided for verbose logging.",
      "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation snippet explicitly discusses the 'Cosine Strategy' and its methodology. The implementation of this strategy is found in the 'CosineStrategy' class, which performs tasks such as vectorizing text and clustering based on semantic similarity, as described in the documentation steps.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The documentation 'cosine.md' specifically focuses on the 'Cosine Strategy' and its functions, which are directly implemented in the 'CosineStrategy' class.",
      "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The 'CosineStrategy' class extends the 'ExtractionStrategy'. Although not mentioned in the documentation, this relationship is necessary because 'CosineStrategy' is a specific implementation of an extraction strategy, and the base class likely provides essential methods and properties that are overridden or used.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "The implementation of 'CosineStrategy' extends from 'ExtractionStrategy', making it a specific type of strategy explained in the documentation. This extension is critical to provide a consistent interface and to inherit basic functionality.",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 22,
      "artifact_title": "FixedLengthWordChunking",
      "predicted_relationship": "explicit",
      "relationship_type": "implementation",
      "relationship_explanation": "The documentation explicitly mentions `FixedLengthWordChunking` as the class used for chunking text based on a fixed number of words. The description and usage example directly correlate to how this class is documented to function.",
      "predicted_trace_chain": "chunking.md -> FixedLengthWordChunking",
      "predicted_trace_chain_explanation": "The document provides a direct explanation and code example for the class `FixedLengthWordChunking`, indicating its implementation is the primary focus of this section.",
      "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> FixedLengthWordChunking",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 6,
      "artifact_title": "ChunkingStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "Since `FixedLengthWordChunking` is a class for chunking strategies, and given the naming convention, it likely extends from a base `ChunkingStrategy` class as part of polymorphic design. This assumption is reinforced by the presence of `ChunkingStrategy` in the available artifacts list.",
      "predicted_trace_chain": "chunking.md -> FixedLengthWordChunking -> ChunkingStrategy",
      "predicted_trace_chain_explanation": "Tracing from the documentation focusing on `FixedLengthWordChunking` leads implicitly to `ChunkingStrategy`, suggesting `FixedLengthWordChunking` implements a specialized version of the strategy defined by the base class.",
      "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet demonstrates the use of a schema for CSS-based extraction, which directly correlates with the `JsonCssExtractionStrategy` class. This class is responsible for extracting web content using schemas, as evidenced by its parameters like `baseSelector` and `fields`.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document focuses on defining a schema for content extraction using CSS selectors, which is what the `JsonCssExtractionStrategy` class implements. It is a direct utilization as per the schema-based extraction methodology described in the snippet.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly mentions the use of the `LLMExtractionStrategy` class to extract structured data from the OpenAI pricing page. It details parameters like 'provider', 'api_token', 'schema', 'extraction_type', and 'instruction' used within the strategy.",
      "predicted_trace_chain": "llm.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The trace starts with the llm.md document and explicitly mentions the utilization of the LLMExtractionStrategy class.",
      "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation specifies the creation of an `AsyncWebCrawler` instance, which is used to execute the `arun` function, thus employing the `AsyncWebCrawler` class directly in the example.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The trace starts with the llm.md document, with the `AsyncWebCrawler` being directly instantiated and used within the example to demonstrate async crawling.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "invoked",
      "relationship_explanation": "Within the usage of `AsyncWebCrawler`, the `arun` method is invoked to crawl the specified OpenAI pricing URL, this is shown as part of the example implementation.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The trace begins at llm.md, references the use of the AsyncWebCrawler, and progresses to its arun method being utilized in example code.",
      "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
      "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
      "ground_truth_trace_chain": "llm.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
      "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
      "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation mentions 'Crawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data...' indicating AsyncWebCrawler is designed to handle LLM-based data extraction.",
      "predicted_trace_chain": "llm.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document directly references AsyncWebCrawler, which suggests its use for LLM-based extraction tasks.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
      "ground_truth_trace_chain": "llm.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions 'LLMExtractionStrategy' as a component used with AsyncWebCrawler, likely indicating its role in the logic for extracting information.",
      "predicted_trace_chain": "llm.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The document directly references LLMExtractionStrategy as one of the strategies used in conjunction with AsyncWebCrawler.",
      "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "While not explicitly mentioned, LLMExtractionStrategy is a specialized implementation of an extraction strategy, extending the base class ExtractionStrategy, indicating a hierarchy of strategy implementations possibly used in extraction tasks.",
      "predicted_trace_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "LLMExtractionStrategy is directly related to ExtractionStrategy as it extends this base class, indicating a need to start from the base strategy class to understand its advanced uses.",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet shows the use of 'AsyncWebCrawler', starting the asynchronous web crawling session with the 'async with' statement and configurations like 'verbose=True'.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "'session-management.md' directly references 'AsyncWebCrawler' to demonstrate how crawling sessions work in practical scenarios, forming the main class utilized in examples involving crawling workflow.",
      "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
      "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "calls",
      "relationship_explanation": "The '.arun()' method of 'AsyncWebCrawler' is invoked within the crawl_dynamic_content() function as part of the session management process for crawling GitHub commit pages with parameters like 'url', 'session_id', and 'extraction_strategy'.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The snippet shows a procedure where the 'arun()' method of 'AsyncWebCrawler' is integral to processing each page of the commits, specifying extraction strategy and session ID.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
      "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 24,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet explicitly creates an instance of 'JsonCssExtractionStrategy' using a schema for extracting commit details like 'title' from HTML elements on the page.",
      "predicted_trace_chain": "session-management.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "'session-management.md' specifies a schema-based extraction strategy using 'JsonCssExtractionStrategy', directly showing its role in content extraction during the crawling process.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
      "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "'kill_session()' is used to clean up the crawling session at the end of the process. It's invoked on the 'AsyncWebCrawler.crawler_strategy' within the code.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "After completing the crawl operation, session cleanup is a necessary step using 'kill_session()' of the underlying strategy linked to 'AsyncWebCrawler'. This ensures resource management and session closure.",
      "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
      "ground_truth_trace_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "'AsyncPlaywrightCrawlerStrategy', which contains the 'kill_session' method mentioned, extends 'AsyncCrawlerStrategy', serving as a base class defining the interface for all crawler strategies.",
      "predicted_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "The snippet describes operations covered by 'AsyncCrawlerStrategy' derivatives, showing how different strategy functionalities work under high-level session management in 'session-management.md'.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
      "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "As a concrete implementation of 'AsyncCrawlerStrategy', 'AsyncPlaywrightCrawlerStrategy' features methods used and extended by 'AsyncWebCrawler', including session control as shown.",
      "predicted_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "Operations detailed in 'session-management.md' involve running and cleaning up sessions, which are controlled by implementing 'AsyncPlaywrightCrawlerStrategy'.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
      "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
      "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
      "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
      "ground_truth_trace_chain": "session-management.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
      "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly mentions 'JsonCssExtractionStrategy' as a tool for extracting highly structured data from complex web pages. It outlines advanced usage tips for this strategy.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document addresses JsonCssExtractionStrategy specifically, providing direct tips for its use, thus forming a direct trace.",
      "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "JsonCssExtractionStrategy, by dealing with data extraction, implicitly relies on overall crawler strategies, such as 'AsyncCrawlerStrategy', for retrieving the content to be processed.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "JsonCssExtractionStrategy likely operates on data provided by a broader crawler strategy like AsyncCrawlerStrategy, establishing a chain of reliance.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly describes using JsonCssExtractionStrategy for extracting patterned information from web pages. This is demonstrated with example code that directly instantiates and utilizes this class.",
      "predicted_trace_chain": "output-formats.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document explains a pattern-based extraction technique, which involves creating a JsonCssExtractionStrategy object to handle structured data extraction from HTML using CSS selectors, linking directly to the JsonCssExtractionStrategy class.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
      "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The code example shows the use of the `arun` method from an object `crawler`, which indicates the use of `AsyncWebCrawler`. It indirectly implies that AsyncWebCrawler utilizes the JsonCssExtractionStrategy as a strategy for extracting data from the given URL.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The example provided uses `crawler.arun`, suggesting that the crawler is an instance of `AsyncWebCrawler`. This pathway signifies that for complex extraction processes on web pages using various strategies including JsonCssExtractionStrategy, AsyncWebCrawler orchestrates the procedure.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "invokes",
      "relationship_explanation": "The documentation snippet instructs using the `arun` method while demonstrating the extraction strategy, implying its role in submitting requests and applying extraction strategies like JsonCssExtractionStrategy.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document uses `arun` within AsyncWebCrawler for executing extraction strategies, showing that arun serves as a vital function processing the provided URL and applying strategies, thus completing the extraction workflow.",
      "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "accesses",
      "relationship_explanation": "In the document, `extracted_content` is accessed from the result of `arun`, illustrating usage of CrawlResult attributes. This indicates linking between extraction strategy results and CrawlResult's structure during retrieval.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "Illustrates accessing extracted results' content, showing how CrawlResult stores data retrieved by extraction strategies employed during the crawl process, processing the content extracted by the strategy.",
      "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
      "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet clearly demonstrates the usage of the 'AsyncWebCrawler' class as the context manager that facilitates the crawling operation.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The chain starts with the document section focusing on user simulation with the explicit use of the 'AsyncWebCrawler' class in the example code.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'arun' method of the 'AsyncWebCrawler' is explicitly shown being used in the documentation to perform a crawling operation with user simulation parameters.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "Following from the 'AsyncWebCrawler' class, the 'arun' method is the next element, representing the primary action performed during the crawling session as indicated in the code snippet.",
      "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "While not directly mentioned, 'AsyncPlaywrightCrawlerStrategy' is implicitly involved as it is a possible default strategy used by 'AsyncWebCrawler' for actual crawling operations, particularly under user simulation settings.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "From the 'AsyncWebCrawler', tracking to 'AsyncPlaywrightCrawlerStrategy' follows its role as a strategy that implements crawling tasks necessary when user simulation flags are enabled.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The 'AsyncPlaywrightCrawlerStrategy' extends 'AsyncCrawlerStrategy', implicitly adding methods for crawling under different settings mentioned in the documentation.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "This pathway clarifies how 'AsyncCrawlerStrategy' sets the abstract foundation for crawling strategies like 'AsyncPlaywrightCrawlerStrategy', which is hooked into the 'AsyncWebCrawler' for its operations.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions 'CosineStrategy' as the strategy used for semantic filtering and clustering content. The parameters 'semantic_filter', 'word_count_threshold', and 'sim_threshold' are directly used in the snippet, indicating their role in configuring the strategy.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The 'cosine.md' document provides a usage example where 'CosineStrategy' is instantiated with specific parameters, making the strategy a core implementation component directly related to the document.",
      "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "In the provided code snippet, 'AsyncWebCrawler' is instantiated within an 'async with' statement, indicating its use in an asynchronous context to perform web crawling operations. While not the primary focus, it is essential for executing the strategy provided by 'CosineStrategy'.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The 'cosine.md' documentation leverages 'AsyncWebCrawler' to execute the crawling process with a given 'CosineStrategy', demonstrating how 'AsyncWebCrawler' integrates with different strategies to facilitate operations.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
      "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "invokes",
      "relationship_explanation": "The documentation snippet shows the use of 'arun()' method within 'AsyncWebCrawler'. It implicitly indicates this method is responsible for executing the crawling with the specified 'CosineStrategy' and the URL 'https://example.com/reviews'.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "'arun()' method is implicitly connected as it performs the operation described in the documentation, tying 'AsyncWebCrawler' to 'CosineStrategy' by executing the crawling process with the strategy's configuration.",
      "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
      "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "accesses",
      "relationship_explanation": "The snippet accesses 'result.extracted_content' indicating a reliance on 'CrawlResult' structure to retrieve the content extracted by the 'CosineStrategy'. It shows how outputs of the crawling process are structured.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "The transition from using 'arun()' to accessing 'extracted_content' in 'CrawlResult' illustrates the typical results capture from an asynchronous crawl process, showing the full pathway from initiation to result access using defined program structures.",
      "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
      "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
      "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The code snippet in the documentation directly mentions 'AsyncWebCrawler', which is an instantiated class. The snippet uses 'AsyncWebCrawler' in an asynchronous context for web crawling with an option for verbosity.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation details how to instantiate and use 'AsyncWebCrawler' within an async function. This is the primary artifact for this functionality.",
      "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The function 'arun' is explicitly invoked in the snippet, where a URL is passed to initiate a web crawl. This demonstrates direct usage of 'arun' as part of 'AsyncWebCrawler'.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation showcases how to run a web crawl using 'arun'. The trace indicates the method 'arun' belongs to the 'AsyncWebCrawler' class.",
      "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 15,
      "artifact_title": "CrawlResult.markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation prints 'result.markdown', implying 'result' returns an instance with 'markdown' content. This suggests a reliance on 'CrawlResult' where 'markdown' is an attribute.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.markdown",
      "predicted_trace_chain_explanation": "The snippet captures the output of 'arun' in 'result', which accesses 'markdown', a property of the response object from 'arun'. This connection demonstrates how results are handled post-crawl.",
      "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "implicit",
      "relationship_type": "returns",
      "relationship_explanation": "While not explicitly named, the documentation implies that 'arun' returns a 'CrawlResult' instance. This is inferred from accessing 'markdown' in the 'result'.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
      "predicted_trace_chain_explanation": "The result of 'arun', being used subsequently, implies it is a 'CrawlResult' as it utilizes its attributes. Therefore, 'arun' acts as a factory for creating 'CrawlResult'.",
      "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The document mentions different strategies like LLM and Cosine for extraction, which are types of 'ExtractionStrategy'. This suggests that 'ExtractionStrategy' is used as a base class/interface for these specific strategies.",
      "predicted_trace_chain": "overview.md -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "The document discusses the potential strategies, referring to the overarching base class or interface as 'ExtractionStrategy'. This is the starting point for different configurations, making it necessary to establish the relationship here for extensibility and polymorphism.",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The document mentions using 'LLM for complex interpretation', which directly corresponds to the 'LLMExtractionStrategy' class implementing such functionality.",
      "predicted_trace_chain": "overview.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The document mentions this strategy specifically for complex interpretations, making a direct link to the specialized strategy class needed for advanced extraction activities.",
      "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The document recommends 'Cosine for content relevance', indicating that the 'CosineStrategy' explicitly addresses this recommendation by implementing relevant functionalities.",
      "predicted_trace_chain": "overview.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The document suggests Cosine for relevance, directly linking to the 'CosineStrategy', which specifically focuses on content relevance extraction capabilities.",
      "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The code example in the document demonstrates the usage of 'arun()' method of 'AsyncWebCrawler' to perform crawling and handle potential errors during content extraction.",
      "predicted_trace_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document provides an example calling 'arun()' method which belongs to 'AsyncWebCrawler', forming a chain from general documentation to specific implementation and usage of this method.",
      "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 10,
      "artifact_title": "CrawlResult.error_message",
      "predicted_relationship": "implicit",
      "relationship_type": "attributes",
      "relationship_explanation": "The document's example uses 'result.error_message' to check and report errors if crawling fails, indicating that 'error_message' is a critical attribute for error handling.",
      "predicted_trace_chain": "overview.md -> CrawlResult -> CrawlResult.error_message",
      "predicted_trace_chain_explanation": "To handle an error, the document's example demonstrates attribute 'error_message' within 'CrawlResult', making it a significant point in understanding the result's state for troubleshooting.",
      "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
      "ground_truth_trace_chain": "overview.md -> CrawlResult.error_message",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "attributes",
      "relationship_explanation": "The document implies the usage of 'result.extracted_content' to process or load data post-extraction, signifying its role as a primary attribute for handling extracted information.",
      "predicted_trace_chain": "overview.md -> CrawlResult -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "In the document's example, 'extracted_content' is accessed to obtain the extracted data, emphasizing its key position in the data processing workflow post-crawl.",
      "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
      "ground_truth_trace_chain": "overview.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
      "ground_truth_trace_chain": "overview.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "JsonCssExtractionStrategy",
      "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
      "ground_truth_trace_chain": "overview.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly discusses the 'advanced usage of JsonCssExtractionStrategy', which directly refers to this class designed for complex HTML structure extraction.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document 'css-advanced.md' talks about the 'advanced usage of JsonCssExtractionStrategy', indicating a direct relationship to the class's functionality in handling complex structural scenarios with nested elements.",
      "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "JsonCssExtractionStrategy extends from ExtractionStrategy. This implicit connection stems from class hierarchy, as JsonCssExtractionStrategy inherits base features for extraction strategies from ExtractionStrategy.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "The advanced document explores JsonCssExtractionStrategy, which as a class hierarchy construct extends ExtractionStrategy. This hierarchy allows JsonCssExtractionStrategy to inherit common extraction capabilities, thus forming the chain.",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation directly references `AsyncWebCrawler` as the context manager used for initiating the session.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The role of `AsyncWebCrawler` is pivotal as it encapsulates the session lifecycle described in the documentation, making it the starting point for the described operations.",
      "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
      "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "method invocation",
      "relationship_explanation": "The method `arun()` is explicitly used in the code example to perform actions within a session using the `session_id`.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The `AsyncWebCrawler` class employs the `arun()` method to execute the crawl as illustrated in the documentation, where its role is crucial for the session handling described.",
      "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
      "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "Though not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is the default crawler strategy employed by `AsyncWebCrawler` as inferred from the constructor logic not shown in the snippet. It relates as the underlying strategy implementation.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The documentation snippet relies on `AsyncWebCrawler`'s default strategy for executing crawling operations, whereby `AsyncPlaywrightCrawlerStrategy` is the default strategy, implementing key crawling mechanisms unseen in the documentation.",
      "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
      "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "explicit",
      "relationship_type": "method invocation",
      "relationship_explanation": "The `kill_session()` method is explicitly invoked in the example to terminate the session identified by `session_id`.",
      "predicted_trace_chain": "session-management.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "The `kill_session()` method within `AsyncPlaywrightCrawlerStrategy` is called to properly close the session, directly fulfilling the final operational step described in the doc.",
      "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
      "ground_truth_trace_chain": "session-management.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
      "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/advanced/session-management.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
      "ground_truth_trace_chain": "session-management.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "usage",
      "relationship_explanation": "The documentation snippet explicitly demonstrates using the 'AsyncWebCrawler' class when discussing proxy rotation, directly instantiating this class.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document directly mentions 'AsyncWebCrawler' as part of the example usage for rotating proxies, indicating usage as a central mechanism for crawling with rotated proxies.",
      "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "usage",
      "relationship_explanation": "Within the documentation snippet, 'arun' is explicitly called on the 'crawler' object, which is an instance of 'AsyncWebCrawler'. This indicates its usage in executing a crawl with the specified URL.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The example shows how 'arun' is used on an 'AsyncWebCrawler' instance to perform the crawling process for each URL, demonstrating a key method in its functionality.",
      "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
      "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation provides a code snippet using 'crawler.arun' to execute a crawl operation. This directly relates to the 'AsyncWebCrawler' class, which includes the 'arun' method as part of its interface.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation in 'browser-config.md' directly references functionality provided by the 'AsyncWebCrawler' class via the 'arun' method, which is part of its public interface.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The method 'arun' is explicitly referenced in the example within the 'browser-config.md' documentation. This indicates the method's role in initiating the crawling process with specific arguments like 'screenshot=True'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The chain follows the documentation reference to the 'AsyncWebCrawler' class and specifically to the 'arun' method, demonstrating its role in the operation.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 18,
      "artifact_title": "CrawlResult.screenshot",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The result of the 'arun' method is checked for the presence of a 'screenshot', which is a base64 encoded image. This refers to the 'CrawlResult.screenshot' attribute, implicitly demonstrating its use in handling screenshot data.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> CrawlResult.screenshot",
      "predicted_trace_chain_explanation": "The chain indicates how 'arun' returns a 'CrawlResult', and the 'screenshot' attribute is directly checked in the provided code snippet, implying its usage in the flow.",
      "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage screenshot as a base64-encoded string that can be decoded into a PNG image file.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult.screenshot",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet directly references the AsyncWebCrawler class in the example function integrated_js_and_wait_crawl. It is used as the context manager in an asynchronous call, demonstrating the instantiation and its role in managing crawling operations.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document describes an advanced technique using integrated JavaScript execution within a web crawling context, which necessitates the use of AsyncWebCrawler to execute asynchronous crawling strategies.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "invokes",
      "relationship_explanation": "Within the example function in the text, the async method arun of the AsyncWebCrawler instance is explicitly invoked. The arun method is used iteratively for each page to manage the crawling process alongside setting various options like session_id and js_code.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document explains a mechanism for crawling across pages by invoking a method of the AsyncWebCrawler, showing the progression from the overarching strategy to a specific execution step.",
      "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 24,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "configures",
      "relationship_explanation": "The documentation includes explicit configuration of the JsonCssExtractionStrategy in the example. This extraction strategy is set up with a schema to extract commits, indicating its role in defining how data is retrieved from the pages.",
      "predicted_trace_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The explanation shows the definition and usage of a strategy within the mentioned advanced technique, highlighting configuration details necessary for its operation.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
      "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "explicit",
      "relationship_type": "invokes",
      "relationship_explanation": "The text snippet explicitly invokes the kill_session method to close the session with the identifier 'integrated_session'. This method plays a crucial role in session management, as described in the documentation.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "The method is used within the context of the described technique for properly terminating a crawling session, demonstrating best practices in resource management during asynchronous crawling.",
      "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
      "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
      "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
      "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions 'AsyncWebCrawler' in the example code as the class used to perform web crawling.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' is directly used in the example within the given documentation snippet, showcasing the class used for creating a crawling session.",
      "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation includes an example calling 'arun()' method to demonstrate crawling with and without bypassing the cache.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The trace shows direct invocation of 'arun()' method from 'AsyncWebCrawler', demonstrating its usage for initiating web crawling with specific parameters.",
      "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "implicit",
      "relationship_type": "returns",
      "relationship_explanation": "In the example, the result returned from 'arun()' method is used, which is of 'CrawlResult' type according to the method's return type.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
      "predicted_trace_chain_explanation": "The 'CrawlResult' is indirectly referred as the output of 'arun()', indicated by accessing the 'markdown' property on 'result1' and 'result2' in the example, linking back to 'CrawlResult'.",
      "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 15,
      "artifact_title": "CrawlResult.markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "accesses",
      "relationship_explanation": "The documentation example uses 'result1.markdown' and 'result2.markdown' which accesses the 'markdown' field in 'CrawlResult'.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.markdown",
      "predicted_trace_chain_explanation": "As part of evaluating 'result1' and 'result2', the 'arun()' method returns an instance of 'CrawlResult', with the 'markdown' attribute accessed to demonstrate its usage.",
      "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "is_default_strategy",
      "relationship_explanation": "Although not directly mentioned, 'AsyncPlaywrightCrawlerStrategy' is the default crawler strategy used unless otherwise specified.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' defaults to using 'AsyncPlaywrightCrawlerStrategy' if no specific strategy is passed, linking this class in typical use.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation snippet describes defining a schema for CSS-based data extraction with references to JSON-like structures, including 'nested' and 'list' types, similar to what the JsonCssExtractionStrategy class is designed to interpret and use for extracting HTML content.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "JsonCssExtractionStrategy is directly responsible for interpreting and applying these JSON schema formats to perform extraction from HTML structures.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The text snippet directly mentions the `arun` method in a Python code example, indicating its usage for retrieving raw HTML of a webpage.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document `output-formats.md` provides a code example using `AsyncWebCrawler.arun()` to retrieve HTML data, emphasizing its role in handling the raw HTML structure.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 13,
      "artifact_title": "CrawlResult.html",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The text snippet shows accessing the `html` attribute from the result of `arun()`, demonstrating its role in storing and returning the raw HTML obtained through the crawling process.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> CrawlResult.html",
      "predicted_trace_chain_explanation": "The document outlines retrieving the raw HTML which is facilitated by invoking `AsyncWebCrawler.arun()`, which returns a `CrawlResult` where the `html` attribute stores the resultant content.",
      "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.html",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'E-commerce Scraping' use case explicitly mentions a 'CSS Strategy' with a JSON-based schema definition, which aligns with the 'JsonCssExtractionStrategy'. The strategy is designed to extract elements from a webpage using CSS selectors specified in a schema.",
      "predicted_trace_chain": "overview.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation directly discusses using a CSS strategy for extraction, referring to the capabilities provided by JsonCssExtractionStrategy, making it the endpoint of the trace.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'News Article Extraction' use case mentions 'LLM Strategy', which corresponds to the 'LLMExtractionStrategy'. This strategy utilizes a language model to extract content from web pages, specifically using schemas provided by model-based APIs.",
      "predicted_trace_chain": "overview.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The reference to using an LLM strategy with a model provider directly points to LLMExtractionStrategy, making it a direct trace in terms of implementation.",
      "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'Content Analysis' use case describes the usage of a 'Cosine Strategy' for topic analysis. This aligns with the 'CosineStrategy' class, which employs cosine similarity techniques for document analysis and clustering.",
      "predicted_trace_chain": "overview.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The use of 'Cosine Strategy' for semantic filtering is a direct invocation of the capabilities provided by CosineStrategy, capturing its application in the documented context.",
      "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation mentions the use of the AsyncWebCrawler class in the example code, specifically showing how it is instantiated and used within an asynchronous context manager to perform web crawling.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' class is directly used within the example, showcasing its role as a web crawler designed for advanced session handling.",
      "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The code snippet explicitly calls the 'arun' method of the 'AsyncWebCrawler', indicating its usage in executing the crawling logic with specific parameters.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The trace starts from the example's overarching usage of 'AsyncWebCrawler', going into specific functionality it provides through the 'arun()' method for single URL crawling.",
      "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'AsyncPlaywrightCrawlerStrategy' is implied as part of the execution strategy for the 'AsyncWebCrawler'. The example suggests using 'set_hook' and 'kill_session', methods characteristic of this strategy.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' uses 'AsyncPlaywrightCrawlerStrategy' for session management, indicated by method usage such as 'set_hook' and 'kill_session', linking it to the crawl strategy.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'kill_session' method is explicitly mentioned in the documentation as being used to terminate a session, showing the utility of ending a browsing session within a hook implementation.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "The 'kill_session' method is detailed in the documentation as part of the session management endpoint of the crawling operation, linked directly back to 'AsyncPlaywrightCrawlerStrategy'.",
      "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 3,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'set_hook' method is explicitly used in the documentation to set the 'on_execution_started' hook, demonstrating its role in facilitating custom hook-based operations.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
      "predicted_trace_chain_explanation": "The trace path illustrates how setting a custom hook via 'set_hook()' is employed for tailored session events in asynchronous crawling.",
      "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.set_hook()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
      "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
      "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet provides a code example demonstrating the use of the 'arun' method. The example showcases how different extraction strategies can be applied using 'arun', as mentioned in the snippet.",
      "predicted_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The arun() method of the AsyncWebCrawler class is directly demonstrated in the code example to execute extraction strategies on a given URL. The method is the primary focus in showing the combination of strategies, hence its direct tracing.",
      "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "instantiates",
      "relationship_explanation": "The variable 'css_strategy' used in the snippet suggests the implementation of a CSS-based extraction strategy, typically represented by a class such as JsonCssExtractionStrategy. Although not explicitly named, it aligns with the context of 'css_strategy'.",
      "predicted_trace_chain": "overview.md -> AsyncWebCrawler.arun() -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The trace begins with the arun method as it is explicitly used in the example. JsonCssExtractionStrategy relates because 'css_strategy' implies a CSS extraction approach, linking to 'arun' as a potential strategy class.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "instantiates",
      "relationship_explanation": "The presence of 'llm_strategy' in the example alludes to a large language model-based extraction component such as LLMExtractionStrategy. Though not explicitly referenced, its role is inferred as an instantiation of a strategy.",
      "predicted_trace_chain": "overview.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The 'arun' method is specified, and LLMExtractionStrategy comes implicitly linked through 'llm_strategy', signifying an LLM-based strategy likely used with 'arun'.",
      "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
      "ground_truth_trace_chain": "overview.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions the use of `LLMExtractionStrategy`. This is evident from the code example where `LLMExtractionStrategy` is instantiated with parameters such as `provider`, `api_token`, and others to create an extraction strategy.",
      "predicted_trace_chain": "output-formats.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The chain starts with the document where `LLMExtractionStrategy` is directly mentioned and used in the example to demonstrate structured data extraction using LLMs.",
      "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
      "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "calls",
      "relationship_explanation": "The function `arun` is shown in the snippet being called on a `crawler` object, which is likely an instance of a class implementing or encapsulating `AsyncWebCrawler`. It uses `extraction_strategy` to perform a crawl, as shown in the example.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The chain identifies that the `arun` function is referenced through its call within the example code, indicating its usage in executing the LLM extraction functionality.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "accesses",
      "relationship_explanation": "In the example, the `extracted_content` attribute is accessed from the result of an `arun` call, indicating where the structured data outcome of the LLM extraction is held.",
      "predicted_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "The chain starts from the document due to the explanation accessing the `extracted_content` attribute, showcasing its role as a container for extracted outputs from `LLMExtractionStrategy`.",
      "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
      "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation explicitly mentions using `AsyncWebCrawler` in the example function `wait_for_parameter_crawl()`, indicating its role in managing the crawling process.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The function `wait_for_parameter_crawl()` begins by utilizing the `AsyncWebCrawler` to manage crawling operations, as specified directly in the documentation.",
      "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `arun()` method is called within the example function with a specific URL and extraction strategy, showing its role in processing crawl requests.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The `arun()` function is called explicitly in the provided example, demonstrating a direct use-case where it processes the given URL using specified strategies.",
      "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 24,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The `JsonCssExtractionStrategy` is instantiated and provided as the `extraction_strategy` argument in the example, illustrating its role in extracting content based on CSS selectors.",
      "predicted_trace_chain": "session-management-advanced.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The example illustrates using `JsonCssExtractionStrategy` directly, which is crucial for content extraction as specified by the provided schema.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
      "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `kill_session()` method is explicitly called at the end of the example session to terminate the session using a specified `session_id`.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "The `kill_session()` method is utilized to conclude the crawler session, ensuring resources are released correctly. This usage is explicitly demonstrated at the end of the example, closing the pathway in the operational sequence.",
      "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
      "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
      "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The code snippet in the documentation calls `crawler.arun()` which directly refers to the `AsyncWebCrawler.arun()` method for executing the crawl process.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The `arun()` method in the `AsyncWebCrawler` class is directly used in the documentation snippet, linking the documentation directly with this implementation.",
      "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 20,
      "artifact_title": "CrawlResult.success",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation checks `result.success` to verify the crawl's success, indicating that `result` is an instance of `CrawlResult` and `success` is an attribute accessed in the code.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult.success",
      "predicted_trace_chain_explanation": "The `arun()` method returns a `CrawlResult` object, where `success` is checked for operation success, thus forming a traceability pathway.",
      "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 10,
      "artifact_title": "CrawlResult.error_message",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet references `result.error_message` to display error details if the crawl fails, linking directly to this attribute within `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult.error_message",
      "predicted_trace_chain_explanation": "The documentation guides usage of `error_message`, an attribute in `CrawlResult`, accessed after calling `arun()`.",
      "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.error_message",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 19,
      "artifact_title": "CrawlResult.status_code",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation uses `result.status_code` to display the HTTP status code of the crawl, linked to this attribute in `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult.status_code",
      "predicted_trace_chain_explanation": "The `status_code` is accessed following the call to `arun()`, part of the `CrawlResult` indicating the HTTP status of a crawl, providing a clear trace from documentation to attribute.",
      "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.status_code",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "implicit",
      "relationship_type": "returns",
      "relationship_explanation": "While not directly mentioned, `arun()` returns a `CrawlResult` object which is implicitly used when accessing `success`, `error_message`, and `status_code`.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> CrawlResult",
      "predicted_trace_chain_explanation": "The `arun()` method usage leads to returning a `CrawlResult` object, a crucial part of understanding how the example checks for success and retrieves error messages.",
      "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet directly demonstrates the instantiation of the `AsyncWebCrawler` class, using it in an asynchronous context manager to crawl a protected site.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document demonstrates usage of `AsyncWebCrawler`, which is essential for initiating web crawling operations as depicted in the example.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The method `arun` of the `AsyncWebCrawler` class is explicitly called to perform crawling, with parameters specified in the code snippet for handling protected sites.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The `arun()` method is called within an instance of `AsyncWebCrawler` as demonstrated in the provided code example, particularly specifying parameters to navigate challenges on protected sites.",
      "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "While not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is the likely default strategy for `AsyncWebCrawler` operations, implicitly handling the strategy mechanics for crawling.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The `AsyncWebCrawler` internally utilizes `AsyncPlaywrightCrawlerStrategy` as its base crawling strategy, establishing a key part of the operational chain without explicit mention in the example.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 3,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The example likely relies on hooks set via `set_hook()` in `AsyncPlaywrightCrawlerStrategy` to manage page events such as removing popups, through the `remove_overlay_elements` setting.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
      "predicted_trace_chain_explanation": "Hooks are pivotal in altering the page behavior and executing functionality such as removing popups, aligning with the `remove_overlay_elements=True` usage in the sample.",
      "ground_truth_relationship": null,
      "ground_truth_trace_chain": null,
      "traceability_granularity": "Class",
      "confusion_metrics": "False Positive",
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "CrawlResult.markdown",
      "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
      "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
      "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
      "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly showcases an example usage of the method `arun` within the `AsyncWebCrawler` class. The `arun` method is invoked to perform a crawl with specific parameters like `process_iframes` and `remove_overlay_elements`.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The snippet suggests using `AsyncWebCrawler.arun()` to execute a crawling task on a given URL, where optional arguments are specified to process iframes and remove overlay elements, showcasing its usage directly.",
      "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "contains",
      "relationship_explanation": "While the `AsyncWebCrawler` class isn't directly mentioned, its method `arun()` is explicitly referenced within the snippet. This implies the use of the `AsyncWebCrawler` class implicitly through its method.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The method `arun()` implies the surrounding context of the `AsyncWebCrawler` class, explaining its incorporation and usage, even though the class is not directly named.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly mentions `LLMExtractionStrategy` as the strategy used for extracting structured data using language models. Evidence: `strategy = LLMExtractionStrategy( provider=\"ollama/llama2\", schema=Product.schema(), instruction=\"Extract product details from the page\")`.",
      "predicted_trace_chain": "overview.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "Starts with the document specifying an LLM-based extraction approach and then directly mentions the `LLMExtractionStrategy` class used to implement this functionality.",
      "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The example in the documentation shows usage of the `AsyncWebCrawler` class with the `LLMExtractionStrategy`, hinting at an implicit utilization. Evidence: `result = await crawler.arun( url=\"https://example.com/product\", extraction_strategy=strategy)`.",
      "predicted_trace_chain": "overview.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation explains the usage of `LLMExtractionStrategy` with `AsyncWebCrawler`. The chain starts with the document and ends at the `AsyncWebCrawler`, which implicitly utilizes the outlined strategy.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The method `arun` in `AsyncWebCrawler` is showcased in the example, demonstrating how `LLMExtractionStrategy` is used to run extractions. Evidence: `result = await crawler.arun(...)`.",
      "predicted_trace_chain": "overview.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document first describes the use of `AsyncWebCrawler`, which involves the `arun` method to execute the extraction logic with `LLMExtractionStrategy`, thus forming the chain.",
      "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
      "ground_truth_trace_chain": "overview.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation text snippet explicitly shows the usage of the `arun` method of the `AsyncWebCrawler` class for making requests with a 'wait_for' parameter to await certain page elements or JavaScript conditions, making it an explicit relationship.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation page (page-interaction.md) directly discusses making web requests using `AsyncWebCrawler.arun()` method, specifically highlighting how to use the 'wait_for' parameter to wait for CSS elements or JavaScript conditions.",
      "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "While not mentioned explicitly in the text, the `arun` method belongs to the `AsyncWebCrawler` class. This makes `AsyncWebCrawler` implicitly involved as any behavior of `arun` is naturally a part of its class.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The `AsyncWebCrawler` class is the context within which the `arun` method is used, establishing the pathway from the class to its method, which is essential to connect the method's functionality back to its parent class from the documented usage.",
      "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "inherits",
      "relationship_explanation": "The implementation of the `arun` method in `AsyncWebCrawler` indirectly depends on the crawling strategy. `AsyncPlaywrightCrawlerStrategy` is a default or primary strategy used within `AsyncWebCrawler`, implicating this artifact as part of the broader pathway through which `arun` achieves its functionality.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "To provide full functionality, `arun` uses a crawling strategy, of which `AsyncPlaywrightCrawlerStrategy` is a specific implementation. This chain contextualizes how an element in the documentation reaches its runtime behavior, from the usage in `AsyncWebCrawler` through `arun` to the actual crawling strategy used.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
      "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly mentions `JsonCssExtractionStrategy` in the context of defining a schema and combining it with JavaScript execution for dynamic content extraction.",
      "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation directly references the `JsonCssExtractionStrategy` class for extracting structured data based on a CSS-based schema.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "Documented as the context manager used for executing asynchronous web crawling and JSON-based CSS extraction. The `AsyncWebCrawler` class is mentioned in the example code provided in the documentation.",
      "predicted_trace_chain": "css.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The example demonstrates the use of `AsyncWebCrawler` to perform crawling operations, indicating its role as a tool for managing the crawling and extraction process.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
      "ground_truth_trace_chain": "css.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `arun()` method is demonstrated in the example code to execute the crawl operation using the defined `JsonCssExtractionStrategy`.",
      "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The chain starts with the documentation mentioning the `AsyncWebCrawler`. The `arun()` method is called on an instance of `AsyncWebCrawler`, hence included as an element of functional operation.",
      "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
      "ground_truth_trace_chain": "css.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
      "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
      "ground_truth_trace_chain": "css.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
      "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
      "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The document explicitly mentions 'CSS Strategy' as the go-to for well-structured HTML content. The 'JsonCssExtractionStrategy' class implements a CSS-based extraction strategy, adhering to the description.",
      "predicted_trace_chain": "overview.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document describes the application of a CSS Strategy for well-structured HTML, directly aligning with 'JsonCssExtractionStrategy', which implements CSS-based extraction.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The document clearly mentions 'LLM Strategy' for natural language text and best semantic understanding, which is implemented by the 'LLMExtractionStrategy' class, aligning with the described function.",
      "predicted_trace_chain": "overview.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The document provides guidance on using LLM Strategy for semantic understanding, directly implemented by 'LLMExtractionStrategy'.",
      "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "'Cosine Strategy' is highlighted in the document for mixed/complex content and best content relevance, which aligns with the functionalities provided by 'CosineStrategy'.",
      "predicted_trace_chain": "overview.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "Mentioned 'Cosine Strategy' for content relevance and complexity maps to 'CosineStrategy' class implementing these functionalities.",
      "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation provides an example of using the 'arun' method, which is part of the 'AsyncWebCrawler' class, to perform web crawling with a proxy. This is explicitly shown in the usage example calling 'crawler.arun(url=\"https://example.com\")'.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document 'proxy-security.md' includes a code snippet demonstrating the usage of 'AsyncWebCrawler.arun()', establishing a direct trace from the documentation to this method.",
      "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "instantiates",
      "relationship_explanation": "In the code snippet, an instance of 'AsyncWebCrawler' is created using the 'with' statement and proxy configuration, indicating direct usage of this class for performing crawling operations with proxy authentication.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document demonstrates the instantiation of 'AsyncWebCrawler', which is necessary to subsequently call 'AsyncWebCrawler.arun()' for conducting web crawling within the proxy settings. This forms a logical pathway: 'proxy-security.md' starts the documentation reference, followed by 'AsyncWebCrawler' class usage, eventually leading to 'arun()' method invocation.",
      "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "inherits",
      "relationship_explanation": "The 'AsyncWebCrawler' can be configured with a strategy pattern, where 'AsyncPlaywrightCrawlerStrategy' is used if not provided specifically. This is implicit because the usage of 'AsyncPlaywrightCrawlerStrategy' is a default behavior of 'AsyncWebCrawler' when instantiating a crawler if no 'crawler_strategy' is explicitly passed.",
      "predicted_trace_chain": "proxy-security.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "This chain shows the implicit configuration where 'AsyncWebCrawler' relies on 'AsyncPlaywrightCrawlerStrategy' by default for executing crawl strategies. It's necessary to understand that 'arun()' method of 'AsyncWebCrawler' would internally utilize the configured crawler strategy, which defaults to 'AsyncPlaywrightCrawlerStrategy' if none is provided.",
      "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
      "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/advanced/proxy-security.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
      "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation snippet directly mentions 'JsonCssExtractionStrategy' in the context of selecting and testing CSS selectors, handling dynamic content, and ensuring error handling. This class implements the strategy necessary for processing CSS selectors within an extraction process.",
      "predicted_trace_chain": "css.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The chain starts with the documentation file 'css.md', which directly references 'JsonCssExtractionStrategy', making it the end destination for the trace.",
      "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
      "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
      "ground_truth_trace_chain": "css.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
      "document_location": "docs/md_v2/extraction/css.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
      "ground_truth_trace_chain": "css.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet provides an example of using the 'arun' method of the 'crawler' instance, which is associated with the 'AsyncWebCrawler' class. This is evident from the line `result = await crawler.arun(url=\"https://example.com\")`, where 'arun' is called to perform a web crawl.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation snippet explicitly mentions the use of the 'arun' method within 'AsyncWebCrawler', which is directly relevant as it describes how to initiate a crawl and retrieve results.",
      "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 13,
      "artifact_title": "CrawlResult.html",
      "predicted_relationship": "explicit",
      "relationship_type": "accesses",
      "relationship_explanation": "The documentation uses 'result.html' to access the raw HTML from the output of the 'arun' method, indicating 'CrawlResult.html' as a property used to retrieve data.",
      "predicted_trace_chain": "output-formats.md -> CrawlResult.html",
      "predicted_trace_chain_explanation": "After calling 'AsyncWebCrawler.arun()', the resulting 'CrawlResult' instance provides access to 'html', directly referenced in the documentation.",
      "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.html",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 9,
      "artifact_title": "CrawlResult.cleaned_html",
      "predicted_relationship": "explicit",
      "relationship_type": "accesses",
      "relationship_explanation": "The snippet shows usage of 'result.cleaned_html' to access cleaned HTML, which indicates 'CrawlResult.cleaned_html' as a property designed for sanitized HTML content.",
      "predicted_trace_chain": "output-formats.md -> CrawlResult.cleaned_html",
      "predicted_trace_chain_explanation": "Upon obtaining a 'CrawlResult' from 'arun', the '.cleaned_html' attribute is used to get cleaned HTML, which the documentation highlights.",
      "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.cleaned_html",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 15,
      "artifact_title": "CrawlResult.markdown",
      "predicted_relationship": "explicit",
      "relationship_type": "accesses",
      "relationship_explanation": "The line 'markdown = result.markdown' directly points to the use of 'CrawlResult.markdown' to access the standard markdown version of crawled content.",
      "predicted_trace_chain": "output-formats.md -> CrawlResult.markdown",
      "predicted_trace_chain_explanation": "The 'CrawlResult' object resulting from 'arun' provides '.markdown', which the documentation explicitly uses to describe access to content in markdown format.",
      "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 12,
      "artifact_title": "CrawlResult.fit_markdown",
      "predicted_relationship": "explicit",
      "relationship_type": "accesses",
      "relationship_explanation": "Within the example, 'fit_md = result.fit_markdown' directly links to 'CrawlResult.fit_markdown', used to access a markdown variant focused on relevant content.",
      "predicted_trace_chain": "output-formats.md -> CrawlResult.fit_markdown",
      "predicted_trace_chain_explanation": "The 'fit_markdown' property is retrieved from the 'CrawlResult' produced by 'arun', demonstrating how to obtain focused markdown content as documented.",
      "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.fit_markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet includes examples using the 'arun' method directly, passing parameters such as 'url', 'wait_for', 'process_iframes', 'js_code', and 'delay_before_return_html'. These parameters are consistent with those defined in the 'AsyncWebCrawler.arun()' method signature.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document 'browser-config.md' discusses handling dynamic content via examples that call the 'arun()' method from the 'AsyncWebCrawler' class to perform actions such as waiting for elements and handling lazy-loaded content.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'arun()' method in the documentation belongs to the 'AsyncWebCrawler' class, implying the usage of this class when handling dynamic content as demonstrated in the examples.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The examples in 'browser-config.md' implicitly refer to the 'AsyncWebCrawler' class through the 'arun()' method. Understanding dynamic content handling requires knowledge of this class since 'arun()' is a method of 'AsyncWebCrawler'.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "Since 'arun()' in 'AsyncWebCrawler' directly involves crawling operations, it interacts with 'AsyncCrawlerStrategy'-based strategies like 'AsyncPlaywrightCrawlerStrategy'. Particularly, strategies are responsible for executing tasks like 'js_code' evaluation and 'process_iframes'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "To manage dynamic content as discussed in the document, 'AsyncWebCrawler' delegates specific tasks to 'AsyncPlaywrightCrawlerStrategy', such as dynamic script execution and iframe handling encapsulated within the 'arun()' pathway.",
      "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The 'AsyncPlaywrightCrawlerStrategy' class extends 'AsyncCrawlerStrategy', providing an asynchronous approach to crawling. This extension is crucial to implementing the dynamic content handling methods described in the documentation.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun() -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "The abstraction 'AsyncCrawlerStrategy' provides a foundation for 'AsyncPlaywrightCrawlerStrategy', creating a structural hierarchy that enables the web crawler to handle dynamic actions scripted in 'browser-config.md'.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation snippet explicitly mentions the `arun` method of the `AsyncWebCrawler` class. The example usage demonstrates configuring various content filtering parameters, such as word_count_threshold, excluded_tags, and link and media exclusions.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The doc mentions configuring the `arun` method, which directly implements the crawling and filtering functionality described, making it the endpoint of this traceability pathway.",
      "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly mentions the `AsyncWebCrawler` class as supporting session-based crawling with a `session_id` parameter, indicating its role in managing persistent browser sessions.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document outlines session management and identifies `AsyncWebCrawler` as the class responsible for session-based functionalities, serving as the core handler of these operations.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The function `arun` within `AsyncWebCrawler` is implicitly referenced by mentioning the usage of `session_id` across `arun` calls for maintaining state, relating directly to the high-level functionality described in the document.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document refers to using `session_id` in `arun` methods to maintain session state, positioning `arun` as an integral method executing session management logic within `AsyncWebCrawler`.",
      "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "Although not explicitly mentioned, `AsyncPlaywrightCrawlerStrategy` extends `AsyncCrawlerStrategy`, which `AsyncWebCrawler` can utilize for executing browsing tasks that include session management elements as described in the documentation.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The chaining occurs because `AsyncWebCrawler` often utilizes strategies like `AsyncPlaywrightCrawlerStrategy` to implement functionalities (including session-based interactions) aligned with what the document describes.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The document explicitly uses 'LLMExtractionStrategy' to define a strategy for extracting specific parts of an article using an LLM. This association is clear in the example where 'LLMExtractionStrategy' is instantiated with parameters 'provider' and 'schema'.",
      "predicted_trace_chain": "content-selection.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The trace starts from 'content-selection.md', as this document discusses the usage of 'LLMExtractionStrategy' directly in the example provided.",
      "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
      "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'arun' method of 'AsyncWebCrawler' is used in the provided example to perform a web crawl with the given strategy. The example demonstrates its usage to execute the 'LLMExtractionStrategy'.",
      "predicted_trace_chain": "content-selection.md -> LLMExtractionStrategy -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The pathway starts from 'content-selection.md' which describes using 'LLMExtractionStrategy', and shows 'arun' being called with the strategy, forming a natural next step in the trace relationship.",
      "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The variable 'result' in the example is assigned the return value of 'arun', which is an instance of 'CrawlResult'. This class encapsulates the results of the web crawling strategy execution.",
      "predicted_trace_chain": "content-selection.md -> LLMExtractionStrategy -> AsyncWebCrawler.arun() -> CrawlResult",
      "predicted_trace_chain_explanation": "The trace follows the execution flow starting from 'content-selection.md', utilizing 'LLMExtractionStrategy', executing via 'AsyncWebCrawler.arun()', and returning a 'CrawlResult'.",
      "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The example in the documentation assigns 'result.extracted_content' to a variable, indicating that the 'CrawlResult' class provides an 'extracted_content' attribute, which holds the extracted content as described.",
      "predicted_trace_chain": "content-selection.md -> LLMExtractionStrategy -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "The document illustrates extracting structured content using 'LLMExtractionStrategy', followed by 'arun' execution that returns 'CrawlResult', which contains 'extracted_content', piecing together a comprehensive trace path.",
      "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
      "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The AsyncWebCrawler class is directly instantiated using the line `async with AsyncWebCrawler(verbose=True) as crawler:`. This shows that the AsyncWebCrawler is the main component facilitating the crawling process.",
      "predicted_trace_chain": "css-advanced.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation specifically mentions using AsyncWebCrawler, making it a direct link to the explained functionality.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
      "ground_truth_trace_chain": "css-advanced.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The method `arun` is explicitly called in the usage example: `result = await crawler.arun(...)`. This indicates that this function is being utilized to execute the crawling operation, passing various parameters like URL and extraction strategy.",
      "predicted_trace_chain": "css-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The AsyncWebCrawler is instantiated and then the arun method is invoked, demonstrating its use directly in the documentation.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
      "ground_truth_trace_chain": "css-advanced.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The JsonCssExtractionStrategy is explicitly instantiated with `extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)`. This strategy is utilized to define how data from the webpage is extracted.",
      "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation mentions the use of a specific extraction strategy, which is directly traced to the JsonCssExtractionStrategy used in the example.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "accesses",
      "relationship_explanation": "The example includes `product_data = json.loads(result.extracted_content)`, which accesses the extracted_content attribute of the CrawlResult class to fetch and parse the data.",
      "predicted_trace_chain": "css-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "The arun method returns a CrawlResult instance, from which the extracted_content is accessed, creating a pathway that ends in the attribute usage.",
      "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
      "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
      "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
      "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
      "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
      "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
      "document_location": "docs/md_v2/extraction/css-advanced.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
      "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The text snippet shows an example usage of the 'arun()' method from the 'AsyncWebCrawler' class, where several parameters like 'url', 'page_timeout', 'delay_before_return_html', and 'wait_for' are passed. These directly relate to the method signature of 'arun()'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The 'browser-config.md' document provides an example using the 'AsyncWebCrawler.arun()' method illustrating how to control page loading behavior, demonstrating this method's functional role in applying timeouts and waiting mechanisms.",
      "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "contains",
      "relationship_explanation": "While 'AsyncWebCrawler' is not explicitly mentioned in the text, it implicitly appears as the class containing the 'arun()' method, which is demonstrated in the usage example. The method belongs to this class, and hence understanding its relationship to the 'AsyncWebCrawler' class is necessary to fully comprehend the usage.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "Understanding 'AsyncWebCrawler' is necessary to locate the 'arun' method and comprehend the context where page loading behaviors are configured, as demonstrated in the provided documentation snippet.",
      "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "'AsyncWebCrawler' can utilize different strategies, including 'AsyncCrawlerStrategy', which is indirectly related through strategy pattern implementation. While not directly in the provided snippet, understanding how 'AsyncWebCrawler' operates implies recognizing it may utilize strategies like 'AsyncCrawlerStrategy'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "Recognizing 'AsyncWebCrawler' may use 'AsyncCrawlerStrategy' or its implementations helps contextualize the page loading configuration example, ensuring it can align with different crawling needs or configurations.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "This specific strategy ('AsyncPlaywrightCrawlerStrategy'), might be used within the 'AsyncWebCrawler' context, especially when detailed configurations like page timeouts and waits are involved, though not explicitly mentioned, it is part of the 'AsyncCrawlerStrategy' pathways.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The example could involve 'AsyncPlaywrightCrawlerStrategy', which provides more explicit implementations for browser interaction, reflecting how the 'arun' method can achieve its configurable behavior regarding timeout and waits.",
      "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation specifies the use of the 'JsonCssExtractionStrategy' class from the 'crawl4ai.extraction_strategy' module to create an extraction strategy based on a schema for content extraction, as evidenced by the commented code snippet.",
      "predicted_trace_chain": "content-selection.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation directly mentions 'JsonCssExtractionStrategy', indicating its use for defining pattern-based content extraction processes in code, which is the key feature described in this snippet.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
      "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'arun' method of an 'AsyncWebCrawler' instance is called with the extraction strategy applied to a URL. This is demonstrated by the example that uses 'await crawler.arun(...)'. This implies usage of the 'arun' method for asynchronous crawling tasks.",
      "predicted_trace_chain": "content-selection.md -> arun()",
      "predicted_trace_chain_explanation": "The code snippet includes a function call to 'arun()', indicating its role in executing the crawling operation using the specified strategy, forming a part of the documented pattern-based content extraction process.",
      "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The usage of 'arun()' in the code snippet explicitly relies on an instance of 'AsyncWebCrawler', which provides access to the method. While its instantiation isn't directly shown, it is implied through 'crawler.arun()'.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "For the snippet to call 'arun()', it requires an 'AsyncWebCrawler' instance, implicitly making 'AsyncWebCrawler' part of the pathway necessary to apply the crawling strategy as described.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
      "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `arun()` method is explicitly mentioned in the usage example within the documentation snippet. It is shown being invoked with parameters such as `url` and `word_count_threshold`, indicating its role in controlling the content retrieval process based on provided criteria.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The chain represents how the documentation snippet directly references the `AsyncWebCrawler.arun()` method, which is the main focus of the content described in the example. This method is the entry point for the functionality being described.",
      "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "Although not directly mentioned, the `AsyncWebCrawler` class is implicitly referenced through its method `arun()`. It provides the context for where `arun()` resides, making `AsyncWebCrawler` essential for understanding the environment in which content control can be executed.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler.arun() -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The chain starts from the documentation file `output-formats.md`, explicitly mentions the `arun()` method, and implicitly connects to the `AsyncWebCrawler` class, which houses the method. This connection is necessary to comprehend the full structure and execution of the described actions.",
      "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet directly calls `arun()` method on a `crawler` object. This method is part of the `AsyncWebCrawler` class, providing evidence from the surrounding code that explicitly mentions using `arun()` for executing the crawl function.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The `cosine.md` documentation explains error handling for the `arun()` method, directly invoking this method. This is a central point for executing crawl operations within the `AsyncWebCrawler` class.",
      "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
      "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "While not directly mentioned, the instantiation of the `extraction_strategy` implies the use of a strategy pattern, and `AsyncPlaywrightCrawlerStrategy` is a concrete implementation of `AsyncCrawlerStrategy` possibly used by `AsyncWebCrawler`.",
      "predicted_trace_chain": "cosine.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The documentation describes `arun()` using an `extraction_strategy`, which is plausible to be `CosineStrategy` or similar, indirectly linking it to `AsyncPlaywrightCrawlerStrategy` through strategic pattern implementation.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
      "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "describes",
      "relationship_explanation": "The documentation explicitly mentions `Cosine Strategy` as an effective strategy when semantic understanding is required. This strategy is likely to be used in conjunction with the `arun()` function as indicated in the snippet.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The documentation highlights `CosineStrategy`, providing detailed contexts such as dealing with inconsistent structures, implying this strategy is employed in the crawling process described in the text.",
      "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "CrawlResult.error_message",
      "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult.error_message",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
      "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
      "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
      "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "class_usage",
      "relationship_explanation": "The documentation snippet uses the AsyncWebCrawler class explicitly in the example, as indicated by the import statement and its instantiation with `AsyncWebCrawler(verbose=True)`.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document discusses session-based crawling using AsyncWebCrawler, which is central to the execution flow in the example.",
      "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "method_usage",
      "relationship_explanation": "The `arun` method of AsyncWebCrawler is implicitly used in the example to perform a crawl for each page. This usage implies its relevance in implementing the described session-based crawling functionality.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The example illustrates the use of the `arun` method of the AsyncWebCrawler class, showing how it is used to execute crawling tasks within the session-based framework.",
      "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "implicit",
      "relationship_type": "method_usage",
      "relationship_explanation": "This method is implicitly referred to by the call `await crawler.crawler_strategy.kill_session(session_id)`, which is aimed at closing the crawl session as part of session management.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "After crawling, the example uses the `kill_session` method of AsyncPlaywrightCrawlerStrategy (accessible from AsyncWebCrawler) to properly close the session with the given session_id.",
      "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "attribute_usage",
      "relationship_explanation": "In the example, `result.extracted_content.count('.content-item')` indicates reliance on this attribute to assess the items extracted, which forms part of the content processing logic.",
      "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "The sequence involves using AsyncWebCrawler to perform crawls that return a CrawlResult, from which the `extracted_content` attribute is used for content assessment after execution.",
      "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
      "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
      "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
      "document_location": "docs/md_v2/advanced/session-management-advanced.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
      "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet directly uses the 'AsyncWebCrawler' class within the example code, indicating it as a central part of the implemented crawling functionality.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation demonstrates how to use the 'AsyncWebCrawler' to perform structured and semantic content extraction from a web page.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'arun' method of 'AsyncWebCrawler' is explicitly invoked to perform crawling with specific strategies for both structured and LLM-based analysis, as shown in the documentation.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The documentation details the use of 'arun' for executing web crawling tasks, utilizing extraction strategies to process content.",
      "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet explicitly mentions and uses 'JsonCssExtractionStrategy' as the strategy to extract structured content from the web page using a CSS selector-based schema.",
      "predicted_trace_chain": "content-selection.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The documentation walks through initializing 'JsonCssExtractionStrategy' for structured data extraction using a defined schema.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
      "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The snippet explicitly details using 'LLMExtractionStrategy' with a specified provider and schema for performing LLM-based semantic analysis.",
      "predicted_trace_chain": "content-selection.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The doc explains using 'LLMExtractionStrategy' for analyzing content by defining an instruction and schema suitable for the LLM techniques.",
      "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
      "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_id": 8,
      "artifact_title": "CrawlResult",
      "predicted_relationship": "implicit",
      "relationship_type": "returns",
      "relationship_explanation": "Though not directly mentioned, 'CrawlResult' is implicated by the return type of the 'arun' method, which outputs a result with structured data, semantic analysis, and media fields.",
      "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
      "predicted_trace_chain_explanation": "The trace demonstrates how 'arun' ultimately returns a 'CrawlResult', which encompasses the processed data extracted from the web crawling operations.",
      "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "CrawlResult.media",
      "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult.media",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "CrawlResult.extracted_content",
      "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
      "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
      "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/content-selection.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
      "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet provides an example using the 'arun' method of a crawler object, which aligns with the usage of the 'AsyncWebCrawler.arun()' method in the available artifacts.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The 'magic-mode.md' documentation directly references the 'arun' method of a crawler, indicating its usage for configuring anti-detection features by setting 'simulate_user' and 'override_navigator' parameters.",
      "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
      "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 28,
      "artifact_title": "TopicSegmentationChunking",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly mentions the 'TopicSegmentationChunking' class. It is described as implementing text segmentation into topic-based chunks using the TextTiling algorithm, matching the class's functionality.",
      "predicted_trace_chain": "chunking.md -> TopicSegmentationChunking",
      "predicted_trace_chain_explanation": "The document 'chunking.md' describes the 'TopicSegmentationChunking' class and its use for segmenting text into topics, directly linking the documentation to the class.",
      "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> TopicSegmentationChunking",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_title": "ChunkingStrategy",
      "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The artifacts explicitly mention the use of parameters such as 'semantic_filter', 'sim_threshold', 'word_count_threshold', and 'top_k' in conjunction with 'CosineStrategy'. This indicates that 'CosineStrategy' is the class implementing the functionality described in the documentation snippet.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The documentation in cosine.md directly refers to the use of 'CosineStrategy' with specific parameters, thereby indicating that it implements the described functionality.",
      "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "'CosineStrategy' is an implementation of an extraction strategy and typically extends a base class for extraction. Although not mentioned explicitly in the snippet, it is related implicitly as it extends 'ExtractionStrategy'.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "'CosineStrategy' is an implementation of the parameters discussed in the documentation. It extends 'ExtractionStrategy', which defines its structural and operational norms.",
      "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 27,
      "artifact_title": "SlidingWindowChunking",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation explicitly details the 'SlidingWindowChunking' class, describing its purpose and parameters, and even gives a code example using this class.",
      "predicted_trace_chain": "chunking.md -> SlidingWindowChunking",
      "predicted_trace_chain_explanation": "The documentation directly refers to and explains the 'SlidingWindowChunking' class, making it the primary focus here.",
      "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> SlidingWindowChunking",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
      "document_location": "docs/md_v2/extraction/chunking.md",
      "artifact_id": 6,
      "artifact_title": "ChunkingStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The 'SlidingWindowChunking' class, detailed in the documentation snippet, is an implementation of the base 'ChunkingStrategy' class, as is typical in object-oriented design patterns where specific strategies implement an abstract base strategy.",
      "predicted_trace_chain": "chunking.md -> SlidingWindowChunking -> ChunkingStrategy",
      "predicted_trace_chain_explanation": "'SlidingWindowChunking' extends 'ChunkingStrategy', which is necessary because it defines the abstract interface that 'SlidingWindowChunking' realizes.",
      "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
      "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The documentation snippet explicitly mentions the 'CosineStrategy' class, detailing the initialization parameters such as 'semantic_filter', 'word_count_threshold', 'sim_threshold', 'max_dist', 'linkage_method', 'top_k', 'model_name', and 'verbose'. These parameters correspond directly to the constructor of the 'CosineStrategy' class.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The document describes the configuration options for the 'CosineStrategy' class, which directly maps to the implementation of this class in the code.",
      "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
      "document_location": "docs/md_v2/extraction/cosine.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "The 'CosineStrategy' class is a subclass of 'ExtractionStrategy', which is not mentioned directly in the snippet but can be identified from the code structure. Its relation to 'CosineStrategy' is inherent as the base class.",
      "predicted_trace_chain": "cosine.md -> CosineStrategy -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "Tracing from cosine.md to 'CosineStrategy', the class hierarchy leads to 'ExtractionStrategy' as the base class, showing the extension relationship.",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
      "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 7,
      "artifact_title": "CosineStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The document snippet explicitly discusses the use of `CosineStrategy` with parameters such as `semantic_filter`, `word_count_threshold`, `sim_threshold`, `max_dist`, and `top_k` to perform content similarity analysis and topic clustering as demonstrated in the Python code snippet.",
      "predicted_trace_chain": "overview.md -> CosineStrategy",
      "predicted_trace_chain_explanation": "The documentation directly references `CosineStrategy` as the focal class used for content similarity analysis, making it the endpoint of the trace from the document.",
      "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The code example within the document shows usage of `AsyncWebCrawler.arun()` to execute the `CosineStrategy` as part of the crawling and content extraction process. Although `arun()` is not directly discussed, it's used in the example code, establishing its role in executing the strategy.",
      "predicted_trace_chain": "overview.md -> CosineStrategy -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The trace begins with `overview.md` referencing `CosineStrategy`, and then through the code example usage, it connects to `AsyncWebCrawler.arun()`, which is necessary for applying the strategy to crawl and extract content using the specified parameters.",
      "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "`CosineStrategy` is derived from `ExtractionStrategy`, indicating that all functionalities and interfaces applicable to extraction strategies apply to `CosineStrategy` as well. This inheritance is not explicit in the document but is inherent in the object-oriented structure shown in the artifacts.",
      "predicted_trace_chain": "overview.md -> ExtractionStrategy -> CosineStrategy",
      "predicted_trace_chain_explanation": "The document refers to functionalities of `CosineStrategy`, which inherently extends `ExtractionStrategy`. The extension is a critical step that underlies the structure of `CosineStrategy`, which connects the overarching strategy specifications to the document's implementation details.",
      "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
      "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
      "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
      "ground_truth_trace_chain": "overview.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
      "document_location": "docs/md_v2/extraction/overview.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
      "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet uses the `AsyncWebCrawler` class within an asynchronous context to run its tasks, as can be seen in the code `async with AsyncWebCrawler(verbose=True) as crawler:`. This demonstrates a clear use of the class for initializing and managing the lifecycle of web crawling tasks.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document 'quickstart.md' provides an example that directly involves the use of the `AsyncWebCrawler` class, indicating it is a central component in managing crawling operations, specifically demonstrating screenshot capabilities.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The function `arun` is explicitly called in the documentation snippet through `await crawler.arun(url=url, screenshot=True, bypass_cache=True)`. This method is responsible for initiating the crawling process, which is essential for capturing the webpage screenshot as depicted in the example.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document 'quickstart.md' leads directly to the use of `AsyncWebCrawler`, which then calls the `arun` method demonstrating its role in executing specific crawling activities like taking a screenshot.",
      "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_id": 18,
      "artifact_title": "CrawlResult.screenshot",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `CrawlResult.screenshot` attribute is used in the snippet to retrieve and decode the captured screenshot as indicated by `if result.success and result.screenshot:`. It illustrates usage of the result object to access the screenshot data returned after crawling.",
      "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.screenshot",
      "predicted_trace_chain_explanation": "The document 'quickstart.md' utilizes `AsyncWebCrawler`, which through `arun` provides a `CrawlResult` object, from which the `screenshot` is accessed to fulfill the task of saving the screenshot, showing a pathway from initialization to result access.",
      "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult.screenshot",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "CrawlResult.success",
      "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
      "ground_truth_trace_chain": "quickstart.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
      "document_location": "docs/md_v2/basic/quickstart.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
      "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The 'AsyncWebCrawler' class is used explicitly in the code snippet from the documentation, which demonstrates setting up a crawler to handle multiple interactions on a dynamic page.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation directly shows instantiation and usage of 'AsyncWebCrawler' for handling page interactions, which is crucial for the described functionality.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The method 'arun()' is explicitly called in the code snippet to execute a crawling task with various JavaScript and content waiting parameters, as demonstrated in the documentation example.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "To execute page crawling in the dynamic content example, 'AsyncWebCrawler.arun()' is invoked with specific parameters, as shown in the documentation.",
      "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 2,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_relationship": "explicit",
      "relationship_type": "invokes",
      "relationship_explanation": "In the example, 'kill_session()' is called to clean up the session after interactions, specifically mentioned in the documentation.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "predicted_trace_chain_explanation": "The session handling is a key part of the workflow described in the documentation, where 'kill_session()' is used post-interaction to manage resources within the 'AsyncPlaywrightCrawlerStrategy', which implements strategies for 'AsyncWebCrawler'.",
      "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncPlaywrightCrawlerStrategy.kill_session()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The 'AsyncPlaywrightCrawlerStrategy' class, while not explicitly named in the snippet, implements the behavior that 'AsyncWebCrawler' relies on for performing page interactions, as it's the underlying strategy class used by 'AsyncWebCrawler'.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' internally uses strategies from 'AsyncPlaywrightCrawlerStrategy', hence creating an indirect pathway where 'AsyncPlaywrightCrawlerStrategy' implements the logic used within 'AsyncWebCrawler'.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "inherits",
      "relationship_explanation": "The 'AsyncPlaywrightCrawlerStrategy' inherits from 'AsyncCrawlerStrategy', forming the base for strategy implementation, implicitly involved in the page interaction examples.",
      "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "To understand the inheritance chain and strategy implementation, it's essential to follow from 'AsyncWebCrawler' to 'AsyncPlaywrightCrawlerStrategy', which is a subclass of 'AsyncCrawlerStrategy'.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
      "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "CrawlResult.cleaned_html",
      "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
      "ground_truth_trace_chain": "page-interaction.md -> CrawlResult.cleaned_html",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
      "document_location": "docs/md_v2/basic/page-interaction.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
      "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation directly shows how to instantiate and use the `AsyncWebCrawler` class. The example code snippet begins with `from crawl4ai import AsyncWebCrawler` and utilizes `AsyncWebCrawler` to perform web crawling operations, including verbose mode configuration and contextual crawling using the `arun` method.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The document `simple-crawling.md` provides an example that starts with the use of `AsyncWebCrawler` as part of demonstrating its functional application in web crawling scenarios. The class is central to executing the demonstrated code.",
      "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "In the documentation's example, the `arun` method of the `AsyncWebCrawler` class is called to perform a web crawl on a specified URL with various parameters. This method is crucial for invoking the actual crawling process as shown in `await crawler.arun(url=\"https://example.com\", ...)`, linking its usage with the example's outcomes.",
      "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document `simple-crawling.md` demonstrates the usage of `AsyncWebCrawler`, which directly calls `arun()`. This shows the pathway from class instantiation to method invocation, helping the reader understand the function that initiates crawling.",
      "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 20,
      "artifact_title": "CrawlResult.success",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `success` attribute of the `CrawlResult` is explicitly checked in the example to determine if the crawl operation was successful. This is illustrated by the conditional statement `if result.success:`.",
      "predicted_trace_chain": "simple-crawling.md -> CrawlResult.success",
      "predicted_trace_chain_explanation": "The document `simple-crawling.md` specifies that after calling `arun()`, the `success` attribute of `CrawlResult` is used to check the crawl outcome, forming a path from result retrieval to success verification.",
      "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 15,
      "artifact_title": "CrawlResult.markdown",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation's example accesses the `markdown` attribute of `CrawlResult` to print the content of the successful crawl result. It is used within `print(\"Content:\", result.markdown[:500])` to extract and display crawled content.",
      "predicted_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
      "predicted_trace_chain_explanation": "After checking `success`, the document `simple-crawling.md` uses `markdown` from `CrawlResult` to access and print the html content, indicating its role in content handling post-crawl.",
      "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 16,
      "artifact_title": "CrawlResult.media",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The example loops through `result.media[\"images\"]` to print details of found images, demonstrating the use of `media` to handle image extraction results within `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> CrawlResult.media",
      "predicted_trace_chain_explanation": "The document `simple-crawling.md` proceeds to handle `media` post-success check, specifically leveraging image data stored in `CrawlResult`, indicating how media results are processed.",
      "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.media",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 14,
      "artifact_title": "CrawlResult.links",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation demonstrates iterating over `result.links[\"internal\"]` to display internal links found during the crawl, showcasing the use of `links` to handle link extraction.",
      "predicted_trace_chain": "simple-crawling.md -> CrawlResult.links",
      "predicted_trace_chain_explanation": "Following media use, `simple-crawling.md` illustrates link handling through the `links` attribute in `CrawlResult`, displaying path from crawl result to link processing.",
      "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.links",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_id": 10,
      "artifact_title": "CrawlResult.error_message",
      "predicted_relationship": "implicit",
      "relationship_type": "uses",
      "relationship_explanation": "If the crawl fails, the documentation example demonstrates printing `result.error_message` to display error details, indicating its role as an error reporting mechanism in `CrawlResult`.",
      "predicted_trace_chain": "simple-crawling.md -> CrawlResult.error_message",
      "predicted_trace_chain_explanation": "In `simple-crawling.md`, the path follows a negative result (failed crawl), employing `error_message` for error reporting and completing the failure handling process.",
      "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.error_message",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
      "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "document_location": "docs/md_v2/basic/simple-crawling.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
      "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The `arun` method of the `AsyncWebCrawler` class is explicitly used in the example provided to demonstrate crawling a webpage content using different extraction strategies.",
      "predicted_trace_chain": "output-formats.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The document demonstrates the usage of `AsyncWebCrawler` and its method `arun` directly in a code example, showing how different strategies are applied to perform crawling with output formats.",
      "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "used_in_context",
      "relationship_explanation": "The `LLMExtractionStrategy` is explicitly mentioned as part of the code example where it is used to perform structured data extraction using an LLM provider.",
      "predicted_trace_chain": "output-formats.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The document provides an example of using `LLMExtractionStrategy` to extract structured data, showcasing its application within the `AsyncWebCrawler.arun()` method.",
      "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
      "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 23,
      "artifact_title": "JsonCssExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "used_in_context",
      "relationship_explanation": "The `JsonCssExtractionStrategy` is explicitly mentioned in the code example to highlight how repeated patterns can be extracted from a webpage, demonstrating its integration with the `AsyncWebCrawler.arun()` method.",
      "predicted_trace_chain": "output-formats.md -> JsonCssExtractionStrategy",
      "predicted_trace_chain_explanation": "The document demonstrates the use of `JsonCssExtractionStrategy` for extracting pattern data through the `AsyncWebCrawler` interface, showing its real-world application.",
      "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
      "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 11,
      "artifact_title": "CrawlResult.extracted_content",
      "predicted_relationship": "implicit",
      "relationship_type": "returns",
      "relationship_explanation": "While not explicitly mentioned, the `extracted_content` attribute of `CrawlResult` is used implicitly when accessing structured and pattern data in the JSON format, as detailed in the example code.",
      "predicted_trace_chain": "output-formats.md -> CrawlResult -> CrawlResult.extracted_content",
      "predicted_trace_chain_explanation": "In the document context, the `CrawlResult` is returned by `AsyncWebCrawler.arun()` containing attributes like `extracted_content` utilized in the example to extract and format the data returned from web crawling.",
      "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_id": 12,
      "artifact_title": "CrawlResult.fit_markdown",
      "predicted_relationship": "implicit",
      "relationship_type": "returns",
      "relationship_explanation": "The `fit_markdown` attribute is accessed in the example for returning the main content in markdown format, indicating its implicit role in handling the results of the crawl.",
      "predicted_trace_chain": "output-formats.md -> CrawlResult -> CrawlResult.fit_markdown",
      "predicted_trace_chain_explanation": "The document shows the use of `CrawlResult.fit_markdown` where the `arun` method's output includes this attribute for capturing the main content formatting in markdown, providing essential result structuring.",
      "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.fit_markdown",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncWebCrawler",
      "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "ExtractionStrategy",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
      "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "CrawlResult.media",
      "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
      "ground_truth_trace_chain": "output-formats.md -> CrawlResult.media",
      "traceability_granularity": "Statement-level",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
      "document_location": "docs/md_v2/basic/output-formats.md",
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
      "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 24,
      "artifact_title": "LLMExtractionStrategy",
      "predicted_relationship": "explicit",
      "relationship_type": "implements",
      "relationship_explanation": "The text snippet explicitly mentions the use of LLMExtractionStrategy. It describes customizing the strategy using different LLM providers, indicating that this class implements configurable logic for extracting information using a language model.",
      "predicted_trace_chain": "llm.md -> LLMExtractionStrategy",
      "predicted_trace_chain_explanation": "The document llm.md discusses leveraging different LLM providers for extracting data which is implemented through the LLMExtractionStrategy class. This forms a direct relationship with no intermediate steps mentioned in the snippet.",
      "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> LLMExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
      "document_location": "docs/md_v2/extraction/llm.md",
      "artifact_id": 21,
      "artifact_title": "ExtractionStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "Although not directly mentioned, LLMExtractionStrategy extends ExtractionStrategy. The text implies functionality enhancements over a base strategy for extraction, which aligns with the paradigm of subclassing to extend base class functionality.",
      "predicted_trace_chain": "llm.md -> LLMExtractionStrategy -> ExtractionStrategy",
      "predicted_trace_chain_explanation": "LLMExtractionStrategy is based on the base class ExtractionStrategy to utilize the extraction framework. This is inferred from LLMExtractionStrategy extending features onto any baseline extraction logic, capturing the pathway from document to base abstraction.",
      "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
      "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet explicitly demonstrates the use of the 'AsyncWebCrawler' class for proxy configuration by showing examples directly using 'with AsyncWebCrawler(...) as crawler'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation provides instructions on using the 'AsyncWebCrawler' class to configure proxies for web crawling tasks, making it the primary subject of the code example.",
      "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "implicit",
      "relationship_type": "calls",
      "relationship_explanation": "The 'arun' method is implicitly used in the document's examples as 'result = await crawler.arun(url=\"https://example.com\")', where it plays the role of executing a crawl task upon initialization of 'AsyncWebCrawler'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The trace chain starts with the 'AsyncWebCrawler' class, demonstrating the usage of the 'arun' method as part of the crawling operation shown in the example.",
      "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "Although not mentioned directly in the snippet, 'AsyncWebCrawler' can utilize 'AsyncPlaywrightCrawlerStrategy' as a default fallback when crawling, and proxy settings might be passed down to strategies implementing 'AsyncCrawlerStrategy'.",
      "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "The 'AsyncWebCrawler' class implicitly depends on underlying strategies for execution, including 'AsyncPlaywrightCrawlerStrategy', which can manage proxies as part of its configuration.",
      "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "AsyncCrawlerStrategy",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
      "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
      "document_location": "docs/md_v2/basic/browser-config.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
      "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    },
    {
      "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 4,
      "artifact_title": "AsyncWebCrawler",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "In the documentation snippet, `AsyncWebCrawler` is used in the example code demonstrating how to enable Magic Mode. It clarifies that the `AsyncWebCrawler` object is used to perform the crawling operation with the `arun` method.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler",
      "predicted_trace_chain_explanation": "The documentation found in magic-mode.md explicitly references the `AsyncWebCrawler` class from the codebase as the main component used to create a crawler instance, showcasing its usage to enable anti-detection features in Magic Mode.",
      "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 5,
      "artifact_title": "AsyncWebCrawler.arun()",
      "predicted_relationship": "explicit",
      "relationship_type": "uses",
      "relationship_explanation": "The documentation snippet includes a Python code example that makes an explicit call to the `arun` method on an `AsyncWebCrawler` instance, which is part of the public API used to start the web crawling process with specific parameters.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
      "predicted_trace_chain_explanation": "The example code demonstrates the use of the `arun` method on an `AsyncWebCrawler` instance, thus establishing a direct link between the documents' description of Magic Mode usage and the underlying method that implements the crawling logic within the system.",
      "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler.arun()",
      "traceability_granularity": "Method",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 0,
      "artifact_title": "AsyncCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "extends",
      "relationship_explanation": "Although not directly mentioned in the documentation snippet, `AsyncCrawlerStrategy` is the base class that `AsyncPlaywrightCrawlerStrategy` extends. It provides the abstract interface which `AsyncWebCrawler` depends upon through `AsyncPlaywrightCrawlerStrategy`, enabling the strategies for web crawling, including features implied by Magic Mode.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
      "predicted_trace_chain_explanation": "`AsyncWebCrawler` indirectly uses methods defined in `AsyncCrawlerStrategy` via its specific implementation `AsyncPlaywrightCrawlerStrategy`. These methods shape the core functionality that Magic Mode leverages, making it critical to understanding how Magic Mode is implemented.",
      "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_id": 1,
      "artifact_title": "AsyncPlaywrightCrawlerStrategy",
      "predicted_relationship": "implicit",
      "relationship_type": "implements",
      "relationship_explanation": "The implementation of anti-bot protections as specified in the Magic Mode documentation would extend or be a feature of `AsyncPlaywrightCrawlerStrategy`, which inherits from `AsyncCrawlerStrategy`. Hence, it's implicitly linked as the specific strategy used by `AsyncWebCrawler` for executing Magic Mode functions.",
      "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
      "predicted_trace_chain_explanation": "`AsyncWebCrawler` employs `AsyncPlaywrightCrawlerStrategy` as part of its strategy pattern, involving functionalities detailed in the Magic Mode documentation. Therefore, the strategy components must execute the anti-detection features.",
      "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
      "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
      "traceability_granularity": "Class",
      "confusion_metrics": "True Positive",
      "prediction_details": {
        "matches_ground_truth": true,
        "relationship_match": false
      }
    },
    {
      "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
      "document_location": "docs/md_v2/advanced/magic-mode.md",
      "artifact_title": "CrawlResult",
      "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
      "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
      "traceability_granularity": "Class",
      "confusion_metrics": "False Negative",
      "predicted_relationship": null,
      "relationship_type": null,
      "relationship_explanation": null,
      "predicted_trace_chain": null,
      "predicted_trace_chain_explanation": null,
      "prediction_details": {
        "matches_ground_truth": false,
        "relationship_match": false,
        "missed_by_llm": true
      }
    }
  ]
}