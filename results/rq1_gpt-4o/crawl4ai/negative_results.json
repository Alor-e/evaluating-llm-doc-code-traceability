{
  "dataset": "crawl4ai",
  "runs": [
    {
      "run_id": 1,
      "timestamp": "2025-02-12T11:34:16.945245",
      "negative_results": [
        {
          "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "base_class",
          "relationship_explanation": "Since the 'AsyncWebCrawler' is known to utilize different strategies and 'JsonCssExtractionStrategy' is one of them, it implies the use of 'AsyncCrawlerStrategy', which defines the abstract interface for crawler strategies.",
          "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "'AsyncWebCrawler' likely relies on the functionality defined in 'AsyncCrawlerStrategy' to accommodate various strategies, including the 'JsonCssExtractionStrategy'.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "returns",
          "relationship_explanation": "While not explicitly mentioned, the code example utilizes the `result1.markdown` attribute returned by `arun()`, which is a likely part of the `CrawlResult` class structure.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The `quickstart.md` example starts with `AsyncWebCrawler`, invokes `arun()`, which provides access to properties of `CrawlResult` such as `html`, from which `markdown` is likely derived.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "The kill_session method is part of the mechanism to clean up sessions as indicated by its name and role within AsyncPlaywrightCrawlerStrategy. This indirectly supports the documentation's session management theme by handling session lifecycle.",
          "predicted_trace_chain": "session-management-advanced.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_trace_chain_explanation": "This trace route captures the progression from the document focusing on session management to how AsyncWebCrawler potentially leverages the kill_session feature implemented in AsyncPlaywrightCrawlerStrategy for managing sessions.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_id": 3,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The `set_hook()` method of `AsyncPlaywrightCrawlerStrategy` can be used to set hooks, potentially related to implementing specific wait conditions based on CSS or JS, as demonstrated in the documentation.",
          "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_trace_chain_explanation": "The `arun()` method, through the `AsyncPlaywrightCrawlerStrategy`, might configure hooks for event handling related to wait conditions specified in the documentation. The `set_hook()` method facilitates setting such hooks.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "related",
          "relationship_explanation": "Although not directly used, `JsonCssExtractionStrategy` is mentioned in the context hierarchically in discussing extraction techniques using different strategies in the same extraction module.",
          "predicted_trace_chain": "llm.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "predicted_trace_chain_explanation": "The document discusses extraction strategies, hinting at the hierarchical relationship by contextualizing `LLMExtractionStrategy` alongside potential alternatives like `JsonCssExtractionStrategy`, showcasing possible variations/expansions.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_id": 20,
          "artifact_title": "CrawlResult.success",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The attribute `success` from the `CrawlResult` is implicitly used as it refers to checking whether the `arun()` method executed successfully. It supports drawing conclusions from the crawl results, indicating the method's reliability and is shown in the code by how results are being processed through `result.markdown`.",
          "predicted_trace_chain": "index.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.success",
          "predicted_trace_chain_explanation": "The use of `AsyncWebCrawler.arun()` method ultimately returns a `CrawlResult`, where attributes like `success` can be evaluated. This constitutes the flow of checking outcomes after invocation, linking the class method and its resultant data model.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
          "ground_truth_trace_chain": "index.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 5,
          "artifact_title": "AsyncWebCrawler.arun()",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "While `arun` is not directly mentioned, the setup implies `AsyncWebCrawler` methods, such as `arun`, will be used to perform crawling within the async context manager.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
          "predicted_trace_chain_explanation": "The AsyncWebCrawler class's methods, including `arun`, are logically inferred to be part of the operations during usage, given the example's context.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.screenshot",
          "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.screenshot",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.process_iframes()",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "The presence of the 'process_iframes=True' argument implies the underlying functionality related to iframe processing, which is implemented by 'AsyncPlaywrightCrawlerStrategy.process_iframes()'. However, it's not mentioned directly in the text but is inferred from the description of 'process_iframes=True'.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy.process_iframes()",
          "predicted_trace_chain_explanation": "The pathway shows explicit use of 'arun' which in turn implicitly relies on 'AsyncPlaywrightCrawlerStrategy.process_iframes()' to manage iframe processing, highlighting its implementation role.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "accesses",
          "relationship_explanation": "Although not explicitly mentioned, the example uses the `result` object to check for `result.html`, indicating indirect access to the `html` attribute from `CrawlResult`. The evidence is seen in checking the `result` object for success and handling the content.",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The `arun` method returns a `CrawlResult`, allowing access to `html` within the result object. The pathway illustrates how the example indirectly uses this attribute after receiving the `CrawlResult` from `arun()`.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores Base64-encoded image data that is captured according to the documented configuration parameters.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "accesses",
          "relationship_explanation": "The example accesses 'result.extracted_content', implying that 'CrawlResult.extracted_content' is an attribute accessed to obtain the JSON converted extracted content.",
          "predicted_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The pathway highlights that after calling 'arun', the 'CrawlResult' object returned holds 'extracted_content'. This is accessed in the example to process the extracted content, thus establishing a correlation between method invocation and data retrieval.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 2,
      "timestamp": "2025-02-12T13:52:08.335739",
      "negative_results": [
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult.cleaned_html",
          "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult.cleaned_html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "access",
          "relationship_explanation": "In the example, 'result.extracted_content' is retrieved after executing the crawl, showing the utilization of the 'extracted_content' field of 'CrawlResult'.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The document snippet ends with accessing 'extracted_content', which denotes fetching results post execution. This shows the pathway from calling 'arun()' to getting the crawl results and using extracted fields.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "Although LLMExtractionStrategy is not directly mentioned, it is a type of ExtractionStrategy that can be used within AsyncWebCrawler's extract operation, hinting at potential implicit usage when processing content.",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "AsyncWebCrawler.arun() accepts various strategies for processing content, pointing to potential internal use of LLMExtractionStrategy among others when extracting meaningful data.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The example code sets options related to session processes like process_iframes, which indirectly implicate the use of kill_session within AsyncPlaywrightCrawlerStrategy to manage session lifecycle during a crawl.",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_trace_chain_explanation": "Use of process_iframes in AsyncWebCrawler.arun() likely involves session management features of AsyncPlaywrightCrawlerStrategy, including session cleanup through the kill_session method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "The 'word_count_threshold' argument in the 'arun' call associates with the 'ChunkingStrategy' which implements text chunking strategies pertinent in filtering the content based on word count.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The 'arun' method integrates chunking strategies to determine the application of the 'word_count_threshold', implying the usage of 'ChunkingStrategy' for such functionality.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_id": 21,
          "artifact_title": "ExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "The behavior to exclude tags and filter media links as described in the snippet likely involves 'ExtractionStrategy'. These strategies are abstracted under the 'AsyncWebCrawler' to manage content extraction.",
          "predicted_trace_chain": "content-selection.md -> AsyncWebCrawler.arun() -> ExtractionStrategy",
          "predicted_trace_chain_explanation": "While the snippet does not directly mention extractions, the functionality to exclude certain tags aligns with methods within 'ExtractionStrategy,' which 'AsyncWebCrawler.arun()' would utilize.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 15,
          "artifact_title": "CrawlResult.markdown",
          "predicted_relationship": "implicit",
          "relationship_type": "accesses",
          "relationship_explanation": "The 'content' in the result returned by the example corresponds to 'result.markdown', showing interaction with the markdown attribute of a CrawlResult instance.",
          "predicted_trace_chain": "browser-config.md -> CrawlResult -> CrawlResult.markdown",
          "predicted_trace_chain_explanation": "CrawlResult is integral in providing the data structure from which markdown content is extracted as depicted in the code example of the documentation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "access",
          "relationship_explanation": "The documentation implies accessing extracted content via 'result.extracted_content', hinting that it is part of a CrawlResult, which aligns with 'CrawlResult.html'.",
          "predicted_trace_chain": "overview.md -> CrawlResult -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The documentation steps mention processing results from AsyncWebCrawler.arun(), implying access to result properties like html, thus bridging execution back to CrawlResult's html property.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.error_message",
          "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.error_message",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
          "ground_truth_trace_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "The `AsyncWebCrawler` uses a strategy pattern for crawling which defaults to `AsyncPlaywrightCrawlerStrategy`, extending `AsyncCrawlerStrategy`. This is implied as the base abstraction which provides the contract that `AsyncPlaywrightCrawlerStrategy` implements, vital for creating the crawler context.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "The documentation explains an instance of `AsyncWebCrawler` which internally uses `AsyncPlaywrightCrawlerStrategy` by default, hence extending from `AsyncCrawlerStrategy`, showing a necessary inheritance path.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 1,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "default_strategy",
          "relationship_explanation": "The `AsyncWebCrawler` by default uses `AsyncPlaywrightCrawlerStrategy` for the crawling strategy. While not explicitly stated, this class serves as an implicit default strategy implementation when initiating the `AsyncWebCrawler` instance.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
          "predicted_trace_chain_explanation": "When an `AsyncWebCrawler` instance is created, it uses `AsyncPlaywrightCrawlerStrategy` as the default crawling strategy, making this relationship an implicit default setup in the code structure.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_relationship": "implicit",
          "relationship_type": "submethod",
          "relationship_explanation": "The `kill_session()` method is an internal component that may be used by the class's broader session management features, which would relate to handling dynamic content in a way that involves session control.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_trace_chain_explanation": "Starting from `browser-config.md`, the `AsyncWebCrawler` class uses `AsyncPlaywrightCrawlerStrategy`, which in turn manages sessions using `kill_session()`. Each step is necessary to trace how session termination could relate to dynamic content processing ensuring resources are properly managed.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 3,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_relationship": "implicit",
          "relationship_type": "integrates_with",
          "relationship_explanation": "The `set_hook()` method in `AsyncPlaywrightCrawlerStrategy` allows custom hooks which are important in managing events during crawling, potentially including events for dynamic content handling as shown in documentation.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_trace_chain_explanation": "The documentation implicitly uses hooks in `arun()`, which would rely on `set_hook()` methods in its strategy class (`AsyncPlaywrightCrawlerStrategy`) to manage dynamic content-based events, thus forming a complete pathway from configurations to implementation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 8,
          "artifact_title": "CrawlResult",
          "predicted_relationship": "implicit",
          "relationship_type": "returns",
          "relationship_explanation": "The CrawlResult class is implicitly used as the returned type of AsyncWebCrawler.arun(), as evidenced by accessing 'result.extracted_content'.",
          "predicted_trace_chain": "llm.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult",
          "predicted_trace_chain_explanation": "The arun() method returns a CrawlResult object, which is then used to access extracted content in the example.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "attribute access",
          "relationship_explanation": "Although not mentioned explicitly, the code example accesses the 'markdown' attribute of the CrawlResult, indicating its role in processing the result of arun().",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The pathway starts from the documentation example, following the route of AsyncWebCrawler's arun() usage, which returns a CrawlResult object with accessible attributes like 'markdown'.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extending",
          "relationship_explanation": "While 'LLMExtractionStrategy' is not directly referenced in the snippet, since 'CosineStrategy' is a specific type of 'ExtractionStrategy' and the documentation refers to it within a broader content extraction context, inferring 'LLMExtractionStrategy' as an analogous strategy class involved in this domain.",
          "predicted_trace_chain": "cosine.md -> ExtractionStrategy -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "The mention of various strategies in 'cosine.md' implies the role of base and derived strategy implementations. Hence, 'LLMExtractionStrategy', as an implementation or extension of 'ExtractionStrategy', can be seen as integrally related along with 'CosineStrategy'.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 3,
      "timestamp": "2025-02-12T13:55:03.233592",
      "negative_results": [
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 5,
          "artifact_title": "AsyncWebCrawler.arun()",
          "predicted_relationship": "implicit",
          "relationship_type": "demonstrates usage",
          "relationship_explanation": "Although not explicitly mentioned, the context manager in the example prepares for actions that would utilize the `arun` method for web crawling operations. The setup implies using AsyncWebCrawler's crawling capabilities, facilitated by `arun()`.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
          "predicted_trace_chain_explanation": "The document suggests using the crawler, implicitly linking to the method `arun` that performs the key functionality of crawling in `AsyncWebCrawler`. The usage context logically extends from `AsyncWebCrawler` to `arun()` as the expected operation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 1,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "Although not directly mentioned in the snippet, 'CosineStrategy' is likely used in conjunction with 'AsyncPlaywrightCrawlerStrategy' since strategies would be implemented within a broader crawling framework.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> AsyncPlaywrightCrawlerStrategy",
          "predicted_trace_chain_explanation": "The document 'cosine.md' leading to 'CosineStrategy' suggests the strategy's use in broader crawling contexts, like those managed by 'AsyncPlaywrightCrawlerStrategy'. This helps in understanding the application context for the strategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The need to traverse through and extract structured data hints towards asynchronous crawling capabilities, which can be enabled by strategies like 'AsyncCrawlerStrategy' that aid in handling such operations efficiently.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> ExtractionStrategy -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "The schema in the document suggests the necessity for crawling strategies like 'AsyncCrawlerStrategy', highlighting its role in the broader process of extracting structured data via an asynchronous approach, thus supporting the extraction strategies within a larger workflow.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "Although not directly named, LLMExtractionStrategy could be inferred as used within the crawling process for handling dynamic content extraction as the documentation involves complex interactions with JavaScript and potentially extracting structured content, which is typical of what LLMs are used for.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "Based on the advanced configuration settings that include JavaScript execution and dynamic content handling mentioned in the doc, it can be inferred that a powerful extraction strategy like LLMExtractionStrategy might be employed internally by AsyncWebCrawler.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 23,
          "artifact_title": "JsonCssExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "JsonCssExtractionStrategy might be used implicitly for handling CSS selector-based content fetching demonstrated in the 'wait_for' parameter in the code snippet, which suggests waiting for a specific dynamic content area to load.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> JsonCssExtractionStrategy",
          "predicted_trace_chain_explanation": "The documentation includes waiting for a CSS selector indicating its relevance in content extraction scenarios. The strategy using CSS selectors underlies functionality such as that described in the documentation snippet.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.screenshot",
          "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.screenshot",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 5,
          "artifact_title": "AsyncWebCrawler.arun()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The mention of handling dynamic content and checking the `result.success` flag implicitly suggests the use of `AsyncWebCrawler.arun()` which performs such actions as part of the crawling and extraction process.",
          "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncWebCrawler.arun()",
          "predicted_trace_chain_explanation": "The `AsyncWebCrawler.arun()` method likely executes strategies like `JsonCssExtractionStrategy` and handles dynamic content and result success checks, which are mentioned as best practices in the documentation.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 4,
          "artifact_title": "AsyncWebCrawler",
          "predicted_relationship": "explicit",
          "relationship_type": "implements",
          "relationship_explanation": "Although not directly mentioned, the context of guidance on using strategies for extracting content from dynamic pages implicitly requires the functionality provided by `AsyncWebCrawler`, particularly through the orchestration of different strategies.",
          "predicted_trace_chain": "css.md -> AsyncWebCrawler",
          "predicted_trace_chain_explanation": "The documentation on working with `JsonCssExtractionStrategy` and handling dynamic content requires encapsulating strategies and orchestration which the `AsyncWebCrawler` provides at a higher level. This lists the base mechanism through which strategies are executed.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_relationship": "implicit",
          "relationship_type": "potential use",
          "relationship_explanation": "While `kill_session()` is not explicitly mentioned, session management as detailed in documentation could imply its utility in real applications, particularly when handling failed retry attempts.",
          "predicted_trace_chain": "llm.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_trace_chain_explanation": "In scenarios where sessions need to be managed or terminated due to retries or failures, the chain from general crawling (via `AsyncWebCrawler`) to session operations in `AsyncPlaywrightCrawlerStrategy` might engage `kill_session()`.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_id": 3,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The use of hooks such as 'remove_overlay_elements' suggests that set_hook might be invoked to modify browser behavior according to given options.",
          "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_trace_chain_explanation": "The ability to modify behavior by setting hooks is indirectly demonstrated by the usage patterns in arun(), which likely involves set_hook operations when executing configurations like 'remove_overlay_elements'.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
          "ground_truth_trace_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 4,
      "timestamp": "2025-02-12T13:57:46.779435",
      "negative_results": [
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
          "ground_truth_trace_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the fit_markdown functionality through its crawl() method, which returns AsyncCrawlResponse objects containing the extracted content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown feature through a dedicated string field that stores the extracted main content after applying heuristic filtering to remove boilerplate elements from the full markdown content.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the fit_markdown functionality through its content extraction methods, particularly in the crawl() method which handles page loading and content processing to enable smart content identification while removing unwanted elements through remove_overlay_elements().",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 14,
          "artifact_title": "CrawlResult.links",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The usage example in the documentation refers to the 'CrawlResult' object returned by the 'arun' method to access image media and their metadata. Although 'links' as an attribute isn't directly used, it implies interaction with CrawlResult's structure regarding media information.",
          "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.links",
          "predicted_trace_chain_explanation": "The trace starts with the use of the AsyncWebCrawler's 'arun' method which returns a CrawlResult, which inherently includes access to links or media-related elements as depicted in the example. This implicit pathway shows how 'arun' connects its results to CrawlResult properties.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "The Lazy-Loaded content handling by setting CSS selectors and waiting implies a connection to chunking strategies possibly used in AsyncWebCrawler. Although not directly mentioned, this background use case infers involvement of chunking strategy for organizing how data is crawled and processed.",
          "predicted_trace_chain": "content-processing.md -> AsyncWebCrawler -> AsyncCrawlerStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The documentation points towards a sophisticated handling of webpage content, implicitly signaling the use of strategies like chunking to manage loading and processing efficiently. Starting with AsyncWebCrawler which uses AsyncCrawlerStrategy and then further leading to ChunkingStrategy which handles chunking of content during crawling.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "page-interaction.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "Although not explicitly mentioned, the tip on combining with JS execution for handling dynamic content implies a broader crawling mechanism, likely implementing the 'AsyncCrawlerStrategy'.",
          "predicted_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "The document hints at broader crawling functionalities that are typically offered by extending 'AsyncCrawlerStrategy', necessary for full-featured web scraping operations.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 4,
          "artifact_title": "AsyncWebCrawler",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'handle dynamic content' and 'error handling' tips suggest the use of a crawler strategy, which is facilitated by 'AsyncWebCrawler', a class employing various strategies including JSON extraction.",
          "predicted_trace_chain": "css.md -> AsyncWebCrawler",
          "predicted_trace_chain_explanation": "The documentation refers to capabilities that involve managing crawl sessions and error handling, typical uses of 'AsyncWebCrawler', which integrates 'JsonCssExtractionStrategy' to perform these actions.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core methods needed to implement the session management behaviors described in the documentation, including crawl() for executing page actions and hook management for handling session state.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.cleaned_html",
          "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.cleaned_html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'kill_session' method is used internally within 'AsyncPlaywrightCrawlerStrategy' to manage session-related functionality, which supports operations like those shown in 'arun' within an example of dynamic content handling.",
          "predicted_trace_chain": "page-interaction.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_trace_chain_explanation": "The session management is critical for executing multiple JavaScript operations during crawling, implicitly related to 'arun' through 'AsyncPlaywrightCrawlerStrategy'.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "utilizes",
          "relationship_explanation": "The example checks for `result.success` and `result.screenshot`, implying the use of `CrawlResult` attributes like `html` indirectly when accessing the overall `CrawlResult` object returned by `arun()`.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The `quickstart.md` demonstrates capturing a screenshot where the `CrawlResult` object fields like `html` and `screenshot` are integral to path completion by providing the final results from `arun()` method.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The documentation indirectly references the results of a crawl operation (`result1.markdown[:100]`) which would be part of the 'CrawlResult' data structure. While not explicitly named in the snippet, the example implies the use of 'extracted_content' or similar derived content from the 'CrawlResult' object.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The chaining shows that after using the 'AsyncWebCrawler' and its 'arun' method, the result is processed to access its representation, highlighting 'CrawlResult' and associated functionality, pointing towards data storage and retrieval within a web crawl response.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The parameter 'word_count_threshold' is used in the example, which is often related to chunking strategies that define behavior based on word count, such as ChunkingStrategy implementations.",
          "predicted_trace_chain": "simple-crawling.md -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The usage of 'word_count_threshold' indirectly indicates ChunkingStrategy, as chunking strategies utilize such parameters to determine text segmentation sizes.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 21,
          "artifact_title": "ExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The documentation mentions content filtering and processing features which are typically handled by extraction strategies, suggesting the use of an ExtractionStrategy.",
          "predicted_trace_chain": "simple-crawling.md -> ExtractionStrategy",
          "predicted_trace_chain_explanation": "Features such as content filtering ('extracted_content') indicate reliance on extraction strategies for processing content during crawling.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 1,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "Implicitly used as part of the `AsyncWebCrawler` initialization, which creates an instance of `AsyncPlaywrightCrawlerStrategy` when no strategy is provided. This is inferred from the `AsyncWebCrawler`'s constructor logic, suggesting the strategy's dependency.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
          "predicted_trace_chain_explanation": "The `AsyncWebCrawler` implicitly uses `AsyncPlaywrightCrawlerStrategy` if no other strategy is provided, demonstrating that the strategy is a critical part of the overall crawling setup within the context of the described functionality.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.html",
          "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_id": 6,
          "artifact_title": "ChunkingStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "The 'CosineStrategy' is expected to involve chunking page content as one of its first steps. 'ChunkingStrategy' as a base class is likely providing the interface or methods for this operation, implicating a relationship as a superclass.",
          "predicted_trace_chain": "cosine.md -> CosineStrategy -> ChunkingStrategy",
          "predicted_trace_chain_explanation": "The 'cosine.md' relates to 'CosineStrategy', which likely uses chunking as described in the document. This suggests that 'CosineStrategy' might depend on 'ChunkingStrategy', establishing an inheritance path.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.error_message",
          "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.error_message",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "JsonCssExtractionStrategy",
          "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "LLMExtractionStrategy",
          "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> LLMExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult.cleaned_html",
          "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult.cleaned_html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    },
    {
      "run_id": 5,
      "timestamp": "2025-02-12T14:00:35.332627",
      "negative_results": [
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawling operations, including the crawl() method that enables the markdown extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the fit_markdown property documented in the markdown section by storing the extracted and cleaned main content as an Optional[str] field that removes boilerplate content.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables content extraction and markdown conversion by using Playwright to navigate web pages and retrieve their HTML content, which can then be processed to extract relevant content as described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "While not directly mentioned, `AsyncWebCrawler` relies on `AsyncCrawlerStrategy` or a derived strategy to function, because `AsyncWebCrawler` is designed to work with a strategy object, and `AsyncCrawlerStrategy` provides the interface for such strategies.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "The `AsyncWebCrawler` is designed to be used with a crawler strategy (`AsyncCrawlerStrategy` or derivatives like `AsyncPlaywrightCrawlerStrategy`). The trace chain reflects that although `AsyncCrawlerStrategy` is not directly mentioned in the doc snippet, it forms part of the underlying implementation that `AsyncWebCrawler` depends on.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 1,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "implements",
          "relationship_explanation": "The `AsyncWebCrawler` by default uses `AsyncPlaywrightCrawlerStrategy` as the strategy if none is passed, indicated by its constructor initialization in the `AsyncWebCrawler` class. This makes `AsyncPlaywrightCrawlerStrategy` an implicit part of the user flow when following the quickstart guide.",
          "predicted_trace_chain": "quickstart.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy",
          "predicted_trace_chain_explanation": "When following the usage pattern of `AsyncWebCrawler` in the quickstart guide, the default behavior includes using `AsyncPlaywrightCrawlerStrategy`, connecting the quickstart documentation, `AsyncWebCrawler`, and its default strategy.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of the documented crawling operations like form submissions and infinite scrolling by capturing HTML content, success status, and extracted data in structured fields.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class manages the filtered domain crawl results by storing cleaned HTML, extracted links, and associated metadata that reflect the domain-based filtering rules specified in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures crawling outcomes including response_headers and status_code which are essential for verifying if anti-detection measures successfully masked the crawler's automated nature.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class provides the abstract interface that enables the documented JsonCssExtractionStrategy to execute its CSS-based data extraction through the crawl method which returns AsyncCrawlResponse objects.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS-based extraction in its 'extracted_content' field, allowing the JsonCssExtractionStrategy to save structured data parsed from HTML elements matching the specified CSS selectors.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the underlying page navigation and browser automation functionality needed to support JsonCssExtractionStrategy's CSS selector-based data extraction from web pages.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the media selection functionality shown in the documentation through its crawl method which returns an AsyncCrawlResponse containing the structured media data.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class includes media processing capabilities that enable the documented functionality of extracting and organizing different media types (images, videos, audios) from crawled web pages, with methods to access their metadata and attributes.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
          "ground_truth_trace_chain": "quickstart.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes an extracted_content field to store the text chunks produced by the RegexChunking strategy described in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements browser automation capabilities that enable the crawler to process RegexChunking by retrieving HTML content which can then be split using regex patterns as shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawling outcomes for any of the three supported browser types (Chromium, Firefox, or WebKit) documented in the browser configuration guide.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes markdown and fit_markdown fields to store the text output from HTML conversion using the documented html2text configuration options.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores crawled webpage data including response_headers and metadata fields that directly receive the custom identity information (user agents and headers) described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_relationship": "implicit",
          "relationship_type": "method reference",
          "relationship_explanation": "While not directly mentioned in the example, AsyncPlaywrightCrawlerStrategy.kill_session() is involved in session management which is part of the crawl operation lifecycle.",
          "predicted_trace_chain": "quickstart.md -> async_crawler_strategy.py -> AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_trace_chain_explanation": "Part of the web crawling operation involves session management handled by AsyncPlaywrightCrawlerStrategy, showing the connectivity in lifecycle handling.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_id": 3,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_relationship": "implicit",
          "relationship_type": "method reference",
          "relationship_explanation": "Though not explicitly mentioned, AsyncPlaywrightCrawlerStrategy.set_hook() supports the flexibility shown in the snippet for managing customizable hooks during a crawl operation.",
          "predicted_trace_chain": "quickstart.md -> async_crawler_strategy.py -> AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_trace_chain_explanation": "The example implies potential use of hooks, which the method set_hook() facilitates, highlighting its indirect role in the flexibility of crawling operations.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Method",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of overlay removal and content fitting operations through its fit_html, fit_markdown, and cleaned_html fields that correspond to the documented overlay handling functionality.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds the extraction results shown in the documentation examples, with the extracted_content field specifically storing the output from both JsonCssExtractionStrategy and LLMExtractionStrategy operations.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_id": 22,
          "artifact_title": "FixedLengthWordChunking",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While `FixedLengthWordChunking` is not directly mentioned, `RegexChunking` implements `ChunkingStrategy`, and other strategies like `FixedLengthWordChunking` are likely to extend `ChunkingStrategy`. This implies a conceptual relationship as different implementation strategies for chunking.",
          "predicted_trace_chain": "chunking.md -> RegexChunking -> FixedLengthWordChunking",
          "predicted_trace_chain_explanation": "The documentation describes `RegexChunking`, a class implementing features of the chunking methodology. It suggests a broader context that includes other chunking implementations, leading to an inference of relationship to `FixedLengthWordChunking`, another strategy implementing similar principles.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from CSS selectors in its extracted_content field, enabling the targeted content extraction functionality described in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the cleaned_html as an optional string field that contains the sanitized HTML output described in the documentation after removing scripts, styles, and user-specified excluded tags.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the example's web crawling functionality, including crawl() which is used by AsyncWebCrawler to fetch and process the NBC News business page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality shown in the documentation example by providing methods like crawl() that handle browser automation, HTML extraction, and custom configurations needed for the AsyncWebCrawler's arun() method.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_id": 2,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The documentation highlights options related to session handling, implicitly utilizing the `kill_session` method for managing sessions. This is inferred from the session and task cleanup operations that a strategy like `AsyncPlaywrightCrawlerStrategy` must support within `arun`.",
          "predicted_trace_chain": "simple-crawling.md -> AsyncWebCrawler.arun() -> AsyncPlaywrightCrawlerStrategy.kill_session()",
          "predicted_trace_chain_explanation": "`AsyncWebCrawler.arun()` interacts with session management indirectly through the strategy class methods such as `kill_session`. Even though not explicitly documented here, understanding session management is critical for hiding intricate details of session cleanup when custom crawl options are applied.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the interface that enables the asynchronous crawling functionality shown in the documentation's 'arun()' example, requiring concrete implementations to handle URL crawling and return response objects containing HTML, markdown, and media data.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documented CrawlResult properties directly correspond to the HTML, status codes, media, and links that are extracted and processed by the AsyncPlaywrightCrawlerStrategy's crawl() method using Playwright's page APIs.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not explicitly named, 'AsyncWebCrawler' utilizes 'JsonCssExtractionStrategy', which is a form of CrawlerStrategy. Given that 'AsyncWebCrawler' operates using strategies, and 'JsonCssExtractionStrategy' indirectly operates through it, 'AsyncCrawlerStrategy' is necessarily part of the implementation path.",
          "predicted_trace_chain": "css.md -> AsyncWebCrawler -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "AsyncWebCrawler uses strategies to execute crawling tasks. As 'JsonCssExtractionStrategy' tied to AsyncWebCrawler implies a strategy-based execution, the base abstract class 'AsyncCrawlerStrategy' forms part of the hierarchical pathway, linking strategy usage in the AsyncWebCrawler.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced features shown in the documentation, such as custom clustering and content filtering through its abstract crawl methods.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data model that captures and stores the extraction results shown in the documentation examples, with its extracted_content field holding the clustered and filtered content returned by the CosineStrategy.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the filtered links data structure in its 'links' dictionary field, which gets populated based on the smart filtering parameters shown in the documentation like exclude_external_links and exclude_domains.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class is used to store and structure the crawling output data regardless of which browser engine (Firefox, WebKit, or Chromium) is used to perform the crawl.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the asynchronous web crawling functionality shown in the Quick Start documentation example.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
          "ground_truth_trace_chain": "index.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core asynchronous web crawling functionality demonstrated in the quick start documentation, providing the underlying browser automation and content extraction capabilities used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "index.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/index.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured data model that stores the crawled webpage content, including the markdown field shown being printed in the documentation's example code.",
          "ground_truth_trace_chain": "index.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the LLM-processed article summaries demonstrated in the documentation example where technology-focused content is extracted from NBC News articles.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured output fields that store the results of a web crawl performed with AsyncWebCrawler, including fields like 'success' and 'cleaned_html' that are particularly relevant when magic mode is enabled for enhanced anti-detection.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing metadata extraction through its crawl method, which returns AsyncCrawlResponse objects containing the metadata fields shown in the documentation example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class extracts metadata from web pages by navigating to URLs and accessing page content through Playwright's browser automation, which enables the metadata extraction functionality documented in the example code snippet.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output after applying the documented timing controls like page_timeout and delay_before_return_html during web crawling.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the cryptocurrency price extraction example by providing essential crawling operations like crawl(), crawl_many(), and screenshot capabilities.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the browser automation logic needed to extract cryptocurrency prices from Coinbase's explore page as shown in the documentation example, handling page navigation, DOM manipulation, and data extraction through Playwright's API.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 24,
          "artifact_title": "LLMExtractionStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "While not directly mentioned in the documentation snippet, `LLMExtractionStrategy` is implicitly related as it extends the capabilities of extraction strategies which `JsonCssExtractionStrategy` belongs to. The complexity of the HTML described suggests the use of advanced extraction strategies like LLM for handling intricate data models.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> LLMExtractionStrategy",
          "predicted_trace_chain_explanation": "Starting with `JsonCssExtractionStrategy`, it\u2019s logical to consider `LLMExtractionStrategy` due to its advanced features that could complement the JSON-CSS extraction techniques.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model captures and stores the outcomes of JavaScript execution commands, including the resulting HTML, success status, and any errors that may occur during the execution of the documented JS commands.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement proxy-based web crawling, which the documentation shows how to configure through the proxy parameter.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the results of proxy-enabled web crawling operations shown in the documentation, including the crawled URL, HTML content, and various optional fields for metadata and error handling.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that the documented CosineStrategy implementations use to perform specialized content extraction tasks like article crawling, review analysis, and technical documentation parsing.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented use cases through its arun() method, which accepts an extraction_strategy parameter that can be configured with different CosineStrategy settings for article content, product reviews, or technical documentation extraction as shown in the examples.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as the data structure for storing various extracted content types shown in the use cases, including the extracted_content field for article content, metadata for reviews, and technical documentation content along with associated metadata like URLs and success status.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the crawling functionality needed to support the three documented use cases (article extraction, product review analysis, and technical documentation) by providing configurable browser automation with customizable waiting conditions, content processing, and error handling.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the scraped data and metadata from dynamic content crawling sessions, with fields like session_id and extracted_content that directly correspond to the session-based crawling approach shown in the documentation.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncWebCrawler.arun()",
          "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncWebCrawler.arun()",
          "traceability_granularity": "Method",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented features like content filtering, markdown conversion, and media handling through its abstract crawl methods.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements all the key properties referenced in the best practices documentation, including fit_markdown for articles, media for image handling, links for content analysis, and cleaned_html for content processing.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented best practices by providing methods for Markdown content processing, media handling with score filtering, link analysis, and customizable content cleaning through configuration parameters in its crawl method.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods used by the AsyncWebCrawler shown in the basic usage documentation, where crawl() is the underlying method powering the arun() functionality.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure of the return value from the AsyncWebCrawler.arun() method shown in the configuration documentation, storing crawled data like HTML, links, and metadata from the target URL.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for implementing the content cleaning operations described in the documentation through abstract methods like crawl() which processes URLs and applies the cleaning configurations.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the outcomes of JavaScript execution during crawling, including the modified HTML content, success status, and any errors that occurred during script execution.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
          "ground_truth_trace_chain": "quickstart.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores response_headers as an optional dictionary field to capture the custom headers used in HTTP requests like those shown in the documentation example.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class has error handling fields like error_message and status_code that support the documented retry mechanism by storing error information when API calls fail.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the interface that concrete crawlers must implement to work with the error handling and retry logic shown in the documentation, where specific implementations would handle the actual HTTP requests and content extraction that might need retrying.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through class closures and cleanup methods in the crawler strategy, complementing the documented retry mechanism by ensuring proper resource management even when errors occur during crawling operations.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
          "ground_truth_trace_chain": "session-management.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores session-specific data like session_id and status_code that are essential for implementing the documented session management practices including state tracking across page navigations.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "AsyncCrawlerStrategy defines the abstract interface for web crawling operations that support the documented markdown conversion functionality through its crawl method which returns responses that can be formatted into markdown.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented markdown conversion functionality through its arun() method, which processes HTML content and converts it to markdown format while optionally preserving links as specified by the include_links_on_markdown parameter.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'markdown' field that stores the converted HTML content as standard markdown format, which directly enables the functionality shown in the documentation example where crawler.arun() returns markdown output.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements web crawling functionality that enables HTML-to-markdown conversion by fetching web content using Playwright, as demonstrated in the documentation's example code showing how to retrieve and convert webpage content to markdown format.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface methods needed to implement the link analysis capabilities described in the documentation, including the crawl method that returns AsyncCrawlResponse objects containing categorized link data.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
          "ground_truth_trace_chain": "content-processing.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
          "document_location": "docs/md_v2/advanced/content-processing.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements link analysis by crawling pages asynchronously and providing methods to extract and categorize internal, external, and social media links through Playwright's DOM manipulation capabilities.",
          "ground_truth_trace_chain": "content-processing.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 11,
          "artifact_title": "CrawlResult.extracted_content",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The method 'arun' returns a 'CrawlResult' object, which contains the 'extracted_content' attribute accessed in the example function.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.extracted_content",
          "predicted_trace_chain_explanation": "The 'arun' method returns a 'CrawlResult', inside which 'extracted_content' is called implicitly in handling the crawling result.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_id": 13,
          "artifact_title": "CrawlResult.html",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The 'arun' method's result could indirectly involve the 'html' attribute of 'CrawlResult'.",
          "predicted_trace_chain": "browser-config.md -> AsyncWebCrawler -> AsyncWebCrawler.arun() -> CrawlResult.html",
          "predicted_trace_chain_explanation": "The 'html' content within 'CrawlResult' is likely used implicitly when the result is processed in the example.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.screenshot",
          "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.screenshot",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables detailed logging by storing diagnostic information like error_message, status_code, and response_headers that get populated when verbose mode is enabled.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
          "ground_truth_trace_chain": "llm.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
          "ground_truth_trace_chain": "llm.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface that enables the asynchronous web crawling functionality shown in the documentation example, where AsyncWebCrawler uses this interface to fetch OpenAI pricing data.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
          "document_location": "docs/md_v2/extraction/llm.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core web crawling functionality that enables the extraction example to fetch and process web content from the OpenAI pricing page before applying the LLMExtractionStrategy for data extraction.",
          "ground_truth_trace_chain": "llm.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field stores the scraped GitHub commit data as a JSON string, which is then parsed and accumulated into the all_commits list during the pagination process.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure that enables the JsonCssExtractionStrategy used in the documentation to extract commit information from GitHub pages through a unified interface.",
          "ground_truth_trace_chain": "session-management.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult model stores the extracted commits and session data from the GitHub crawling example by providing fields like 'extracted_content' for the commit data and 'session_id' for tracking the TypeScript commits session.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to verify if commit extraction was successful before appending the commits to all_commits list.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_id": 0,
          "artifact_title": "AsyncCrawlerStrategy",
          "predicted_relationship": "implicit",
          "relationship_type": "extends",
          "relationship_explanation": "JsonCssExtractionStrategy, by dealing with data extraction, implicitly relies on overall crawler strategies, such as 'AsyncCrawlerStrategy', for retrieving the content to be processed.",
          "predicted_trace_chain": "css-advanced.md -> JsonCssExtractionStrategy -> AsyncCrawlerStrategy",
          "predicted_trace_chain_explanation": "JsonCssExtractionStrategy likely operates on data provided by a broader crawler strategy like AsyncCrawlerStrategy, establishing a chain of reliance.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface methods that enable pattern-based extraction strategies like JsonCssExtractionStrategy to execute their crawling and content extraction operations through the crawl() method.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the pattern-based extraction results in its extracted_content field, which the documentation shows being accessed and parsed as JSON after running the JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation capabilities that enable the pattern-based extraction described in the documentation by providing methods to navigate pages and interact with DOM elements using CSS selectors defined in JsonCssExtractionStrategy.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of simulated user browsing sessions, including metadata like session_id and status_code that help verify the authenticity of the simulated user interaction.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented crawler usage pattern, including the main crawl() method that processes URLs and returns responses.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that holds the crawling output, including the extracted_content field shown being accessed in the documentation's example code.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements an AsyncPlaywrightCrawlerStrategy class that powers the AsyncWebCrawler shown in the documentation, handling the low-level browser automation needed to execute the semantic content extraction described in the example usage.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class implements the 'Choose the Right Strategy' section of the documentation by defining core methods that each concrete strategy (CSS, LLM, Cosine) must implement for web crawling.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "JsonCssExtractionStrategy",
          "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> JsonCssExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented error handling pattern through its crawl method, which returns an AsyncCrawlResponse containing success/failure information and extracted content exactly as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
          "ground_truth_trace_chain": "session-management.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/advanced/session-management.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements session tracking through its session_id field, which aligns with the documented session-based crawling functionality shown in the usage example.",
          "ground_truth_trace_chain": "session-management.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the response data from crawler requests made through rotating proxies, with fields like success and error_message to track proxy-related issues.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class serves as a structured container for storing the extracted commits and related metadata from the integrated JavaScript execution process described in the documentation.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core crawling methods that enable the caching functionality demonstrated in the documentation through its crawl method, which concrete implementations can use to handle cached and non-cached requests.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the output of custom hook executions by storing the extracted content and session state that are essential for implementing the documented commit-tracking functionality.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class supports combining strategies through its 'arun' method which accepts different extraction_strategy parameters, allowing sequential application of CSS and LLM strategies as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class provides the foundational interface that allows different crawling strategies (like CSS and LLM) to be implemented and combined as shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class enables storage of multiple extraction results by providing fields like extracted_content and metadata that can hold output from different strategies like CSS and LLM approaches shown in the documentation.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables sequential execution of different extraction strategies through its crawl method, which returns AsyncCrawlResponse objects that can be used as input for subsequent extraction operations as shown in the documentation example.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the LLM extraction output in its extracted_content field, which stores the structured data produced by the LLMExtractionStrategy as documented in the example.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content field in CrawlResult stores the JSON-formatted commit data obtained from the page after waiting for the specified condition to be met using the wait_for parameter.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for handling the extraction_strategy parameter shown in the documentation's example code, defining the abstract interface for parsing HTML content that concrete implementations like JsonCssExtractionStrategy use to extract commit information.",
          "ground_truth_trace_chain": "session-management-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that returns an AsyncCrawlResponse, which contains the success, error_message, and status_code properties shown in the error handling documentation.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation demonstrates error handling using the success attribute and error_message fields that are explicitly set in the AsyncWebCrawler's arun() method when exceptions occur during crawling.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements error handling through the AsyncCrawlResponse class which returns success status, error messages, and status codes that can be checked exactly as shown in the documentation's example.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_id": 3,
          "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_relationship": "implicit",
          "relationship_type": "uses",
          "relationship_explanation": "The example likely relies on hooks set via `set_hook()` in `AsyncPlaywrightCrawlerStrategy` to manage page events such as removing popups, through the `remove_overlay_elements` setting.",
          "predicted_trace_chain": "magic-mode.md -> AsyncWebCrawler -> AsyncPlaywrightCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy.set_hook()",
          "predicted_trace_chain_explanation": "Hooks are pivotal in altering the page behavior and executing functionality such as removing popups, aligning with the `remove_overlay_elements=True` usage in the sample.",
          "ground_truth_relationship": null,
          "ground_truth_trace_chain": null,
          "traceability_granularity": "Class",
          "confusion_metrics": "False Positive",
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.markdown",
          "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.markdown",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawl outcomes, with fields like markdown and success that are accessed in the crawl_protected_site function's return statement.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields for storing iframe-processed content through its html, cleaned_html, and extracted_content attributes which can contain the processed iframe results when process_iframes=True is set.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that the LLMExtractionStrategy needs to crawl web content before performing LLM-based data extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields like extracted_content and metadata to store the structured data obtained through LLMExtractionStrategy's semantic parsing of web content.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the web crawling infrastructure that enables the LLMExtractionStrategy to access and retrieve web content before passing it to language models for semantic extraction.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
          "ground_truth_trace_chain": "page-interaction.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the data collected after waiting for dynamic content to load using the documented CSS and JavaScript wait conditions, including the final HTML, extracted content, and success status.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted_content field which holds the structured data parsed by JsonCssExtractionStrategy as shown in the documentation example where crypto price data is extracted and stored.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the dynamic data extraction capabilities demonstrated in the documentation's example, particularly through its crawl method that supports JavaScript execution parameters.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
          "ground_truth_trace_chain": "css.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract class provides the technical foundation for implementing the three distinct strategies (CSS, LLM, and Cosine) outlined in the strategy selection guide through its abstract extract method and parallel processing capabilities.",
          "ground_truth_trace_chain": "overview.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
          "ground_truth_trace_chain": "proxy-security.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/advanced/proxy-security.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results when making authenticated proxy requests, capturing details like success status, HTML content, and error messages that may occur during proxy-authenticated web requests.",
          "ground_truth_trace_chain": "proxy-security.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
          "ground_truth_trace_chain": "css.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
          "ground_truth_trace_chain": "css.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
          "document_location": "docs/md_v2/extraction/css.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
          "ground_truth_trace_chain": "css.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the crawler to fetch content in different output formats through its crawl method which returns an AsyncCrawlResponse containing the various formats mentioned in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The code implements the documented output formats through the CrawlResult class, which returns html, cleaned_html, markdown, and fit_markdown properties corresponding exactly to the formats shown in the documentation example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality that enables the extraction of different output formats (raw HTML, cleaned HTML, markdown) by retrieving the page content through its crawl() method and returning an AsyncCrawlResponse object containing the raw HTML which can then be transformed into other formats.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures and stores the results of dynamic content crawling operations, including the raw HTML, processed content, and metadata generated from the browser interactions described in the documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented LLM-based content extraction by providing the crawling functionality needed to fetch web content before it can be processed by the LLMExtractionStrategy.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the backend infrastructure needed for the LLMExtractionStrategy to perform web crawling and content extraction, specifically providing the browser automation capabilities required to fetch web content that can then be processed by the LLM for structured content selection.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the documented AsyncWebCrawler to perform web scraping operations with customizable extraction strategies.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
          "ground_truth_trace_chain": "css-advanced.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
          "ground_truth_trace_chain": "css-advanced.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
          "document_location": "docs/md_v2/extraction/css-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality needed to execute the documented AsyncWebCrawler example by providing a Playwright-based web scraping engine that handles browser automation, JavaScript execution, and HTML extraction for complex web scraping tasks.",
          "ground_truth_trace_chain": "css-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface that enables pattern-based extraction by defining crawl methods that the JsonCssExtractionStrategy uses to fetch and process structured content like news articles.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class's extracted_content field stores the JSON output from pattern-based extraction strategies like JsonCssExtractionStrategy shown in the documentation.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented pattern-based selection by providing a flexible crawling engine that can navigate web pages and enable CSS-based extraction through its crawl method, allowing the JsonCssExtractionStrategy to perform the actual pattern matching and content extraction.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores filtered content outputs from the crawler, with properties like cleaned_html and extracted_content that reflect the content filtering options documented in the configuration parameters.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering through its crawl method by accepting parameters like word_count_threshold and excluded_tags which are used to control what content is included in the final HTML output.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.error_message",
          "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.error_message",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
          "ground_truth_trace_chain": "cosine.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the interface methods that enable the error-handled crawling operations shown in the documentation, particularly through its crawl method which returns AsyncCrawlResponse objects that contain success and error states.",
          "ground_truth_trace_chain": "cosine.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
          "ground_truth_trace_chain": "cosine.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
          "document_location": "docs/md_v2/extraction/cosine.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
          "ground_truth_trace_chain": "cosine.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation example, with crawl() and crawl_many() methods being essential for handling both single and batch URL processing.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
          "ground_truth_trace_chain": "session-management-advanced.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
          "document_location": "docs/md_v2/advanced/session-management-advanced.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
          "ground_truth_trace_chain": "session-management-advanced.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "CrawlResult.extracted_content",
          "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
          "ground_truth_trace_chain": "content-selection.md -> CrawlResult.extracted_content",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
          "ground_truth_trace_chain": "content-selection.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable the comprehensive example's combined extraction functionality, including crawl() and crawl_many() which are used by the extract_article_content() function to perform both structured and LLM-based extractions.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/content-selection.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the asynchronous web crawling functionality that enables the comprehensive example's pattern-based and LLM-based content extraction by providing browser automation capabilities and page manipulation methods.",
          "ground_truth_trace_chain": "content-selection.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures all the crawling outcomes including success/failure status and extracted content, which provides the return structure for the anti-bot crawling operations documented in the manual configuration options.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
          "ground_truth_trace_chain": "magic-mode.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
          "document_location": "docs/md_v2/extraction/chunking.md",
          "artifact_title": "ChunkingStrategy",
          "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
          "ground_truth_trace_chain": "chunking.md -> ChunkingStrategy -> ChunkingStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
          "ground_truth_trace_chain": "overview.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class stores the extracted content from the CosineStrategy in its extracted_content field along with metadata and source information needed for similarity-based clustering operations.",
          "ground_truth_trace_chain": "overview.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
          "document_location": "docs/md_v2/extraction/overview.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the underlying browser automation infrastructure needed to execute the cosine-based content extraction strategy by loading web pages and making their content available for analysis and clustering.",
          "ground_truth_trace_chain": "overview.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "CrawlResult.success",
          "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
          "ground_truth_trace_chain": "quickstart.md -> CrawlResult.success",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
          "document_location": "docs/md_v2/basic/quickstart.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
          "ground_truth_trace_chain": "quickstart.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult.cleaned_html",
          "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult.cleaned_html",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
          "document_location": "docs/md_v2/basic/page-interaction.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that stores and organizes the crawled content, headers, and metadata returned by each crawler.arun() call in the documented dynamic content crawling example.",
          "ground_truth_trace_chain": "page-interaction.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
          "ground_truth_trace_chain": "simple-crawling.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
          "document_location": "docs/md_v2/basic/simple-crawling.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
          "ground_truth_trace_chain": "simple-crawling.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncWebCrawler",
          "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncWebCrawler",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the multi-format extraction capabilities demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "ExtractionStrategy",
          "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
          "ground_truth_trace_chain": "output-formats.md -> ExtractionStrategy -> ExtractionStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class defines the structure for storing all possible outputs shown in the example code, including fit_markdown, extracted_content, and media which are used to capture the main content, structured data, and pattern data respectively.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "CrawlResult.media",
          "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
          "ground_truth_trace_chain": "output-formats.md -> CrawlResult.media",
          "traceability_granularity": "Statement-level",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
          "document_location": "docs/md_v2/basic/output-formats.md",
          "artifact_title": "AsyncPlaywrightCrawlerStrategy",
          "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
          "ground_truth_trace_chain": "output-formats.md -> AsyncCrawlerStrategy -> AsyncPlaywrightCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "AsyncCrawlerStrategy",
          "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
          "ground_truth_trace_chain": "browser-config.md -> AsyncCrawlerStrategy",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
          "document_location": "docs/md_v2/basic/browser-config.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class provides fields to store the crawling output including proxy-related data like response_headers and status_code which are essential for tracking proxy communication results shown in the proxy configuration documentation.",
          "ground_truth_trace_chain": "browser-config.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        },
        {
          "sent_document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
          "document_location": "docs/md_v2/advanced/magic-mode.md",
          "artifact_title": "CrawlResult",
          "ground_truth_relationship": "The CrawlResult class captures the outcomes of Magic Mode anti-bot operations by storing essential crawling data including success status, cleaned HTML, metadata, and session information needed to verify successful anti-detection measures.",
          "ground_truth_trace_chain": "magic-mode.md -> CrawlResult",
          "traceability_granularity": "Class",
          "confusion_metrics": "False Negative",
          "predicted_relationship": null,
          "relationship_type": null,
          "relationship_explanation": null,
          "predicted_trace_chain": null,
          "predicted_trace_chain_explanation": null,
          "prediction_details": {
            "matches_ground_truth": false,
            "relationship_match": false,
            "missed_by_llm": true
          }
        }
      ]
    }
  ]
}